{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework 3 - Supervised Learning II - MDS Computational Linguistics\n",
    "\n",
    "## T3 Helper Code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all the necessary imports\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torchtext\n",
    "from torchtext.data import Field, LabelField\n",
    "from torchtext.data import TabularDataset\n",
    "from torchtext.data import Iterator, BucketIterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# set the seed\n",
    "manual_seed = 572\n",
    "torch.manual_seed(manual_seed)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.0000, 0.2302, 0.4976, 0.1462, 0.2659, 0.1160, 0.2331, 0.2770,\n",
      "          0.1295, 0.3005],\n",
      "         [0.2625, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000],\n",
      "         [0.3008, 0.0134, 0.0624, 0.2059, 0.1582, 0.1532, 0.0704, 0.0551,\n",
      "          0.0224, 0.0000],\n",
      "         [0.1708, 0.0017, 0.0000, 0.0516, 0.0199, 0.0000, 0.1053, 0.0412,\n",
      "          0.0722, 0.0000]],\n",
      "\n",
      "        [[0.0933, 0.2024, 0.3714, 0.2602, 0.1871, 0.1673, 0.0322, 0.5629,\n",
      "          0.1314, 0.5456],\n",
      "         [0.0278, 0.0589, 0.0000, 0.1606, 0.0000, 0.0000, 0.1000, 0.0000,\n",
      "          0.0000, 0.0000],\n",
      "         [0.0788, 0.1129, 0.0000, 0.2778, 0.0000, 0.1022, 0.2206, 0.0000,\n",
      "          0.2809, 0.0000],\n",
      "         [0.1701, 0.1184, 0.0053, 0.0000, 0.0000, 0.0000, 0.0847, 0.0000,\n",
      "          0.1222, 0.0000]]], grad_fn=<ReluBackward0>)\n",
      "Take the highest value in each window using max pool\n",
      "tensor([[[0.2302, 0.4976, 0.2659, 0.2770, 0.3005],\n",
      "         [0.2625, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.3008, 0.2059, 0.1582, 0.0704, 0.0224],\n",
      "         [0.1708, 0.0516, 0.0199, 0.1053, 0.0722]],\n",
      "\n",
      "        [[0.2024, 0.3714, 0.1871, 0.5629, 0.5456],\n",
      "         [0.0589, 0.1606, 0.0000, 0.1000, 0.0000],\n",
      "         [0.1129, 0.2778, 0.1022, 0.2206, 0.2809],\n",
      "         [0.1701, 0.0053, 0.0000, 0.0847, 0.1222]]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "tensor([[[0.4976],\n",
      "         [0.2625],\n",
      "         [0.3008],\n",
      "         [0.1708]],\n",
      "\n",
      "        [[0.5629],\n",
      "         [0.1606],\n",
      "         [0.2809],\n",
      "         [0.1701]]], grad_fn=<SqueezeBackward1>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "x = torch.rand((2,5,10))   # batch size 2 with length 10 and 5 dim embedding.\n",
    "\n",
    "in_dim = 5\n",
    "filters = 4\n",
    "\n",
    "cnn1d = nn.Conv1d(in_dim,filters,kernel_size=3,padding=1) \n",
    "max_pool = nn.MaxPool1d(kernel_size=2)\n",
    "ad_max_pool = nn.AdaptiveMaxPool1d(1)\n",
    "activation = nn.ReLU()\n",
    "x = cnn1d(x)\n",
    "x = activation(x)\n",
    "print(x)\n",
    "print(\"Take the highest value in each window using max pool\")\n",
    "print(max_pool(x))\n",
    "print(ad_max_pool(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#useful for calculating size after padding and kernel\n",
    "OutputSize = Length+ p*2 -(kernel-1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper Code for loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext\n",
    "from torchtext.data import Field, LabelField\n",
    "from torchtext.data import TabularDataset\n",
    "\n",
    "# define the white space tokenizer to get tokens\n",
    "def tokenize_en(tweet):\n",
    "    \"\"\"\n",
    "    Tokenizes English tweet from a string into a list of strings (tokens)\n",
    "    \"\"\"\n",
    "    return tweet.strip().split()\n",
    "\n",
    "# define the TorchText's fields\n",
    "TEXT = Field(sequential=True, tokenize=tokenize_en, lower=True)\n",
    "LABEL = Field(sequential=False, unk_token = None)\n",
    "\n",
    "\n",
    "train, val, test = TabularDataset.splits(\n",
    "    path=\"./data/sentiment-twitter-2016-task4/\", # the root directory where the data lies\n",
    "    train='train.tsv', validation=\"dev.tsv\", test=\"test.tsv\", # file names\n",
    "    format='tsv',\n",
    "    skip_header=False, # if your tsv file has a header, make sure to pass this to ensure it doesn't get proceesed as data!\n",
    "    fields=[('tweet', TEXT), ('label', LABEL)])\n",
    "\n",
    "TEXT.build_vocab(train, min_freq=3) # builds vocabulary based on all the words that occur at least twice in the training set\n",
    "LABEL.build_vocab(train)\n",
    "\n",
    "train_iter, val_iter, test_iter = BucketIterator.splits(\n",
    " (train, val, test), # we pass in the datasets we want the iterator to draw data from\n",
    " batch_sizes=(4,64,64),\n",
    " sort_key=lambda x: len(x.tweet), \n",
    " sort=True,\n",
    "# A key to use for sorting examples in order to batch together examples with similar lengths and minimize padding. \n",
    " sort_within_batch=True\n",
    ")\n",
    "\n",
    "VOCAB_SIZE = len(TEXT.vocab.stoi)\n",
    "\n",
    "WORD_VEC_SIZE=300\n",
    "# Note, the parameters to Embedding class below are:\n",
    "# num_embeddings (int): size of the dictionary of embeddings\n",
    "# embedding_dim (int): the size of each embedding vector\n",
    "# For more details on Embedding class, see: https://github.com/pytorch/pytorch/blob/master/torch/nn/modules/sparse.py\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3330\n"
     ]
    }
   ],
   "source": [
    "print(VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ConvNet(nn.Module):\n",
    "  \n",
    "  def __init__(self, layer_num, filtersize, filters,nonlin, output_size, VOCAB_SIZE,  WORD_VEC_SIZE):  #feel free to add additional parameters\n",
    "    super(ConvNet, self).__init__()\n",
    "    self.embedding = nn.Embedding(VOCAB_SIZE, WORD_VEC_SIZE, sparse=True)\n",
    "    #your code here\n",
    "    self.embedding.weight.data.normal_(0.0,0.05)\n",
    "    self.layers = nn.ModuleList()\n",
    "    self.nonlin = nonlin\n",
    "    for i in range(layer_num):\n",
    "        if i == 0:\n",
    "            self.layers.append(nn.Conv1d(WORD_VEC_SIZE, filters, filtersize, padding=(filtersize-1)//2))\n",
    "        else:\n",
    "            self.layers.append(nn.Conv1d(filters,filters,filtersize, padding=(filtersize-1)//2))\n",
    "        self.layers.append(self.nonlin)    \n",
    "    self.max_layer = nn.AdaptiveMaxPool1d(1)\n",
    "    self.output = nn.Linear(filters, output_size)\n",
    "    self.softmax = nn.LogSoftmax(dim=1) \n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.embedding(x.permute(1,0)).permute(0,2,1)\n",
    "    for layer in self.layers:\n",
    "        x = layer(x)\n",
    "    x = self.max_layer(x).squeeze(dim=-1)\n",
    "    x =self.softmax(self.output(x))  # [N,H]\n",
    "    \n",
    "    return x\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "def train(loader,model,criterion,optimizer,device):\n",
    "    total_loss = 0.0\n",
    "    # iterate throught the data loader\n",
    "    num_sample = 0\n",
    "    for batch in loader:\n",
    "        # load the current batch\n",
    "        batch_input = batch.tweet\n",
    "        batch_output = batch.label\n",
    "        \n",
    "        batch_input = batch_input.to(device)\n",
    "        batch_output = batch_output.to(device)\n",
    "        # forward propagation\n",
    "        # pass the data through the model\n",
    "        model_outputs = model(batch_input)\n",
    "        # compute the loss\n",
    "        cur_loss = criterion(model_outputs, batch_output)\n",
    "        total_loss += cur_loss.item()\n",
    "\n",
    "        # backward propagation (compute the gradients and update the model)\n",
    "        # clear the buffer\n",
    "        optimizer.zero_grad()\n",
    "        # compute the gradients\n",
    "        cur_loss.backward()\n",
    "        # update the weights\n",
    "        optimizer.step()\n",
    "\n",
    "        num_sample += batch_output.shape[0]\n",
    "    return total_loss/num_sample\n",
    "\n",
    "# evaluation logic based on classification accuracy\n",
    "def evaluate(loader,model,criterion,device):\n",
    "    all_pred=[]\n",
    "    all_label = []\n",
    "    with torch.no_grad(): # impacts the autograd engine and deactivate it. reduces memory usage and speeds up computation\n",
    "        for batch in loader:\n",
    "             # load the current batch\n",
    "            batch_input = batch.tweet\n",
    "            batch_output = batch.label\n",
    "\n",
    "            batch_input = batch_input.to(device)\n",
    "            # forward propagation\n",
    "            # pass the data through the model\n",
    "            model_outputs = model(batch_input)\n",
    "            # identify the predicted class for each example in the batch\n",
    "            probabilities, predicted = torch.max(model_outputs.cpu().data, 1)\n",
    "            # put all the true labels and predictions to two lists\n",
    "            all_pred.extend(predicted)\n",
    "            all_label.extend(batch_output)\n",
    "            \n",
    "    accuracy = accuracy_score(all_label, all_pred)\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example Random Search (if you have trouble working with Skorch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats\n",
    "\n",
    "LEARNING_RATE=.1\n",
    "MAX_EPOCHS=10\n",
    "\n",
    "def random_search(num_iter):\n",
    "    results = []\n",
    "    for i in range(num_iter):\n",
    "        config = {\n",
    "            #define hyperparameters here\n",
    "            \"layers\": scipy.stats.randint.rvs(1,3),\n",
    "            \"filters\": scipy.stats.randint.rvs(10,200)\n",
    "            \n",
    "        }\n",
    "        \n",
    "        print(\"new config\")\n",
    "        print(config)\n",
    "        model = ConvNet(config[\"layers\"],3,config[\"filters\"],nn.ReLU(),output_size=3, VOCAB_SIZE=VOCAB_SIZE, WORD_VEC_SIZE=WORD_VEC_SIZE)\n",
    "        model.to(device)\n",
    "        criterion = nn.NLLLoss()\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    \n",
    "        max_val = 0\n",
    "        best_epoch = 0\n",
    "        for epoch in range(MAX_EPOCHS):\n",
    "        # train the model for one pass over the data\n",
    "            train_loss = train(train_iter,model,criterion,optimizer,device)  \n",
    "        # compute the training accuracy\n",
    "            train_acc = evaluate(train_iter,model,criterion,device)\n",
    "        # compute the validation accuracy\n",
    "            val_acc = evaluate(val_iter,model,criterion,device)\n",
    "            if val_acc > max_val:\n",
    "                max_val = val_acc\n",
    "                best_epoch = epoch+1\n",
    "        # print the loss for every epoch\n",
    "            print('Epoch [{}/{}], Loss: {:.4f}, Training Accuracy: {:.4f}, Validation Accuracy: {:.4f}'.format(epoch+1, MAX_EPOCHS, train_loss, train_acc, val_acc))\n",
    "        results.append((max_val,best_epoch,config))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new config\n",
      "{'layers': 2, 'filters': 110}\n",
      "Epoch [1/10], Loss: 0.2467, Training Accuracy: 0.5157, Validation Accuracy: 0.4217\n",
      "Epoch [2/10], Loss: 0.2451, Training Accuracy: 0.5157, Validation Accuracy: 0.4217\n",
      "Epoch [3/10], Loss: 0.2364, Training Accuracy: 0.5763, Validation Accuracy: 0.4712\n",
      "Epoch [4/10], Loss: 0.2268, Training Accuracy: 0.6090, Validation Accuracy: 0.4742\n",
      "Epoch [5/10], Loss: 0.2190, Training Accuracy: 0.6288, Validation Accuracy: 0.4747\n",
      "Epoch [6/10], Loss: 0.2089, Training Accuracy: 0.6398, Validation Accuracy: 0.4627\n",
      "Epoch [7/10], Loss: 0.1961, Training Accuracy: 0.6480, Validation Accuracy: 0.4547\n",
      "Epoch [8/10], Loss: 0.1820, Training Accuracy: 0.6607, Validation Accuracy: 0.4432\n",
      "Epoch [9/10], Loss: 0.1661, Training Accuracy: 0.6787, Validation Accuracy: 0.4487\n",
      "Epoch [10/10], Loss: 0.1533, Training Accuracy: 0.7137, Validation Accuracy: 0.4207\n",
      "new config\n",
      "{'layers': 1, 'filters': 26}\n",
      "Epoch [1/10], Loss: 0.2439, Training Accuracy: 0.5527, Validation Accuracy: 0.4687\n",
      "Epoch [2/10], Loss: 0.2267, Training Accuracy: 0.6248, Validation Accuracy: 0.4822\n",
      "Epoch [3/10], Loss: 0.2081, Training Accuracy: 0.6780, Validation Accuracy: 0.4867\n",
      "Epoch [4/10], Loss: 0.1898, Training Accuracy: 0.7218, Validation Accuracy: 0.4932\n",
      "Epoch [5/10], Loss: 0.1680, Training Accuracy: 0.7518, Validation Accuracy: 0.4892\n",
      "Epoch [6/10], Loss: 0.1420, Training Accuracy: 0.7920, Validation Accuracy: 0.4742\n",
      "Epoch [7/10], Loss: 0.1177, Training Accuracy: 0.7675, Validation Accuracy: 0.4422\n",
      "Epoch [8/10], Loss: 0.0941, Training Accuracy: 0.8543, Validation Accuracy: 0.4702\n",
      "Epoch [9/10], Loss: 0.0824, Training Accuracy: 0.8777, Validation Accuracy: 0.4637\n",
      "Epoch [10/10], Loss: 0.0728, Training Accuracy: 0.8865, Validation Accuracy: 0.4717\n",
      "new config\n",
      "{'layers': 1, 'filters': 84}\n",
      "Epoch [1/10], Loss: 0.2434, Training Accuracy: 0.5335, Validation Accuracy: 0.4487\n",
      "Epoch [2/10], Loss: 0.2252, Training Accuracy: 0.6350, Validation Accuracy: 0.4787\n",
      "Epoch [3/10], Loss: 0.2049, Training Accuracy: 0.6895, Validation Accuracy: 0.4717\n",
      "Epoch [4/10], Loss: 0.1829, Training Accuracy: 0.6948, Validation Accuracy: 0.4517\n",
      "Epoch [5/10], Loss: 0.1551, Training Accuracy: 0.7182, Validation Accuracy: 0.4282\n",
      "Epoch [6/10], Loss: 0.1193, Training Accuracy: 0.7378, Validation Accuracy: 0.4107\n",
      "Epoch [7/10], Loss: 0.0828, Training Accuracy: 0.8657, Validation Accuracy: 0.4672\n",
      "Epoch [8/10], Loss: 0.0742, Training Accuracy: 0.8015, Validation Accuracy: 0.4332\n",
      "Epoch [9/10], Loss: 0.0698, Training Accuracy: 0.9085, Validation Accuracy: 0.4882\n",
      "Epoch [10/10], Loss: 0.0617, Training Accuracy: 0.9248, Validation Accuracy: 0.4652\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0.4747373686843422, 5, {'layers': 2, 'filters': 110}),\n",
       " (0.4932466233116558, 4, {'layers': 1, 'filters': 26}),\n",
       " (0.4882441220610305, 9, {'layers': 1, 'filters': 84})]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_search(3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
