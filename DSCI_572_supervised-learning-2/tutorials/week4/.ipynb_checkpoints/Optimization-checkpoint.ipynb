{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization - Supervised Learning II - MDS Computational Linguistics\n",
    "\n",
    "### Tutorial Topics\n",
    "- Regularization\n",
    "- Training Optimization\n",
    "- Hyperparameter Tuning\n",
    "- BiLSTM\n",
    "\n",
    "### Software Requirements\n",
    "- Python (>=3.6)\n",
    "- PyTorch (>=1.2.0) \n",
    "- Jupyter (latest)\n",
    "\n",
    "## Getting Started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all the necessary imports\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch\n",
    "import torchtext\n",
    "from torchtext.data import Field, LabelField\n",
    "from torchtext.data import TabularDataset\n",
    "from torchtext.data import Iterator, BucketIterator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this tutorial we will be using the HappyDB dataset (also used in assignment 3) to showcase how you can fine-tune your neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Regularization\n",
    "\n",
    "``Regularization`` refers to techniques you can apply to machine learning algorithms to counteract the impact of ``overfitting``. One way to do this is by giving a penalty to model parameters. There are a number of different ways to do this but we'll mainly be looking at the two main types of regularization, *L1* and *L2* regularization, as well as a technique called *dropout* which is a neural-network specific style of regularization. We will also talk briefly about *l0* regularization, but implementing it is left up to you. \n",
    "\n",
    "## Model Capacity\n",
    "\n",
    "An important concept you need to be aware of is that of ``model capacity``. While there are theoritical definitions of what model capacity is, you can generally think of it as the number of hypotheses (hypothesis space) a model can select. In deep learning, we can increase model capacity by increaing for example (1) ``depth``, by adding more hidden layers and (2) ``width``, by adding more nodes to the network hidden layers. A higher capacity model can fit very complex functions (find complicated solutions). Note: ``*Higher capacity models are more prone to overfitting, while lower capacity models can underfit*``.\n",
    "\n",
    "### L2 Regularization\n",
    "\n",
    "The most popular regularization technique in machine learning, L2 regularization, puts some penalty on model features based on the squared weights of these features, calculated using the L2-norm:  $\\frac{\\lambda}{2}||w||_2^2=\\frac{\\lambda}{2}\\sum_i |w_i|^2$. The amount of regularization we apply to our model is a *hyperparameter* value indicated by $ \\lambda$, some positive value (e.g., $0.001$) that will need to be optmized.\n",
    "\n",
    "The goal of using a squared penalty on the value of the weights is to discourage models that have very large values for any particular weights, this in turn means that it's less likely to overfit to the particular patterns (features that has such highest features) it has seen in the training data.\n",
    "\n",
    "The only major downside occurs in situations where your data has lots of *exteraneous* features, in the case where you may have lots of garbage features mixed in with your data (much greater than the number of features that are useful for your problem) it may still try to use these features, leading to suboptimal results.\n",
    "\n",
    "### Using L2 Regularization in Pytorch\n",
    "\n",
    "In Pytorch L2 regularization is implemented through the inclusion *weight decay*, the name Pytorch uses to refer to ``L2 penalty``. Typically, when you set the value of ``weight_decay`` to be $>0$, L2 regularization will be used by the model. This value of Weight Decay is related to the lambda in the L2 regularization equation above, through the following relationship:  $Weight Decay = 2*\\lambda$ (see:[here](https://bbabenko.github.io/weight-decay/) for the nitty gritty). The following is an example of setting the value of ``weight_decay``: [Note we haven't declared a model yet, we'll run these things together at the end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), weight_decay = 0.001)\n",
    "#So easy!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When to use L2 Regularization\n",
    "\n",
    "Generally speaking L2 regularization is going to almost always help reduce overfitting in your problems. L2 regularization is also a popular method especially outside deep learning. For deep learning, ``dropout`` (introduced later) is currently a more popular approach to regularization and it does work well. You can also combine both L2 regularization and ``droupout``, although you don't usually need to.\n",
    "\n",
    "The one area where L2 regularization might not quite be good enough, is as mentioned above, when you have really noisy data that includes a lot of irrelevant features. Instead, you might want to consider...\n",
    "\n",
    "### L1 Regularization\n",
    "\n",
    "*L1 regularization* functions similarly to L2 regularization, except it gives a linear penalty to the model weights using the L1 norm:  $\\lambda ||w||_1 = \\lambda \\sum_i |w_i|$. The immediate upside of L1 regularization is that it can ignore exteranous features. By giving equal emphasis to all weight values, it will spend as much time trying to decrease the size of small weights as big weights, to the extent that it can shrink weight values to 0! Essentially this allows your model to perform automatic feature selection, getting rid of features that don't help (at least as much) solve the problem and leaving you with sparse weight matrices.\n",
    "\n",
    "As a wonderful side effect of having the sparse matrices, this can (1) spead up training (due to less computations) and (2) decrease memory needed for storing your model. In addition, (3) L1-selected features (i.e., those not set to ``zero`` are ones that you can view as *relevant* and hence you can use them for ``iterpretability`` (i.e., as a way to interpret why your model makes the decision it makes). This is important as a way to *see through* our models and help explain them, including to the general public. Increasingly, this is becoming more important over time as AI and its applications become more pervasive in our lives. \n",
    "\n",
    "### L1 Regularization in Pytorch\n",
    "\n",
    "While Pytorch implements L1 Loss (an alternative to MSE loss that is used to deal with outliers in your dataset), it does not implement L1 regularization. That said it's somewhat straightforward to plug it into your models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here l1 is the total of the l1 norm of your paramters, and lmbd is your value for the l1 lambda (some positive value).\n",
    "l1 = torch.tensor(0.)\n",
    "for name, param in model.named_parameters():\n",
    "    if 'weight' in name:\n",
    "        l1 += torch.norm(param, p=1)\n",
    "total_loss += cur_loss.item() + lmbd*l1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L0 Regularization\n",
    "\n",
    "Finally, one thing you might come across is something called \"L0\" regularization, similar to L1 and L2 regularization, it applies some penalty based on the weights, but this time it applies the penalty based on the *number of non-zero weigths*. This means it will try really hard to set your weight values to zero, and is an even stronger form of feature selection in comparison to L1 regularization. You can implement it similar to L1 above, but instead sum up the number of non-zero elements.\n",
    "\n",
    "#### AKA\n",
    "\n",
    "For your info: the L1 and L2 regularization techniques also have some other names they are refered to, sometimes people call L1 regularization ``LASSO`` and L2: ``Ridge`` regularization.\n",
    "\n",
    "\n",
    "### Dropout\n",
    "\n",
    "For a neural network specific regularization approach, dropout is a way to spread out what each node is learning. It does this by \"dropping\" some of the nodes from being used during an iteration of gradient descent, without those nodes, the rest of the nodes will then have to pick up the slack, thereby spreading out which nodes are used to understand different aspects of a problem. Dropout also has some really interesting properties when it comes to training, we'll look at this later in the tutorial. \n",
    "\n",
    "Ultimately, using dropout means that you are actually training several networks, rather then a single network, and somewhat using an ensemble of these networks to predict. This is particulary useful since it `forces` your model to distribute its dependence to more nodes by preventing ``co-adaptation`` between only particular nodes. In spite of its simplicity, ``dropout` is elegant and effective (as stated earlier).\n",
    "\n",
    "### Dropout in Pytorch\n",
    "\n",
    "For ``LSTMs`` and ``RNNs`` dropout is dead simple [see LSTM documentation](https://pytorch.org/docs/stable/nn.html?highlight=lstm#torch.nn.LSTM):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#just add \"dropout=\" and the value you want to apply [the probability of dropping any given unit (0,1)]\n",
    "self.rnn_lstm = nn.LSTM(input_size=embedding_size, dropout=.7, hidden_size=hidden_size, num_layers=num_layers)\n",
    "self.rnn_layer = nn.RNN(input_size=embedding_size, dropout=.7, hidden_size=hidden_size, num_layers=num_layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For ``feedforward`` neural networks add a dropout layer and then call this before the linear layer in your forward function ([see documentation](https://pytorch.org/docs/stable/nn.html?highlight=dropout#torch.nn.Dropout)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### inside model definition:\n",
    "def __init__(self, num_inputs, hidden_layers, num_outputs):\n",
    "    self.droplayer = nn.Dropout(p=.5)\n",
    "    self.ll = nn.Linear(input_dim, hidden_layer)\n",
    "\n",
    "def forward(self,x):\n",
    "    #### somwhere in your forward function...\n",
    "    out = self.droplayer(out)\n",
    "    out = self.ll(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Words on Regularization\n",
    "\n",
    "All of these approaches can be combined in interesting ways, you for instance could do L0, L1, L2 and Dropout all at once, will it work better than other approaches? Only way to find out is through trying and then tuning the hyperparamenters.\n",
    "\n",
    "Another thing to also consider is that while their main purpose is to improve generalizeability of ML models, they can have interesting other effects on your network, L1 and L0 regularization improve the training time because of their properties of creating sparse matrices. Dropout does a similar thing. Don't underlook the value of these techniques in terms of saving time in the training/validating/testing process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 Optimization\n",
    "\n",
    "Alongside using regularization, there are a number of different ways to improve the training process of your network. In ``Assignment 2``, we already looked at the impact of changing your ``learning rate`` on the performace of neural networks. This still is important, but there are some other methods you can use to improve (e.g., speed up) your gradient descent technique.\n",
    "\n",
    "### Momentum\n",
    "\n",
    "The classic approach to improving your training procedure is to introduce ``momentum`` into your SGD procedure. All this means is that a certain percentage of forward momentum from one iteration of SGD will be carried over to the next iteration. Why would you want to do this? Sometimes you can get stuck in local minimum before finding a global minimum, momentum will push you out of these local minimums and help your network converge on optimal configurations faster. For more information, you can check Section 8.3.2 [https://www.deeplearningbook.org/contents/optimization.html](here).\n",
    "\n",
    "#### Momentum in Pytorch\n",
    "\n",
    "Dead simple, add \"momentum=\" into your SGD function [SGD documentation](https://pytorch.org/docs/stable/optim.html?highlight=sgd#torch.optim.SGD):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Common values for Momentum include .99, .9, .5, but try things out yourself!\n",
    "\n",
    "### RMSProp\n",
    "\n",
    "``Root Mean Squared Propogation`` is another common technique. It basically changes the gradient accumulation into an exponentially weighted moving average. We won't get into the details but you can check Section 8.5.2 [https://www.deeplearningbook.org/contents/optimization.html](here) for more info.\n",
    "\n",
    "#### RMSProp in Pytorch\n",
    "Just switch out your SGD optimizer for RMSprop. RMSprop needs to set an **alpha** value which indicates how much smoothing you want to apply between different iteration of the algorithm. And similar to SGD you can add momentum and weight decay if you want. [Documenation on RMSProp](https://pytorch.org/docs/stable/optim.html?highlight=rmsprop#torch.optim.RMSprop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.RMSprop(model.parameters(), lr=LEARNING_RATE, alpha=0.99)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adam\n",
    "\n",
    "Adam is somewhat a combination of ``RMSprop`` and ``momentum`` (with some distinctions, see Section 8.5.3 [https://www.deeplearningbook.org/contents/optimization.html](here)). As such, it is a fast optimizer.\n",
    "\n",
    "### Adam in Pytorch\n",
    "Similarly to RMSProp, it has parameters that you can change to impact its performance, Adam has two of these parameters, called the **beta** values, one to impact the averaging it uses for the gradient, and the other the square of the gradient (respectively). [Adam documentation](https://pytorch.org/docs/stable/optim.html#torch.optim.Adam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(),betas=(0.9, 0.999),lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Last Thoughts on Optimization\n",
    "\n",
    "Generally, although other optimizers have more recently been proposed, ``Adam`` and ``SGD + momentum`` are currently probably the two most popular optimizers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3 Hyperparameter Tuning\n",
    "\n",
    "We've got a lot of variables to play with, and finding the best combination is a matter of experimentation. Generally you will want to use a validation set, or even use something like cross-validation, to experiment with different configurations of hyper parameters and then decide on the best values.\n",
    "\n",
    "Hyperparameters can influence each other greatly. If you could test these out independently, things would just be a matter of finding the best learning rate, the best weight-decay, what optimizer etc, and then piecing them all together. This sadly isn't the case. You'll need to cover the grid of all possible combinations to see which values are going to work best. Usually an approach of picking a set of values over an interval and then running your models over and over again is the best way to find your best model. This is called a grid search, and generally ML papers will report what ranges they tested when reporting their results. \n",
    "\n",
    "For now, you could simply choose a set of 3 reasonable values of each of these most important hyperparameters and try these. You can find what a reasonable value is in research papers, or textbooks (such as the links from the Deep Learning book by Goodfellow et al. (2016) above). Over time, you will gain experience as to what to try.\n",
    "\n",
    "### Hyperparemeter Tuning with an LSTM\n",
    "\n",
    "Back to the HappyDB example, let's test out these different configurations and see how they do. At your table pick which parameters you want to try out and run the training code with them. I've included my example run outputs at the bottom to give you a sense of what to expect.\n",
    "\n",
    "#### Parameters:\n",
    "- ``Model features`` (in our case just words)\n",
    "\n",
    "#### Hyperparameters:\n",
    "- ``Learning Rate``\n",
    "- ``Optimizer`` (Type and values of Optimizer)\n",
    "- ``Regularization`` (Type and values of Regularizer)\n",
    "- ``Batch Size``\n",
    "- ``Epochs`` (Number of epochs)\n",
    "\n",
    "#### BiLSTM Tangent\n",
    "\n",
    "In the model below I've also shown how to turn your uni-directional LSTM into a bi-directional LSTM (reading the sentence forward AND backwards). This is useful for certain types of tasks, such as Named Entity Recognition, and generally might learn a little faster. One note on changing from uni to BiLSTM, is that BiLSTM output dimensions will be double the output dimensions from the uni-directional LSTM (you literally run it twice, once forward and once backwards and then concatenate  the two results together to get passed on). Make sure whatever is receiving the output from the BiLSTM is set to double the dimensions for their input!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the seed\n",
    "manual_seed = 123\n",
    "torch.manual_seed(manual_seed)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "n_gpu = torch.cuda.device_count()\n",
    "if n_gpu > 0:\n",
    "    torch.cuda.manual_seed(manual_seed)\n",
    "\n",
    "def whitespace_tokenize(text):\n",
    "    return text.strip().split()\n",
    "\n",
    "TEXT = Field(sequential=True, tokenize=whitespace_tokenize, lower=False)\n",
    "LABEL = Field(sequential=False, unk_token = None)\n",
    "train, val, test = TabularDataset.splits(\n",
    "               path=\"./data/happy_db/\", # the root directory where the data lies\n",
    "               train='train.tsv', validation=\"dev.tsv\", test=\"test.tsv\", # file names\n",
    "               format='tsv',\n",
    "               skip_header=True, # if your tsv file has a header, make sure to pass this to ensure it doesn't get proceesed as data!\n",
    "               fields=[('tweet', TEXT), ('label', LABEL)])\n",
    "TEXT.build_vocab(train, max_size=5000)\n",
    "LABEL.build_vocab(train)\n",
    "train_iter, val_iter, test_iter = BucketIterator.splits(\n",
    " (train, val, test), # we pass in the datasets we want the iterator to draw data from\n",
    " batch_sizes=(32,32,32),\n",
    " sort_key=lambda x: len(x.tweet), \n",
    " sort=True,\n",
    "# A key to use for sorting examples in order to batch together examples with similar lengths and minimize padding. \n",
    " sort_within_batch=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMmodel(nn.Module):\n",
    "  \n",
    "  def __init__(self, embedding_size, vocab_size, output_size, hidden_size, num_layers):\n",
    "    # In the constructor we define the layers for our model (same as our previous RNN)\n",
    "    super(LSTMmodel, self).__init__()\n",
    "    # word embedding lookup table\n",
    "    self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_size)\n",
    "    self.embedding.weight.data.normal_(0.0,0.05) # mean=0.0, mu=0.05\n",
    "\n",
    "    \n",
    "    # core LSTM module\n",
    "    #UPDATE DROPOUT HERE\n",
    "\n",
    "    #baseline:\n",
    "    #self.lstm_rnn = nn.LSTM(input_size=embedding_size, hidden_size=hidden_size, num_layers=num_layers) # input_size, hidden_size, num_layers\n",
    "\n",
    "    #with droptout:\n",
    "    self.lstm_rnn = nn.LSTM(input_size=embedding_size, dropout=.7, hidden_size=hidden_size, num_layers=num_layers) # input_size, hidden_size, num_layers\n",
    "\n",
    "    #BiLSTM tangent: For the HW you might also try a bidirectional LSTM architecture, this is good for certain tasks such as\n",
    "    #Named entity recognition. Just add bidirectional=true to your LSTM parameters list. NOTE THIS DOUBLES THE OUTPUT DIMENSION!\n",
    "    #self.lstm_rnn = nn.LSTM(input_size=embedding_size, bidirectional=True, dropout=.7, hidden_size=hidden_size, num_layers=num_layers) # input_size, hidden_size, num_layers\n",
    "\n",
    "    \n",
    "    self.activation_fn = nn.Sigmoid()\n",
    "    self.linear_layer = nn.Linear(hidden_size, output_size) \n",
    "\n",
    "    #For BiLSTM you end up doubling the output from the LSTM (concatenating the two directions together)\n",
    "    #You'll need to double the input dimension into the linear layer to compensate!\n",
    "    #self.linear_layer = nn.Linear(hidden_size*2, output_size) \n",
    "\n",
    "\n",
    "\n",
    "    self.softmax_layer = nn.LogSoftmax(dim=0)\n",
    "  \n",
    "  def forward(self, x):\n",
    "    # In the forward function we define the forward propagation logic\n",
    "    out = self.embedding(x)\n",
    "    out, (h_state, c_state) = self.lstm_rnn(out) # h_0 initialized to zeros by default\n",
    "    # classify based on the hidden representation at the last token\n",
    "    out = out[-1] # unsqueeze converts 1D input (D dimension) into 2D input (1xD) \n",
    "    out = self.linear_layer(out)\n",
    "    out = self.softmax_layer(out) # accepts 2D or more dimensional inputs\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here \n",
    "\n",
    "EMBEDDING_SIZE = 300 \n",
    "VOCAB_SIZE = 5002\n",
    "NUM_CLASSES = 2\n",
    "HIDDEN_SIZE = 500\n",
    "NUM_LAYERS = 2\n",
    "model = LSTMmodel(EMBEDDING_SIZE, VOCAB_SIZE, NUM_CLASSES, HIDDEN_SIZE, NUM_LAYERS)\n",
    "model = model.to(device)\n",
    "print(model)\n",
    "\n",
    "LEARNING_RATE = 0.1\n",
    "criterion = nn.NLLLoss()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def train(loader):\n",
    "    total_loss = 0.0\n",
    "    # iterate throught the data loader\n",
    "    num_sample = 0\n",
    "    for batch in loader:\n",
    "        # load the current batch\n",
    "        batch_input = batch.tweet\n",
    "        batch_output = batch.label\n",
    "        \n",
    "        batch_input = batch_input.to(device)\n",
    "        batch_output = batch_output.to(device)\n",
    "        # forward propagation\n",
    "        # pass the data through the model\n",
    "        model_outputs = model(batch_input)\n",
    "        # compute the loss\n",
    "        \n",
    "        cur_loss = criterion(model_outputs, batch_output)\n",
    "        \n",
    "        #l1_pen = torch.tensor(0.)\n",
    "        #lambda_1 = 0.00001\n",
    "        #for name, param in model.named_parameters():\n",
    "        #    if 'weight' in name:\n",
    "        #        l1_pen += torch.norm(param, p=1)\n",
    "        \n",
    "        total_loss += cur_loss.item()# + len(batch)*lambda_1*l1_pen\n",
    "\n",
    "        # backward propagation (compute the gradients and update the model)\n",
    "        # clear the buffer\n",
    "        optimizer.zero_grad()\n",
    "        # compute the gradients\n",
    "        cur_loss.backward()\n",
    "        # update the weights\n",
    "        optimizer.step()\n",
    "\n",
    "        num_sample += batch_output.shape[0]\n",
    "    return total_loss/num_sample\n",
    "\n",
    "# evaluation logic based on classification accuracy\n",
    "def evaluate(loader):\n",
    "    all_pred=[]\n",
    "    all_label = []\n",
    "    with torch.no_grad(): # impacts the autograd engine and deactivate it. reduces memory usage and speeds up computation\n",
    "        for batch in loader:\n",
    "             # load the current batch\n",
    "            batch_input = batch.tweet\n",
    "            batch_output = batch.label\n",
    "\n",
    "            batch_input = batch_input.to(device)\n",
    "            # forward propagation\n",
    "            # pass the data through the model\n",
    "            model_outputs = model(batch_input)\n",
    "            # identify the predicted class for each example in the batch\n",
    "            probabilities, predicted = torch.max(model_outputs.cpu().data, 1)\n",
    "            # put all the true labels and predictions to two lists\n",
    "            all_pred.extend(predicted)\n",
    "            all_label.extend(batch_output)\n",
    "            \n",
    "    accuracy = accuracy_score(all_label, all_pred)\n",
    "    f1score = f1_score(all_label, all_pred, average='macro') \n",
    "    return accuracy,f1score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_seed = 123\n",
    "torch.manual_seed(manual_seed)\n",
    "if n_gpu > 0:\n",
    "    torch.cuda.manual_seed(manual_seed)\n",
    "\n",
    "#Play around with the optimizer, L2 regularization, and momentum here.\n",
    "#optimizer = torch.optim.RMSprop(model.parameters(),lr=LEARNING_RATE, alpha=0.99)\n",
    "#optimizer = torch.optim.Adam(model.parameters(),lr=LEARNING_RATE, betas=(0.9,0.99))\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=.9)\n",
    "#optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE)\n",
    "#optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=.9)\n",
    "#optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE, weight_decay=.00001)\n",
    "\n",
    "# start the training\n",
    "for epoch in range(30):\n",
    "    # train the model for one pass over the data\n",
    "    train_loss = train(train_iter)  \n",
    "    # compute the training accuracy\n",
    "    train_acc,f1t = evaluate(train_iter)\n",
    "    # compute the validation accuracy\n",
    "    val_acc,f1v = evaluate(val_iter)\n",
    "    \n",
    "    # print the loss for every epoch\n",
    "    print('Epoch [{}/{}], Loss: {:.4f}, Training Accuracy: {:.4f}, Validation Accuracy: {:.4f}'.format(epoch+1, 30, train_loss, train_acc, val_acc))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some baseline results I got for comparison. (Formatting looks fine in Jupyter notebook if you have issues reading it)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Baseline:\n",
    "Epoch [1/30], Loss: 0.1083, Training Accuracy: 0.5974, Validation Accuracy: 0.5682\n",
    "Epoch [2/30], Loss: 0.1083, Training Accuracy: 0.6161, Validation Accuracy: 0.5814\n",
    "Epoch [3/30], Loss: 0.1083, Training Accuracy: 0.6238, Validation Accuracy: 0.5928\n",
    "Epoch [4/30], Loss: 0.1083, Training Accuracy: 0.6291, Validation Accuracy: 0.6061\n",
    "Epoch [5/30], Loss: 0.1083, Training Accuracy: 0.6349, Validation Accuracy: 0.6335\n",
    "Epoch [6/30], Loss: 0.1083, Training Accuracy: 0.6399, Validation Accuracy: 0.6411\n",
    "Epoch [7/30], Loss: 0.1082, Training Accuracy: 0.6435, Validation Accuracy: 0.6420\n",
    "Epoch [8/30], Loss: 0.1082, Training Accuracy: 0.6489, Validation Accuracy: 0.6496\n",
    "Epoch [9/30], Loss: 0.1081, Training Accuracy: 0.6626, Validation Accuracy: 0.6534\n",
    "Epoch [10/30], Loss: 0.1070, Training Accuracy: 0.7501, Validation Accuracy: 0.7377\n",
    "Epoch [11/30], Loss: 0.1023, Training Accuracy: 0.8318, Validation Accuracy: 0.8106\n",
    "Epoch [12/30], Loss: 0.1005, Training Accuracy: 0.8612, Validation Accuracy: 0.8153\n",
    "Epoch [13/30], Loss: 0.0995, Training Accuracy: 0.8738, Validation Accuracy: 0.8144\n",
    "Epoch [14/30], Loss: 0.0989, Training Accuracy: 0.8902, Validation Accuracy: 0.8258\n",
    "Epoch [15/30], Loss: 0.0982, Training Accuracy: 0.8529, Validation Accuracy: 0.7689\n",
    "Epoch [16/30], Loss: 0.0978, Training Accuracy: 0.8745, Validation Accuracy: 0.8078\n",
    "Epoch [17/30], Loss: 0.0974, Training Accuracy: 0.8484, Validation Accuracy: 0.7670\n",
    "Epoch [18/30], Loss: 0.0970, Training Accuracy: 0.9020, Validation Accuracy: 0.8400\n",
    "Epoch [19/30], Loss: 0.0966, Training Accuracy: 0.9272, Validation Accuracy: 0.8693\n",
    "Epoch [20/30], Loss: 0.0962, Training Accuracy: 0.9264, Validation Accuracy: 0.8580\n",
    "Epoch [21/30], Loss: 0.0958, Training Accuracy: 0.9247, Validation Accuracy: 0.8542\n",
    "Epoch [22/30], Loss: 0.0956, Training Accuracy: 0.9358, Validation Accuracy: 0.8741\n",
    "Epoch [23/30], Loss: 0.0953, Training Accuracy: 0.9382, Validation Accuracy: 0.8722\n",
    "Epoch [24/30], Loss: 0.0951, Training Accuracy: 0.9387, Validation Accuracy: 0.8674\n",
    "Epoch [25/30], Loss: 0.0949, Training Accuracy: 0.9450, Validation Accuracy: 0.8797\n",
    "Epoch [26/30], Loss: 0.0947, Training Accuracy: 0.9076, Validation Accuracy: 0.8210\n",
    "Epoch [27/30], Loss: 0.0944, Training Accuracy: 0.9464, Validation Accuracy: 0.8788\n",
    "Epoch [28/30], Loss: 0.0944, Training Accuracy: 0.9439, Validation Accuracy: 0.8703\n",
    "Epoch [29/30], Loss: 0.0940, Training Accuracy: 0.9505, Validation Accuracy: 0.8788\n",
    "Epoch [30/30], Loss: 0.0938, Training Accuracy: 0.9418, Validation Accuracy: 0.8693\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
