{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feedforward Neural Networks - Supervised Learning II - MDS Computational Linguistics\n",
    "\n",
    "### Goal of this tutorial\n",
    "- Implement single layer and multilayer neural networks for sentiment analysis\n",
    "- Implement word embeddings based on word2vec\n",
    "\n",
    "### General\n",
    "- This notebook was last tested on Python 3.6.9, PyTorch 1.2.0 and sklearn 0.21.3\n",
    "\n",
    "We would like to acknowledge the following materials which helped as a reference in preparing this tutorial:\n",
    "- https://github.com/UBC-NLP/dlnlp2019/blob/master/slides/feedforward_nets.pdf\n",
    "- https://github.com/UBC-NLP/dlnlp2019/blob/master/slides/word2vec.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feedforward neural networks\n",
    "\n",
    "Neural networks are a family of classifiers which can model **non-linear decision boundaries**. \n",
    "\n",
    "Recommended reading for the theory behind neural networks: https://github.com/UBC-NLP/dlnlp2019/blob/master/slides/feedforward_nets.pdf\n",
    "\n",
    "We will first define the practical task that we are going to deal with in this tutorial.\n",
    "\n",
    "In this tutorial, we will focus only on sentiment analysis task. Specifically, we focus on classifying the sentiment of the tweet. We make use of the dataset provided by ``SemEval-2016 Task 4 on Sentiment Analysis on Twitter`` (http://alt.qcri.org/semeval2016/task4/). We focus on the subtask A which is coined as **message polarity classification task**. In this task, given a tweet, we need to predict whether the tweet is of **positive, negative or neutral sentiment**. We have 6,000, 1,999 and 20,632 tweets in train set, validation set and test set respectively. We have already preprocessed (tokenization, removing URLs, mentions, hashtags and so on) the tweets and placed it under ``data/sentiment-twitter-2016-task4`` folder in three files as ``train.tsv``, ``dev.tsv`` and ``test.tsv``. Some example tweets include:\n",
    "\n",
    "| class index | class name | tweet example |\n",
    "| ----------------- | ----------- |-------------|\n",
    "| 0  | Negative   | --MENTION-- --MENTION-- the reason i ask is because it may be the manufacturer's fault and they could help you |\n",
    "| 1  | Neutral | just ordered my ever tablet --MENTION-- surface pro --DIGIT-- ssd hopefully it works out for dev to replace my laptop |\n",
    "| 2  | Positive | dear --MENTION-- the newooffice for mac is great and all but no lync update c'mon |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all the necessary imports\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "\n",
    "# set the seed for reproducibility\n",
    "manual_seed = 123\n",
    "torch.manual_seed(manual_seed)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_gpu = torch.cuda.device_count()\n",
    "if n_gpu > 0:\n",
    "    torch.cuda.manual_seed(manual_seed)\n",
    "\n",
    "# hyperparameters\n",
    "BATCH_SIZE = 5 # size of the mini-batch for training\n",
    "MAX_EPOCHS = 15 # maximum no. of passes over the training data\n",
    "LEARNING_RATE = 0.5 # learning rate\n",
    "MAX_FEATURES = 5000 # x_j, the number of j, maximum number of features to represent a tweet (or) size of the tweet feature vector\n",
    "\n",
    "# other parameters\n",
    "NUM_CLASSES = 3\n",
    "\n",
    "# dataset paths\n",
    "DATA_FOLDER = \"data/sentiment-twitter-2016-task4\"\n",
    "TRAIN_FILE = DATA_FOLDER + \"/train.tsv\"\n",
    "VALID_FILE = DATA_FOLDER + \"/dev.tsv\"\n",
    "TEST_FILE = DATA_FOLDER + \"/test.tsv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extractor\n",
    "The training example is a **tweet**, which contains an ordered list of terms (e.g., words). We need to **vectorize** the tweet, that is, convert the tweet to a fixed-length vector to be fed as input to a ML model. We also call this vector as a **feature vector**. In this tutorial, we will utilize simple features based on **term frequency-inverse document frequency (tf-idf)** to represent a tweet. In our case a *document* is a *tweet* and a *word* is a *term*. There are two functions:\n",
    "- tf: A function for **'term frequency'** (basically counts, i.e., how many times the term occurred in our document) \n",
    "- idf:  A function for **'inverse document frequency'** (the total number of documents divided by the number of documents in which the term occurred).\n",
    "\n",
    "Tf-idf of a tweet $d$ is a $|V|$ dimensional feature vector, where each component corresponds to a term $t$ in the vocabulary V whose size is $|V|$. Tf-idf is formulated as,\n",
    "\n",
    "tf-idf(d,t) $ = (1 + \\log f_{d,t}) * \\log(1 + \\frac{N}{n_t})$\n",
    "\n",
    "where,\n",
    "- $f_{d,t}$ corresponds to the raw frequency of the term $t$ in tweet $d$\n",
    "- $N$ corresponds to the number of tweets in train set\n",
    "- $n_t$ corresponds to the number of tweets in train set that has the term $t$\n",
    "\n",
    "Note: There are variations of tf-idf, but here:\n",
    "- We take the log and **add one** to idf to avoid dividing by zero if the term never occurs in any document.\n",
    "- We also similarly add 1 and take the log of tf.\n",
    "Ultimately, we will let sklearn take care of [tf-idf construction](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html). \n",
    "\n",
    "Essentially, **high weight** in tf–idf feature vector is attained by a **high term frequency (in the given tweet) and a low tweet frequency of the term in the whole collection of tweets**. Hence, the weights tend to filter out common terms.\n",
    "\n",
    "Let us construct the tf-idf feature vectorizer from the tweets in train set. First, let us read the training corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of training records: 6000\n"
     ]
    }
   ],
   "source": [
    "# tool from sklearn library to compute tfidf of a document\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# function for loading file\n",
    "def read_corpus(file):\n",
    "    corpus = [] \n",
    "    for line in open(file):\n",
    "        content, label = line.strip().split(\"\\t\")\n",
    "        corpus.append(content)\n",
    "    return corpus\n",
    "\n",
    "# reads the train corpus\n",
    "train_corpus = read_corpus(TRAIN_FILE) \n",
    "\n",
    "# print the number of training records\n",
    "print(\"No. of training records: %d\"%(len(train_corpus)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us instantiate an object from the TfidfVectorizer class by specifying the maximum number of features. Other (optional) arguments to the class can be found in the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the vectorizer\n",
    "# builds a vocabulary that only considers the **top max_features** ordered by **term frequency** across the corpus.\n",
    "vectorizer = TfidfVectorizer(max_features=MAX_FEATURES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us fit the vectorizer with the training records, that is, compute $f_{d,t}$, $N$ and $n_t$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                input='content', lowercase=True, max_df=1.0, max_features=5000,\n",
       "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
       "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
       "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=None, use_idf=True, vocabulary=None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the vectorizer on train set\n",
    "vectorizer.fit(train_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us check the number of features our vectorizer will output for a given tweet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n"
     ]
    }
   ],
   "source": [
    "# print the number of features\n",
    "print(len(vectorizer.get_feature_names()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us print the feature names (term, in our case) corresponding to first 10 features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aa', 'aapl', 'abc', 'abigail', 'able', 'about', 'above', 'absence', 'absolute', 'absolutely']\n"
     ]
    }
   ],
   "source": [
    "# print feature names (term) corresponding to first 10 features\n",
    "print(vectorizer.get_feature_names()[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's important to note that the tf-idf representation of a tweet ignores the word order present in the tweet (this shallow feature representation might be limiting the performance of a model for many NLP tasks). In the latter part of the tutorial, we will look at word embeddings, a key concept in building the state-of-the-art deep learning models for NLP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataLoader\n",
    "\n",
    "Next, let us construct the dataloader for sentiment dataset. In the previous tutorial, we studied dataloader for loading regression datasets. Here again, we create a custom class **TweetSentimentDataset** that inherits the Dataset class and define the functionality of the constructor function (**__init__**),  get item function (**__getitem__**) and length function (**__len__**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new class inheriting torch.utils.data.Dataset\n",
    "class TweetSentimentDataset(Dataset):\n",
    "    \"\"\" sentiment-twitter-2016-task4 dataset.\"\"\"\n",
    "    def __init__(self, file, vectorizer):\n",
    "        # read the corpus\n",
    "        corpus, labels = [], []\n",
    "        for line in open(file):\n",
    "            content, label = line.strip().split(\"\\t\")\n",
    "            corpus.append(content)\n",
    "            labels.append(int(label))\n",
    "\n",
    "        # set the size of the corpus\n",
    "        self.n = len(corpus)\n",
    "\n",
    "        # vectorize all the tweets\n",
    "        features = vectorizer.transform(corpus)\n",
    "\n",
    "        # convert features and labels to torch.tensor\n",
    "        self.features = torch.from_numpy(features.toarray()).float()\n",
    "        self.features.to(device)\n",
    "        self.labels = torch.tensor(labels, device=device, requires_grad=False)\n",
    "\n",
    "    # return input and output of a single example\n",
    "    # Input: Feature vectors, where each vector corresponds to a tweet. \n",
    "    # Output: Labels, where each label is one index for each of our tags in the set {positive, negative, neutral}\n",
    "    def __getitem__(self, index):\n",
    "        return self.features[index], self.labels[index]\n",
    "\n",
    "    # return the total number of examples\n",
    "    def __len__(self):\n",
    "        return self.n\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now instantiate the loader for training, validation and test set. Note that we only shuffle the training dataset (hence, we set shuffle argument to True for creating training data loader)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the dataloader object\n",
    "train_loader = DataLoader(dataset=TweetSentimentDataset(TRAIN_FILE, vectorizer), batch_size=BATCH_SIZE, shuffle=True, num_workers=2) \n",
    "valid_loader = DataLoader(dataset=TweetSentimentDataset(VALID_FILE, vectorizer), batch_size=BATCH_SIZE, shuffle=False, num_workers=1)\n",
    "test_loader = DataLoader(dataset=TweetSentimentDataset(TEST_FILE, vectorizer), batch_size=BATCH_SIZE, shuffle=False, num_workers=1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us access a single batch from the training dataset and print the properties of the batch (such as input and output size):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input (sample batch) vector size:  torch.Size([5, 5000]) \n",
      "Golden label (sample batch) vector size:  torch.Size([5])\n"
     ]
    }
   ],
   "source": [
    "# iterate over the dataset for one epoch\n",
    "num_batches = 0\n",
    "for i, data in enumerate(train_loader, 0):\n",
    "    input, output = data\n",
    "    if i == 0:\n",
    "        print(\"Input (sample batch) vector size: \",input.size(), \"\\nGolden label (sample batch) vector size: \",output.size())\n",
    "    num_batches += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also check the number of batches (remember the batch size is set as 5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of batches:  1200\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of batches: \", num_batches) # prints the number of batches per epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **computational graph** for a single (hidden) layer neural network for classification that we will now build in this tutorial, looks like this:\n",
    "\n",
    "<img src=\"images/sl2_textclassification_neuralnets_cg.jpg\" alt=\"MLP\" title=\"Single Layer neural network - Computational Graph\" width=\"650\" height=\"450\"/>\n",
    "\n",
    "For our setting, **x** and **y** correspond to tweet feature vector and tweet sentiment label respectively. \n",
    "\n",
    "The LogSoftmax function is akin to Softmax function multiplied by **log** and has better numerical stability in practice. The documentation for LogSoftmax can be found [here](https://pytorch.org/docs/stable/generated/torch.nn.LogSoftmax.html).\n",
    "\n",
    "Let us look at an example for using **LogSoftmax** layer. First, let's create sample input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3.])\n"
     ]
    }
   ],
   "source": [
    "sample_input = torch.Tensor([1.0,2.0,3.0]) # 1x3\n",
    "print(sample_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, let's create a LogSoftmax layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "logsoftmax_layer = nn.LogSoftmax()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Third, let's pass the **sample_input** to the logsoftmax_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-2.4076, -1.4076, -0.4076])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/ctrl_transformers/lib/python3.6/site-packages/ipykernel_launcher.py:1: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "print(logsoftmax_layer(sample_input))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the output from logsoftmax_layer matches with multiplying the output from softmax_layer with log "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-2.4076, -1.4076, -0.4076])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/ctrl_transformers/lib/python3.6/site-packages/ipykernel_launcher.py:1: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "print(nn.Softmax()(sample_input).log())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's look at another module in the computational graph we've not seen before: **NLLLoss**\n",
    "\n",
    "The **NLLLoss** corresponds to negative log-likelihood loss, a commonly used loss to train a classification problem with fixed number of classes (in our setting, the number of classes is 3). The documentation for NLLLoss can be found [here](https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html).\n",
    "\n",
    "Like **MSLELoss**, **NLLLoss** takes the model prediction (class-wise log proababilities) and the ground truth (or target) (vector containing true class indices) and measures the degree to which the model prediction deviates from the ground truth.\n",
    "\n",
    "The loss function for a single example (input $x_n$, target $y_n$) is given by, $l = -w_{y_n} * x_{n,y_n}$, where $y_n$ represent the target class index, $x_{y_n}$ is the input dimension corresponding to the target class index and $w_{y_n}$ represent the weight corresponding to the target class (hyperparameter, default is 1 for all the classes. for imbalanced datasets, we might need to tune these weights so that mistakes in minority class can be adequately penalized).\n",
    "\n",
    "Let's see an example by creating the **NLLLoss** criterion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's pass the model prediction (output from logsoftmax) and ground truth (assuming the right class is 2 (0-indexed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_prediction =  tensor([[-2.4076, -1.4076, -0.4076]])\n",
      "target =  tensor([2])\n",
      "loss, l = tensor(0.4076)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/ctrl_transformers/lib/python3.6/site-packages/ipykernel_launcher.py:2: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# calculate the model prediction\n",
    "model_prediction = logsoftmax_layer(sample_input).unsqueeze(0) # add extra dimension to match layer syntax (check documentation)\n",
    "print(\"model_prediction = \", model_prediction)\n",
    "\n",
    "# store the ground truth for this example\n",
    "target = torch.tensor([2], dtype=torch.long)\n",
    "print(\"target = \", target)\n",
    "\n",
    "# pass the model prediction and the ground truth to the loss layer to compute l\n",
    "print(\"loss, l =\", criterion(model_prediction, target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, $l = -1.0 * -0.4076 = 0.4076$ \n",
    "\n",
    "Let us implement the single layer neural network in PyTorch.\n",
    "\n",
    "<img src=\"images/sl2_textclassification_neuralnets_cg.jpg\" alt=\"MLP\" title=\"Single Layer neural network - Computational Graph\" width=\"650\" height=\"450\"/>\n",
    "\n",
    "Let us start with defining the class for the main class for the neural network, defining the layers in the network along with the forward propagation logic.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "create a custom model class inheriting torch.nn.Module\n",
    "\"\"\"\n",
    "class SingleLayerNeuralNetworkModel(nn.Module):\n",
    "  \n",
    "  def __init__(self, num_inputs, hidden_layers, num_outputs, debug=False):\n",
    "    # In the constructor we define the layers for our model\n",
    "    super(SingleLayerNeuralNetworkModel, self).__init__()\n",
    "    self.input_to_hidden = nn.Linear(num_inputs, hidden_layers[0]) # includes W_input and bias_input \n",
    "    self.sigmoid_layer = nn.Sigmoid()  \n",
    "    self.hidden_to_output = nn.Linear(hidden_layers[0], num_outputs) # includes W_hidden and bias_hidden\n",
    "    self.softmax_layer = nn.LogSoftmax(dim=1)\n",
    "    self.debug = debug\n",
    "  \n",
    "  def forward(self, x):\n",
    "    # In the forward function we define the forward propagation logic\n",
    "    if self.debug:\n",
    "        print(\"input to hidden layer input (or x) shape = \", x.size())\n",
    "    out = self.input_to_hidden(x) # Wx + c\n",
    "    if self.debug:\n",
    "        print(\"input to hidden layer output shape = \", out.size())\n",
    "    out = self.sigmoid_layer(out) # h = g(Wx + c)\n",
    "    if self.debug:\n",
    "        print(\"sigmoid layer output (or hidden to output layer input) shape = \", out.size())\n",
    "    out = self.hidden_to_output(out) # W^T h + b\n",
    "    if self.debug:\n",
    "        print(\"logsoftmax layer input (or hidden to output layer output) shape = \", out.size())\n",
    "    out = self.softmax_layer(out) # out = softmax(W^T h + b)\n",
    "    if self.debug:\n",
    "        print(\"logsoftmax layer output shape = \", out.size())\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us define the training and evaluation logic. Most of the steps are similar to the steps we saw in the previous tutorial for regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train logic (similar to that of linear regression model)\n",
    "def train(loader):\n",
    "    total_loss = 0.0\n",
    "    # iterate throught the data loader\n",
    "    num_batches = 0\n",
    "    for batch in loader:\n",
    "        # load the current batch\n",
    "        batch_input, batch_output = batch\n",
    "\n",
    "        # forward propagation\n",
    "        # pass the data through the model\n",
    "        model_outputs = model(batch_input)\n",
    "        # compute the loss\n",
    "        cur_loss = criterion(model_outputs, batch_output)\n",
    "        total_loss += cur_loss.item()\n",
    "\n",
    "        # backward propagation (compute the gradients and update the model)\n",
    "        # clear the buffer\n",
    "        optimizer.zero_grad()\n",
    "        # compute the gradients\n",
    "        cur_loss.backward()\n",
    "        # update the weights\n",
    "        optimizer.step()\n",
    "\n",
    "        num_batches += 1\n",
    "    return total_loss/num_batches\n",
    "\n",
    "# evaluation logic based on classification accuracy\n",
    "def evaluate(loader):\n",
    "    accuracy, num_examples = 0.0, 0\n",
    "    with torch.no_grad(): # impacts the autograd engine and deactivate it. reduces memory usage and speeds up computation\n",
    "        for batch in loader:\n",
    "            # load the current batch\n",
    "            batch_input, batch_output = batch\n",
    "            # forward propagation\n",
    "            # pass the data through the model\n",
    "            model_outputs = model(batch_input)\n",
    "            # identify the predicted class for each example in the batch (row-wise)\n",
    "            _, predicted = torch.max(model_outputs.data, 1) # Returns a (values, indices) \n",
    "            # compare with batch_output (gold labels) to compute accuracy\n",
    "            accuracy += (predicted == batch_output).sum().item()\n",
    "            num_examples += batch_output.size(0)\n",
    "    return accuracy/num_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us set the important hyperparameter: number of hidden neurons (or units) in the hidden layer of the model.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameter of neural network\n",
    "hidden_layers = [50]  # set number of hidden features of the first and only hidden layer to 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us instantiate the model, create the criterion and optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model specs:  SingleLayerNeuralNetworkModel(\n",
      "  (input_to_hidden): Linear(in_features=5000, out_features=50, bias=True)\n",
      "  (sigmoid_layer): Sigmoid()\n",
      "  (hidden_to_output): Linear(in_features=50, out_features=3, bias=True)\n",
      "  (softmax_layer): LogSoftmax()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# instantiate a model based on the class and move it to the right device (cpu or gpu)\n",
    "model = SingleLayerNeuralNetworkModel(MAX_FEATURES, hidden_layers, NUM_CLASSES)\n",
    "model.to(device)\n",
    "print(\"Model specs: \", model) # prints the model properties\n",
    "\n",
    "# define the loss function (last node of the graph)\n",
    "criterion = nn.NLLLoss() # The negative log likelihood loss\n",
    "\n",
    "# create an instance of SGD with required hyperparameters\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us perform a feedforward pass of the neural network with a sample input and print layer outputs (carefully look at the shape of the intermediate outputs):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input to hidden layer input (or x) shape =  torch.Size([1, 5000])\n",
      "input to hidden layer output shape =  torch.Size([1, 50])\n",
      "sigmoid layer output (or hidden to output layer input) shape =  torch.Size([1, 50])\n",
      "logsoftmax layer input (or hidden to output layer output) shape =  torch.Size([1, 3])\n",
      "logsoftmax layer output shape =  torch.Size([1, 3])\n",
      "model prediction =  tensor([[-1.1530, -1.0574, -1.0878]])\n",
      "loss =  tensor(1.0878)\n"
     ]
    }
   ],
   "source": [
    "# toggle debug mode so that all intermediate layer outputs are printed out\n",
    "model.debug = True\n",
    "\n",
    "# create sample input and target\n",
    "sample_input = torch.Tensor(1, MAX_FEATURES) # random tensor of required number of features\n",
    "sample_target = torch.tensor([2], dtype=torch.long) # assuming the target class for this example is 2 (0-indexed)\n",
    "\n",
    "# perform feedforward propagation\n",
    "model_prediction = model(sample_input)\n",
    "print(\"model prediction = \", model_prediction.data)\n",
    "\n",
    "# calculate loss\n",
    "print(\"loss = \", criterion(model_prediction, sample_target).data)\n",
    "\n",
    "# turn off debug mode\n",
    "model.debug = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us start the training of our first neural network model (in addition to training loss, we will also print the training accuracy and validation accuracy): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/15], Loss: 1.1222, Training Accuracy: 0.5157,  Validation Accuracy: 0.4217\n",
      "Epoch [2/15], Loss: 1.0186, Training Accuracy: 0.5157,  Validation Accuracy: 0.4217\n",
      "Epoch [3/15], Loss: 0.9626, Training Accuracy: 0.6305,  Validation Accuracy: 0.5033\n",
      "Epoch [4/15], Loss: 0.8864, Training Accuracy: 0.5432,  Validation Accuracy: 0.4302\n",
      "Epoch [5/15], Loss: 0.8240, Training Accuracy: 0.5955,  Validation Accuracy: 0.4577\n",
      "Epoch [6/15], Loss: 0.7746, Training Accuracy: 0.7260,  Validation Accuracy: 0.5248\n",
      "Epoch [7/15], Loss: 0.7328, Training Accuracy: 0.4887,  Validation Accuracy: 0.4212\n",
      "Epoch [8/15], Loss: 0.6813, Training Accuracy: 0.7442,  Validation Accuracy: 0.4952\n",
      "Epoch [9/15], Loss: 0.6452, Training Accuracy: 0.8222,  Validation Accuracy: 0.5063\n",
      "Epoch [10/15], Loss: 0.5984, Training Accuracy: 0.8393,  Validation Accuracy: 0.5043\n",
      "Epoch [11/15], Loss: 0.5608, Training Accuracy: 0.7920,  Validation Accuracy: 0.4777\n",
      "Epoch [12/15], Loss: 0.5247, Training Accuracy: 0.8703,  Validation Accuracy: 0.5083\n",
      "Epoch [13/15], Loss: 0.4880, Training Accuracy: 0.6860,  Validation Accuracy: 0.4137\n",
      "Epoch [14/15], Loss: 0.4527, Training Accuracy: 0.8330,  Validation Accuracy: 0.4762\n",
      "Epoch [15/15], Loss: 0.4312, Training Accuracy: 0.8655,  Validation Accuracy: 0.4857\n"
     ]
    }
   ],
   "source": [
    "# start the training\n",
    "for epoch in range(MAX_EPOCHS):\n",
    "    # train the model for one pass over the data\n",
    "    train_loss = train(train_loader)\n",
    "    # compute the training accuracy\n",
    "    train_acc = evaluate(train_loader)\n",
    "    # compute the validation accuracy \n",
    "    val_acc = evaluate(valid_loader)\n",
    "    # print the loss for every epoch\n",
    "    \n",
    "    print('Epoch [{}/{}], Loss: {:.4f}, Training Accuracy: {:.4f},  Validation Accuracy: {:.4f}'.format(epoch+1, MAX_EPOCHS, train_loss, train_acc, val_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding more hidden layers\n",
    "\n",
    "Single layer neural network uses **one hidden layer**. A deep neural network typically stacks multiple hidden layers leading to good performance on many tasks. And the computational graph for a **2 layer neural network** for classification can look like this:\n",
    "\n",
    "<img src=\"images/sl2_textclassification_neuralnets_multi_cg.jpg\" alt=\"MLP\" title=\"2 Layer neural network - Computational Graph\" />\n",
    "\n",
    "Let us implement this two layer neural network in PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "create a custom model class inheriting torch.nn.Module\n",
    "\"\"\"\n",
    "class MultiLayerNeuralNetworkModel(nn.Module):\n",
    "  \n",
    "  def __init__(self, num_inputs, hidden_layers, num_outputs, debug=False):\n",
    "    # In the constructor we define the layers for our model\n",
    "    super(MultiLayerNeuralNetworkModel, self).__init__()\n",
    "    \n",
    "    self.modules = [] # stores all the layers for the neural network\n",
    "    input_dim = num_inputs\n",
    "    # add input layer followed by hidden layers\n",
    "    for hidden_layer in hidden_layers:\n",
    "      # add one layer followed by non-linearity (nn.Sigmoid)\n",
    "      self.modules.append(nn.Linear(input_dim, hidden_layer))\n",
    "      self.modules.append(nn.Sigmoid())\n",
    "      input_dim = hidden_layer\n",
    "    # add the classification module (output layer)\n",
    "    self.modules.append(nn.Linear(input_dim, num_outputs))\n",
    "    self.modules.append(nn.LogSoftmax(dim=1))\n",
    "    \n",
    "    # create the model from all the modules\n",
    "    self.model = nn.Sequential(*self.modules) # container of layers, for more details: https://pytorch.org/docs/stable/nn.html#torch.nn.Sequential\n",
    "    \n",
    "    # set the debug flag\n",
    "    self.debug = debug\n",
    " \n",
    "  def forward(self, x):\n",
    "    # In the forward function we define the forward propagation logic\n",
    "    if self.debug:\n",
    "        out = x\n",
    "        for li, layer in enumerate(self.modules[0:-2]):\n",
    "            if li == 0:\n",
    "                print(\"hidden layer 1 input (or x) shape = \", out.size())\n",
    "                out = layer(out)\n",
    "                print(\"hidden layer 1 output shape = \", out.size())\n",
    "            elif li%2 == 0:\n",
    "                print(\"hidden layer %d input shape = \"%(1+(li/2)), out.size())\n",
    "                out = layer(out)\n",
    "                print(\"hidden layer %d output shape = \"%(1+(li/2)), out.size())\n",
    "            else:\n",
    "                print(\"sigmoid layer %d input shape = \"%(1+(li/2)), out.size())\n",
    "                out = layer(out)\n",
    "                print(\"sigmoid layer %d output shape = \"%(1+(li/2)), out.size())\n",
    "        print(\"sigmoid layer output (or hidden to output layer input) shape = \", out.size())\n",
    "        out = self.modules[-2](out)\n",
    "        print(\"logsoftmax layer input (or hidden to output layer output) shape = \", out.size())\n",
    "        out = self.modules[-1](out)\n",
    "        print(\"logsoftmax layer output shape = \", out.size())\n",
    "        return out\n",
    "    out = self.model(x)\n",
    "    return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us set the important hyperparameter: number of hidden neurons (or units) in each hidden layer of the model.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameter of neural network\n",
    "hidden_layers = [50, 50]  # [num. of hidden units in first layer, num. of hidden units in second layer]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us instantiate the model, create the criterion and optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model specs:  MultiLayerNeuralNetworkModel(\n",
      "  (model): Sequential(\n",
      "    (0): Linear(in_features=5000, out_features=50, bias=True)\n",
      "    (1): Sigmoid()\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Sigmoid()\n",
      "    (4): Linear(in_features=50, out_features=3, bias=True)\n",
      "    (5): LogSoftmax()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# define the loss function (last node of the graph)\n",
    "model = MultiLayerNeuralNetworkModel(MAX_FEATURES, hidden_layers, NUM_CLASSES)\n",
    "model.to(device)\n",
    "print(\"Model specs: \", model)\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "# create an instance of SGD with required hyperparameters\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us perform a feedforward pass of the neural network with a sample input and print layer outputs (carefully look at the shape of the intermediate outputs):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden layer 1 input (or x) shape =  torch.Size([1, 5000])\n",
      "hidden layer 1 output shape =  torch.Size([1, 50])\n",
      "sigmoid layer 1 input shape =  torch.Size([1, 50])\n",
      "sigmoid layer 1 output shape =  torch.Size([1, 50])\n",
      "hidden layer 2 input shape =  torch.Size([1, 50])\n",
      "hidden layer 2 output shape =  torch.Size([1, 50])\n",
      "sigmoid layer 2 input shape =  torch.Size([1, 50])\n",
      "sigmoid layer 2 output shape =  torch.Size([1, 50])\n",
      "sigmoid layer output (or hidden to output layer input) shape =  torch.Size([1, 50])\n",
      "logsoftmax layer input (or hidden to output layer output) shape =  torch.Size([1, 3])\n",
      "logsoftmax layer output shape =  torch.Size([1, 3])\n",
      "model prediction =  tensor([[-1.2292, -1.1165, -0.9675]])\n",
      "loss =  tensor(0.9675)\n"
     ]
    }
   ],
   "source": [
    "# toggle debug mode so that all intermediate layer outputs are printed out\n",
    "model.debug = True\n",
    "\n",
    "# create sample input and target\n",
    "sample_input = torch.Tensor(1, MAX_FEATURES) # random tensor of required number of features\n",
    "sample_target = torch.tensor([2], dtype=torch.long) # assuming the target class for this example is 2 (0-indexed)\n",
    "\n",
    "# perform feedforward propagation\n",
    "model_prediction = model(sample_input)\n",
    "print(\"model prediction = \", model_prediction.data)\n",
    "\n",
    "# calculate loss\n",
    "print(\"loss = \", criterion(model_prediction, sample_target).data)\n",
    "\n",
    "# turn off debug mode\n",
    "model.debug = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us start the training of our multilayer neural network model (in addition to training loss, we will also print the training accuracy and validation accuracy): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/15], Loss: 1.0212, Training Accuracy: 0.5157, Validation Accuracy: 0.4217\n",
      "Epoch [2/15], Loss: 1.0044, Training Accuracy: 0.3405, Validation Accuracy: 0.3827\n",
      "Epoch [3/15], Loss: 1.0032, Training Accuracy: 0.5157, Validation Accuracy: 0.4217\n",
      "Epoch [4/15], Loss: 1.0043, Training Accuracy: 0.5157, Validation Accuracy: 0.4217\n",
      "Epoch [5/15], Loss: 1.0019, Training Accuracy: 0.5157, Validation Accuracy: 0.4217\n",
      "Epoch [6/15], Loss: 1.0019, Training Accuracy: 0.5157, Validation Accuracy: 0.4217\n",
      "Epoch [7/15], Loss: 1.0002, Training Accuracy: 0.5157, Validation Accuracy: 0.4217\n",
      "Epoch [8/15], Loss: 0.9952, Training Accuracy: 0.5157, Validation Accuracy: 0.4217\n",
      "Epoch [9/15], Loss: 0.9736, Training Accuracy: 0.5157, Validation Accuracy: 0.4217\n",
      "Epoch [10/15], Loss: 0.9281, Training Accuracy: 0.5567, Validation Accuracy: 0.4362\n",
      "Epoch [11/15], Loss: 0.8724, Training Accuracy: 0.6172, Validation Accuracy: 0.4937\n",
      "Epoch [12/15], Loss: 0.8304, Training Accuracy: 0.6478, Validation Accuracy: 0.5063\n",
      "Epoch [13/15], Loss: 0.7931, Training Accuracy: 0.5803, Validation Accuracy: 0.4432\n",
      "Epoch [14/15], Loss: 0.7615, Training Accuracy: 0.6943, Validation Accuracy: 0.5258\n",
      "Epoch [15/15], Loss: 0.7377, Training Accuracy: 0.7193, Validation Accuracy: 0.5103\n"
     ]
    }
   ],
   "source": [
    "# start the training\n",
    "for epoch in range(MAX_EPOCHS):\n",
    "  # train the model for one pass over the data\n",
    "  train_loss = train(train_loader)\n",
    "  # compute the training accuracy\n",
    "  train_acc = evaluate(train_loader)\n",
    "  # compute the validation accuracy \n",
    "  val_acc = evaluate(valid_loader)\n",
    "  # print the loss for every epoch\n",
    "  print('Epoch [{}/{}], Loss: {:.4f}, Training Accuracy: {:.4f}, Validation Accuracy: {:.4f}'.format(epoch+1, MAX_EPOCHS, train_loss, train_acc, val_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us shift gears and take a look at word embeddings, a key concept in building the state-of-the-art deep learning models. \n",
    "\n",
    "## Word Embeddings\n",
    "\n",
    "In this tutorial, we will focus on **word embeddings (or vectors)** generated by a [word2vec model](https://code.google.com/archive/p/word2vec/). Specifically, we will focus on **continuous bag of words (CBOW)** model. At its core, word2vec relies on the hypothesis that words which occur in **similar contexts** (defined by a select set of words before and after the word in the sentences in which the target word appears) tend to have **similar meaning** and therefore must have similar embeddings. CBOW model learns word embedding (of a certain size called **embedding size** which is a hyperparameter) by setting up an auxiliary task. The auxiliary task is to predict the word (target) given the surrounding words of the target word in a sentence as input. We typically consider **window size** (another hyperparameter) words before and after the target word as surrounding words (or context).\n",
    "\n",
    "The model architecture of CBOW based word2vec model from the [original paper](https://arxiv.org/pdf/1301.3781.pdf) is:\n",
    "\n",
    "<img src=\"images/sl2_textclassification_neuralnets_cbow.png\" alt=\"word2vec\" title=\"CBOW model - Model Architecture\" width=\"250\" height=\"100\" />\n",
    "\n",
    "For instance, consider our corpus has only one sentence **\"I had so much fun in my semester break\"**. If the window size is set to 2, then the train data for CBOW model corresponding to the auxiliary classification task looks like this:\n",
    "\n",
    "| example no. | train input | train label |\n",
    "| ----------------- | ----------- |-------------|\n",
    "| 1  | 'I', 'had', 'much', 'fun'   | 'so' |\n",
    "| 2  | 'had', 'so', 'fun', 'in' | 'much' |\n",
    "| 3  | 'so', 'much', 'in', 'my' | 'fun' |\n",
    "| 4  | 'much', 'fun', 'my', 'semester' | 'in' |\n",
    "| 5  | 'fun', 'in', 'semester', 'break' | 'my' |\n",
    "\n",
    "Our vocabulary has 9 words and CBOW model learns a word embedding for every word in the vocabulary. These word embeddings are typically stored in a giant matrix $W_{input} \\in 9\\times3$, that is **vocabulary size $\\times$ embedding size** (assuming embedding size is 3). Each row in this giant matrix corresponds to a unique word in the vocabulary. The feature vector for the train input is constructed by **averaging the word embeddings** corresponds to the tokens in the train input. For instance, the feature vector for the first training example is given by $\\frac{1}{4} （W_{(I,:)} + W_{(had,:)} + W_{(much,:)} + W_{(fun,:)}）$. \n",
    "\n",
    "This input is fed to a classification layer to identify the target word. Note that the CBOW model does not have bias term in the affine transformation module (``nn.Linear``) corresponding to the classification layer in the neural networks model. The number of categories ($K$) in CBOW model is equivalent to the size of the vocabulary ($9$ in our case).\n",
    "\n",
    "The word embeddings are **intialized** with small numbers sampled randomly **from Gaussian distribution** before training. During training, the word embeddings are jointly learned along with the parameters of the classification layer to predict the target word well given the context. Once the training is done, the word embeddings can capture some semantic features of the word. Note that post training, we do not care for the auxiliary task, whose main goal is to induce meaningful word embeddings.\n",
    "\n",
    "Recommended reading for word embeddings: https://github.com/UBC-NLP/dlnlp2019/blob/master/slides/word2vec.pdf, https://arxiv.org/pdf/1301.3781.pdf and https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html\n",
    "\n",
    "\n",
    "We will use [``torch.nn.Embedding``](https://pytorch.org/docs/stable/nn.html#torch.nn.Embedding) module to simulate the giant matrix, $W_{input}$. Let us look at an example now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************  word lookup table  ********************\n",
      "tensor([[-0.5321, -0.3469, -1.2427],\n",
      "        [-0.6171, -1.3775,  0.8161],\n",
      "        [ 0.5251,  3.0070,  1.4533],\n",
      "        [ 0.5846, -1.0117,  1.0847],\n",
      "        [ 1.0463, -1.3028, -0.9977],\n",
      "        [ 0.0837, -1.3473, -1.5042],\n",
      "        [-0.3642, -0.1346, -1.0112],\n",
      "        [-0.8088,  0.9030,  2.2841],\n",
      "        [ 1.3468,  1.1902, -2.2917]])\n"
     ]
    }
   ],
   "source": [
    "# an Embedding module containing 9 tensors of size 3 to represent W_{input}\n",
    "\n",
    "# Note, the parameters to Embedding class below are:\n",
    "# num_embeddings (int): size of the dictionary of embeddings\n",
    "# embedding_dim (int): the size of each embedding vector\n",
    "# For more details on Embedding class, see: https://github.com/pytorch/pytorch/blob/master/torch/nn/modules/sparse.py\n",
    "embedding = nn.Embedding(9, 3, sparse=True)\n",
    "\n",
    "# print all the word embeddings from the lookup table\n",
    "print(\"*\" *20 ,\" word lookup table \", \"*\" *20)\n",
    "print(embedding.weight.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us construct the word to index and index to word mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************  word2id dictionary  ********************\n",
      "{'I': 0, 'had': 1, 'so': 2, 'much': 3, 'fun': 4, 'in': 5, 'my': 6, 'semester': 7, 'break': 8}\n",
      "********************  id2word dictionary  ********************\n",
      "{0: 'I', 1: 'had', 2: 'so', 3: 'much', 4: 'fun', 5: 'in', 6: 'my', 7: 'semester', 8: 'break'}\n"
     ]
    }
   ],
   "source": [
    "# let us construct the word to index mapping\n",
    "word2id, id2word = {}, {}\n",
    "for word in 'I had so much fun in my semester break'.split():\n",
    "    if word not in word2id:\n",
    "        word2id[word] = len(word2id)\n",
    "        id2word[word2id[word]] = word\n",
    "print(\"*\" *20 ,\" word2id dictionary \", \"*\" *20)\n",
    "print(word2id)\n",
    "print(\"*\" *20 ,\" id2word dictionary \", \"*\" *20)\n",
    "print(id2word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us print all the word embeddings from the lookup table against the corresponding word:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************  word lookup table  ********************\n",
      "I tensor([-0.5321, -0.3469, -1.2427])\n",
      "had tensor([-0.6171, -1.3775,  0.8161])\n",
      "so tensor([0.5251, 3.0070, 1.4533])\n",
      "much tensor([ 0.5846, -1.0117,  1.0847])\n",
      "fun tensor([ 1.0463, -1.3028, -0.9977])\n",
      "in tensor([ 0.0837, -1.3473, -1.5042])\n",
      "my tensor([-0.3642, -0.1346, -1.0112])\n",
      "semester tensor([-0.8088,  0.9030,  2.2841])\n",
      "break tensor([ 1.3468,  1.1902, -2.2917])\n"
     ]
    }
   ],
   "source": [
    "# print all the word embeddings from the lookup table against the words\n",
    "print(\"*\" *20 ,\" word lookup table \", \"*\" *20)\n",
    "for wi in range(embedding.weight.data.size(0)):\n",
    "    print(id2word[wi], embedding.weight.data[wi])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us create the input tensor containing indices of context words for a sample input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************  train_input  ********************\n",
      "tensor([0, 1, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "# construct the train input for example 1\n",
    "train_input = torch.LongTensor([word2id['I'], word2id['had'], word2id['much'], word2id['fun']])\n",
    "print(\"*\" *20 ,\" train_input \", \"*\" *20)\n",
    "print(train_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then convert the input tensor of word indices to corresponding word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************  word_embeddings  ********************\n",
      "tensor([[-0.5321, -0.3469, -1.2427],\n",
      "        [-0.6171, -1.3775,  0.8161],\n",
      "        [ 0.5846, -1.0117,  1.0847],\n",
      "        [ 1.0463, -1.3028, -0.9977]], grad_fn=<EmbeddingBackward>)\n",
      "torch.Size([4, 3])\n"
     ]
    }
   ],
   "source": [
    "# obtain the word embeddings corresponding to the words in the train input\n",
    "word_embeddings = embedding(train_input)\n",
    "print(\"*\" *20 ,\" word_embeddings \", \"*\" *20)\n",
    "print(word_embeddings)\n",
    "print(word_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compute the input feature vector, CBOW model performs mean of all the word embedings in the context:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************  feature_input  ********************\n",
      "tensor([ 0.1204, -1.0097, -0.0849], grad_fn=<MeanBackward1>)\n"
     ]
    }
   ],
   "source": [
    "# If we want to do text classification on the sentence, we can take the mean of the vectors of all the words in the sentence,\n",
    "# thus acquiring a single vector:\n",
    "# construct the feature input (mean of word embeddings) to be fed to the classification module\n",
    "feature_input = word_embeddings.mean(0) # column-wise, dimension-wise\n",
    "print(\"*\" *20 ,\" feature_input \", \"*\" *20)\n",
    "print(feature_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The size of the feature vector for a training input in CBOW model is equal to the size of a word embedding (**3** in our case). The rest of the network is a classification module (hidden to output layer + logsoftmax) to predict the middle word (**so** in this case).\n",
    "\n",
    "Thus, the computational graph for the CBOW model will look like this:\n",
    "\n",
    "<img src=\"images/sl2_textclassification_word2vec.jpg\" alt=\"MLP\" title=\"Word2vec - Computational Graph\" />\n",
    "\n",
    "Let us train a CBOW model from scratch on our one-sentence corpus.\n",
    "\n",
    "Before that, we will define the dataset reader for our one-sentence dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "create a dataset reader\n",
    "\"\"\"\n",
    "class CBOWDataset(Dataset):\n",
    "  \"\"\" one-sentence dataset.\"\"\"\n",
    "  def __init__(self, window_size=2):\n",
    "    # read the corpus\n",
    "    corpus = ['I had so much fun in my semester break']\n",
    "\n",
    "    window = 2 * window_size + 1\n",
    "    token2id = {}\n",
    "    # populate the word to id mapping and generate train inputs/targets\n",
    "    for sentence in corpus:\n",
    "      tokens = sentence.strip().split()\n",
    "      for token in tokens:\n",
    "        if token not in token2id.keys(): # add new word\n",
    "          token2id[token] = len(token2id) # new word index = length of token2id\n",
    "    \n",
    "      # map to index\n",
    "      tokens = [token2id[token] for token in tokens]\n",
    "    \n",
    "      # generate all training samples for this sentence\n",
    "      train_inputs, train_outputs = [], []\n",
    "      for num_win in range(len(tokens) - window + 1):\n",
    "        cur_tokens = tokens[num_win:num_win + window]\n",
    "        tgt_token = cur_tokens[window_size]\n",
    "        train_inputs.append(cur_tokens[0:window_size]+cur_tokens[window_size+1:])\n",
    "        train_outputs.append(tgt_token)\n",
    "    self.token2id = token2id\n",
    "    \n",
    "    # set the vocab. size\n",
    "    self.vocab_size = len(token2id)\n",
    "    \n",
    "    # set the total number of training examples\n",
    "    self.n = len(train_inputs)\n",
    "    \n",
    "    # convert features and labels to torch.tensor\n",
    "    self.features = torch.LongTensor(train_inputs, device=device)\n",
    "    self.labels = torch.LongTensor(train_outputs, device=device)\n",
    "    \n",
    "  # return input and output of a single example\n",
    "  # Input: Feature vectors, where each vector corresponds to a tweet. \n",
    "  # Output: Labels, where each label is one index for each of our tags in the set {positive, negative, neutral}\n",
    "  def __getitem__(self, index):\n",
    "    return self.features[index], self.labels[index]\n",
    "  \n",
    "  # return the total number of examples\n",
    "  def __len__(self):\n",
    "    return self.n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us define the CBOW model (layers and forward propagation logic):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "create a model for CBOW\n",
    "\"\"\"\n",
    "class CBOWmodel(nn.Module):\n",
    "  \n",
    "  def __init__(self, embedding_size, vocab_size, output_size, debug=False):\n",
    "    # In the constructor we define the layers for our model\n",
    "    super(CBOWmodel, self).__init__()\n",
    "    self.embedding = nn.Embedding(vocab_size, embedding_size, sparse=True)\n",
    "    \n",
    "    self.embedding.weight.data.normal_(0.0,0.05) # mean=0.0, mu=0.05\n",
    "    \n",
    "    self.linear_layer = nn.Linear(embedding_size, output_size, bias=False) # the layer will not learn an additive bias\n",
    "    self.softmax_layer = nn.LogSoftmax(dim=1)\n",
    "    \n",
    "    self.debug = debug\n",
    "\n",
    "  def forward(self, x):\n",
    "    # In the forward function we define the forward propagation logic\n",
    "    if self.debug:\n",
    "        print('input (word ids) shape = ', x.size())\n",
    "        print('input word embeddings shape = ', self.embedding(x).size())\n",
    "        print('input feature vector (mean of word embeddings) shape = ', self.embedding(x).mean(1).size())\n",
    "        out = self.embedding(x).mean(1)\n",
    "        print('hidden to output layer input shape = ', out.size())\n",
    "        out = self.linear_layer(out)\n",
    "        print('hidden to output layer output (or logsoftmax input) shape = ', out.size())\n",
    "        out = self.softmax_layer(out)\n",
    "        print('logsoftmax output shape = ', out.size())\n",
    "        return out\n",
    "    out = self.embedding(x).mean(1)\n",
    "    out = self.linear_layer(out)\n",
    "    out = self.softmax_layer(out)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the hyperparameters of the CBOW model will be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameter of CBOW model\n",
    "EMBEDDING_SIZE = 3 # size of the word embedding\n",
    "LEARNING_RATE = 0.5 # learning rate of gradient descent\n",
    "WINDOW_SIZE = 2  # number of words to be considerd before (or after) the target word for making the context\n",
    "MAX_EPOCHS = 5 # number of passes over the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's instantiate an object from the CBOW dataset class and print properties of the training data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of samples in the dataset: 5\n",
      "feature matrix: tensor([[0, 1, 3, 4],\n",
      "        [1, 2, 4, 5],\n",
      "        [2, 3, 5, 6],\n",
      "        [3, 4, 6, 7],\n",
      "        [4, 5, 7, 8]])\n",
      "label matrix: tensor([2, 3, 4, 5, 6])\n"
     ]
    }
   ],
   "source": [
    "# load the dataset reader, corpus: 'I had so much fun in my semester break'\n",
    "dataset = CBOWDataset(window_size = WINDOW_SIZE)\n",
    "print(\"number of samples in the dataset:\", dataset.n)\n",
    "print(\"feature matrix:\", dataset.features)\n",
    "print(\"label matrix:\", dataset.labels)\n",
    "\n",
    "# create loader for reading the training dataset\n",
    "train_loader = DataLoader(dataset=dataset, batch_size=1, shuffle=True, num_workers=1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create the model object, criterion and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model specs:  CBOWmodel(\n",
      "  (embedding): Embedding(9, 3, sparse=True)\n",
      "  (linear_layer): Linear(in_features=3, out_features=9, bias=False)\n",
      "  (softmax_layer): LogSoftmax()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# create the model object\n",
    "model = CBOWmodel(EMBEDDING_SIZE, dataset.vocab_size, dataset.vocab_size)\n",
    "model.to(device)\n",
    "print(\"model specs: \", model)\n",
    "\n",
    "# create the loss function (last node of the graph)\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "# create an instance of SGD with required hyperparameters\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before kick-starting the training, we can see the forward propagation logic with a sample input and target (as usual, closely pay attention to the size of intermediate vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input (word ids) shape =  torch.Size([1, 4])\n",
      "input word embeddings shape =  torch.Size([1, 4, 3])\n",
      "input feature vector (mean of word embeddings) shape =  torch.Size([1, 3])\n",
      "hidden to output layer input shape =  torch.Size([1, 3])\n",
      "hidden to output layer output (or logsoftmax input) shape =  torch.Size([1, 9])\n",
      "logsoftmax output shape =  torch.Size([1, 9])\n",
      "model prediction =  tensor([[-2.1980, -2.1916, -2.1857, -2.2005, -2.1972, -2.1909, -2.2009, -2.2082,\n",
      "         -2.2023]])\n",
      "loss =  tensor(2.1857)\n"
     ]
    }
   ],
   "source": [
    "# toggle debug mode so that all intermediate layer outputs are printed out\n",
    "model.debug = True\n",
    "\n",
    "# create sample input and target\n",
    "sample_input = torch.LongTensor([word2id['I'], word2id['had'], word2id['much'], word2id['fun']]).unsqueeze(0)\n",
    "sample_target = torch.LongTensor([word2id['so']]) \n",
    "\n",
    "# perform feedforward propagation\n",
    "model_prediction = model(sample_input)\n",
    "print(\"model prediction = \", model_prediction.data) # size is no. of classes\n",
    "\n",
    "# calculate loss\n",
    "print(\"loss = \", criterion(model_prediction, sample_target).data)\n",
    "\n",
    "# turn off debug mode\n",
    "model.debug = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us kick-start training the CBOW model on our one-sentence dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 2.1992\n",
      "Epoch [2/5], Loss: 2.1746\n",
      "Epoch [3/5], Loss: 2.1468\n",
      "Epoch [4/5], Loss: 2.1166\n",
      "Epoch [5/5], Loss: 2.0769\n"
     ]
    }
   ],
   "source": [
    "# start the training\n",
    "for epoch in range(MAX_EPOCHS):\n",
    "  # train the model for one pass over the data\n",
    "  train_loss = train(train_loader)   \n",
    "  # print the loss for every epoch\n",
    "  print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, MAX_EPOCHS, train_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We trained CBOW model on a toyish corpus possible. In general, the model is trained on a **large corpus** of raw text and the quality of resulting word embeddings tend to get better with increase in the size of training corpus. Several researchers have released their word embeddings trained on large amounts of data. We can directly use their embeddings in the future work. For example, Google offers a [pre-trained model](https://code.google.com/archive/p/word2vec/) which was trained on Google News dataset containing about 100 billion words.\n",
    "\n",
    "You can access the word embedding in the following way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding for the word 'I': tensor([ 0.1686, -0.1465,  0.2619])\n"
     ]
    }
   ],
   "source": [
    "# prints the embedding of word 'I'\n",
    "print(\"embedding for the word 'I':\", model.embedding.weight.data[dataset.token2id['I']]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding for the word 'semester': tensor([-0.2040, -0.4036,  0.1287])\n"
     ]
    }
   ],
   "source": [
    "# prints the embedding of word 'semester'\n",
    "print(\"embedding for the word 'semester':\", model.embedding.weight.data[dataset.token2id['semester']]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ctrl_transformers)",
   "language": "python",
   "name": "ctrl_transformers"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
