{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Torchtext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Torchtext is a python library that helps you easily process text data for NLP task such as read text file from disk, tokenize the text, convert the text to lists of integers, and pad the sequence in batch, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure you have Python 3.5+ and PyTorch 1.4.0 or newer. You can then install torchtext using pip:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pip3 install torchtext`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use English tokenizer from [SpaCy](https://spacy.io/), you need to install SpaCy and download its English model:\n",
    "\n",
    "```\n",
    "pip3 install spacy\n",
    "python3 -m spacy download en_core_web_sm\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Torchtext takes in raw data in the form of text files, e.g., csv/tsv files, or json files, and converts them to `torchtext.data.Datasets`. \n",
    "\n",
    "Torchtext then passes the Dataset to an Iterator. Iterators handle numericalizing, batching, packaging, and moving the data to the given device (CPU or GPU)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./images/torchtext.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a small trial dataset which include 100 samples of [EmoNet](https://www.aclweb.org/anthology/P17-1067.pdf). Fisrt row is header of this corpus. The first column is sample id, second column is the tweet text, and the third columm is corresponding labels (emotion). We can use pandas to load this tsv file. You can find more information of pandas [here](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html). It provides many arguments to help you load file appropriately. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Ann , Bruce and I went to Selkirk Council of C...</td>\n",
       "      <td>trust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>At the movies .. seeing sex and the city .. by...</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Do not your memory ; it is a net full of holes...</td>\n",
       "      <td>trust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>I seriously hate it when older men act like th...</td>\n",
       "      <td>fear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>The fact that my vote will be based on who I d...</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                              tweet    label\n",
       "0   0  Ann , Bruce and I went to Selkirk Council of C...    trust\n",
       "1   1  At the movies .. seeing sex and the city .. by...  sadness\n",
       "2   2  Do not your memory ; it is a net full of holes...    trust\n",
       "3   3  I seriously hate it when older men act like th...     fear\n",
       "4   4  The fact that my vote will be based on who I d...  sadness"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"./data/small_trial.tsv\", sep = '\\t', header=0) # the separator of tsv file is `\\t`\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select Columns \n",
    "1. `tweet`: the tweet text, i.e., input x\n",
    "2. `label`: label of given tweet, i.e., prediction goal y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf = df[['tweet','label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ann , Bruce and I went to Selkirk Council of C...</td>\n",
       "      <td>trust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>At the movies .. seeing sex and the city .. by...</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Do not your memory ; it is a net full of holes...</td>\n",
       "      <td>trust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I seriously hate it when older men act like th...</td>\n",
       "      <td>fear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The fact that my vote will be based on who I d...</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet    label\n",
       "0  Ann , Bruce and I went to Selkirk Council of C...    trust\n",
       "1  At the movies .. seeing sex and the city .. by...  sadness\n",
       "2  Do not your memory ; it is a net full of holes...    trust\n",
       "3  I seriously hate it when older men act like th...     fear\n",
       "4  The fact that my vote will be based on who I d...  sadness"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ddf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we use `sklearn` to splite data set into train, validation, and test sets.\n",
    "\n",
    "You can install `sklearn` by \n",
    "```\n",
    "pip3 install scikit-learn\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load sklearn\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `train_test_split() ` only can split data into two partitions  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, randomize samples and split into train (80%) and validation_test (20%). \n",
    "# random_state is the seed used by the random number generator, you can use any integer number.\n",
    "train, validation_test = train_test_split(ddf, test_size=0.2, random_state=200) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then, split dev_test into validation (10%) and test (10%) set.\n",
    "validation, test = train_test_split(validation_test, test_size=0.5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set shape: (80, 2)\n",
      "Validation set shape: (10, 2)\n",
      "Test set shape: (10, 2)\n"
     ]
    }
   ],
   "source": [
    "print(\"Train set shape:\", train.shape)\n",
    "print(\"Validation set shape:\", validation.shape)\n",
    "print(\"Test set shape:\", test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new directory to save split datasets.\n",
    "import os\n",
    "save_path = \"./split/\"\n",
    "if not os.path.exists(save_path):\n",
    "    os.mkdir(save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `to_csv()` to save pandas DataFrame to tsv file. \n",
    "Please find more information about this function [here](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_csv.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to tsv files\n",
    "train.to_csv(\"./split/train.tsv\", sep='\\t', index=False)\n",
    "validation.to_csv(\"./split/val.tsv\", sep='\\t', index=False)\n",
    "test.to_csv(\"./split/test.tsv\", sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Declaring the Fields"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Torchtext takes a declarative approach to loading its data: you tell torchtext how you want the data to look like, and torchtext handles it for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import related packages\n",
    "import torch\n",
    "import torchtext\n",
    "from torchtext.data import Field, LabelField\n",
    "from torchtext.data import TabularDataset\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Use [SpaCy English model](https://spacy.io/models/en) to tokenize text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load SpaCy English model to process \n",
    "spacy_en = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you cannot load the model. \n",
    "You may try this: \n",
    "\n",
    "`pip3 install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.0/en_core_web_sm-2.2.0.tar.gz`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use [`tokenizer()`](https://spacy.io/api/tokenizer) method from spacy to tokenize string text to list of tokens. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Use', 'SpaCy', 'English', 'model', 'to', 'tokenize', 'text', '.']"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[tok.text for tok in spacy_en.tokenizer(\"Use SpaCy English model to tokenize text.\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create the `tokenize_en` functions. It will be passed to `TorchText` and take in the sentence as a string and return the sentence as a list of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_en(text):\n",
    "    \"\"\"\n",
    "    Tokenizes English text from a string into a list of strings (tokens)\n",
    "    \"\"\"\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`TorchText's` Fields handle how data should be processed. You can read all of the possible arguments [here](https://torchtext.readthedocs.io/en/latest/data.html#field). These arguments load and process the input data appropriately.\n",
    "\n",
    "* **For the tweet text**, we pass in the preprocessing we want the field to do as keyword arguments. We give it the tokenizer we want the field to use, tell it to convert the input to lowercase, and also tell it the input is sequential.\n",
    "\n",
    "* **For the label input**, it is not sequential input and doesn't need unknown token for out-of-vocabulary words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = Field(sequential=True, tokenize=tokenize_en, lower=True)\n",
    "LABEL = Field(sequential=False, unk_token = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructing the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fields know what to do when given raw data. Now, we need to tell the fields what data it should work on. This is where we use Datasets. To process the tsv file, we use `TabularDataset` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val, test = TabularDataset.splits(\n",
    "               path=\"./split/\", # the root directory where the data lies\n",
    "               train='train.tsv', validation=\"val.tsv\", test=\"test.tsv\", # file names\n",
    "               format='tsv',\n",
    "               skip_header=True, # if your tsv file has a header, make sure to pass this to ensure it doesn't get proceesed as data!\n",
    "               fields=[('tweet', TEXT), ('label', LABEL)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the TabularDataset, we pass in a list of `(name, field)` pairs as the fields argument. The fields we pass in must be in the same order as the columns.\n",
    "\n",
    "The splits method creates three datasets for the train, validation, and test data by applying the same processing. We give the the root directory of datasets and corresponding names."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each dataset can be treated as a list-like dataset and can be indexed and iterated over like normal lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torchtext.data.example.Example object at 0x12d79bac8>\n"
     ]
    }
   ],
   "source": [
    "print(train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get an `Example object`. Each `Example object` includes the attributes of a single data point. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['tweet', 'label'])\n",
      "['<', 'user', '>', 'me', 'too', 'i', 'fell', 'off', 'pretty', 'much', 'everything']\n",
      "sadness\n"
     ]
    }
   ],
   "source": [
    "print(train[0].__dict__.keys())\n",
    "print(train[0].tweet)\n",
    "print(train[0].label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to build our vocabulary to map words to integers. To build the vocabulary in an unbiased way, we need to do so on the training data only. We will take the words which occur more than one in the train set, so we set `min_freq=2`. You can also use `max_size=n` to define the maximal number of vocabulary size. It will only keep the top $n$ most frequent tokens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT.build_vocab(train, min_freq=2)\n",
    "LABEL.build_vocab(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, let us take a look at the vocabulary of tweet texts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<bound method Vocab._default_unk_index of <torchtext.vocab.Vocab object at 0x12d799eb8>>, {'<unk>': 0, '<pad>': 1, '#': 2, '.': 3, 'i': 4, ',': 5, '!': 6, 'the': 7, '<': 8, 'a': 9, 'to': 10, '>': 11, 'user': 12, 'of': 13, '..': 14, 'is': 15, 'in': 16, 'you': 17, 'my': 18, '?': 19, 'do': 20, \"n't\": 21, 'on': 22, 'and': 23, 'for': 24, 'just': 25, 'me': 26, 'that': 27, 'this': 28, 'be': 29, '\"': 30, \"'m\": 31, \"'s\": 32, '-': 33, 'at': 34, 'but': 35, 'have': 36, 'like': 37, ':(': 38, 'as': 39, 'it': 40, 'no': 41, 'not': 42, 'one': 43, 'people': 44, 'want': 45, 'was': 46, 'we': 47, 'your': 48, 'all': 49, 'are': 50, 'god': 51, 'how': 52, 'never': 53, 'she': 54, 'so': 55, 'what': 56, 'when': 57, 'with': 58, '&': 59, \"'\": 60, '3': 61, 'an': 62, 'back': 63, 'before': 64, 'by': 65, 'day': 66, 'go': 67, 'got': 68, 'her': 69, 'ko': 70, 'morning': 71, 'much': 72, 'off': 73, 'oh': 74, 'or': 75, 'them': 76, 'there': 77, 'up': 78, 'vote': 79, 'wanna': 80, 'week': 81, 'were': 82, \"'re\": 83, 'about': 84, 'ai': 85, 'anyone': 86, 'blommesteynweg': 87, 'can': 88, 'college': 89, 'come': 90, 'crying': 91, 'cuz': 92, 'da': 93, 'did': 94, 'ebay': 95, 'everything': 96, 'fact': 97, 'feel': 98, 'feelings': 99, 'from': 100, 'going': 101, 'graduation': 102, 'guys': 103, 'has': 104, 'hate': 105, 'his': 106, 'into': 107, 'j': 108, 'knowing': 109, 'list': 110, 'love': 111, 'm': 112, 'met': 113, 'money': 114, 'more': 115, 'music': 116, 'nba': 117, 'netherlands': 118, 'night': 119, 'now': 120, 'nung': 121, 'out': 122, 'pero': 123, 'que': 124, 'rather': 125, 'respect': 126, 'rip': 127, 'sad': 128, 'school': 129, 'seen': 130, 'shall': 131, 'shit': 132, 'songs': 133, 'sorry': 134, 'summer': 135, 'than': 136, 'then': 137, 'today': 138, 'tonight': 139, 'too': 140, 'tweet': 141, 'van': 142, 'vergiliusstraat': 143, 'wat': 144, 'well': 145, 'who': 146, 'why': 147, 'words': 148, 'world': 149, 'wrong': 150, 'zaandam': 151, 'woman': 0, 'large': 0, 'purse': 0, 'walking': 0, 'gate': 0, 'am': 0, 'very': 0, 'good': 0, 'onefreeadvice': 0, 'long': 0, 'distance': 0, 'relationships': 0, 'work': 0, 'absolutely': 0, 'burger': 0, 'king': 0, 'feels': 0, 'though': 0, 'slowly': 0, 'dyeing': 0})\n"
     ]
    }
   ],
   "source": [
    "print(TEXT.vocab.stoi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and the vocabulary of labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(None, {'sadness': 0, 'joy': 1, 'disgust': 2, 'fear': 3, 'trust': 4, 'anger': 5, 'surprise': 6})\n"
     ]
    }
   ],
   "source": [
    "print(LABEL.vocab.stoi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's show an example to convert text strings to tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token 1: ['a', 'woman', 'with', 'a', 'large', 'purse', 'is', 'walking', 'by', 'a', 'gate', '.']\n",
      "token 2: ['i', 'am', 'very', 'good', '.']\n",
      "tensor([[ 9,  4],\n",
      "        [ 0,  0],\n",
      "        [58,  0],\n",
      "        [ 9,  0],\n",
      "        [ 0,  3],\n",
      "        [ 0,  1],\n",
      "        [15,  1],\n",
      "        [ 0,  1],\n",
      "        [65,  1],\n",
      "        [ 9,  1],\n",
      "        [ 0,  1],\n",
      "        [ 3,  1]])\n",
      "torch.Size([12, 2])\n"
     ]
    }
   ],
   "source": [
    "token_1 = TEXT.preprocess('a woman with a large purse is walking by a gate.')\n",
    "print(\"token 1:\", token_1)\n",
    "token_2 = TEXT.preprocess('i am very good.')\n",
    "print(\"token 2:\", token_2)\n",
    "# convert tokens to tensor\n",
    "tensor = TEXT.process([token_1,token_2])\n",
    "print(tensor)\n",
    "print(tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the output tensor, second sentence is paded to length of 12. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructing the Iterator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final step of preparing data is to create the iterators. \n",
    "Dataset can be iterated by iterator. At each step, the iterator generates a batch of data which will have a text attribute (the PyTorch tensors containing a batch of numericalized text) and a label attribute (the PyTorch tensors containing a batch of numericalized labels)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is code for how you would initialize the Iterators for the train, validation, and test data, repsectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data import Iterator, BucketIterator\n",
    "\n",
    "train_iter, val_iter, test_iter = BucketIterator.splits(\n",
    " (train, val, test), # we pass in the datasets we want the iterator to draw data from\n",
    " batch_sizes=(4,64,64), #batch size for Train, dev and Test, respectively.\n",
    " sort_key=lambda x: len(x.tweet), \n",
    " sort=True,\n",
    "# A key to use for sorting examples in order to batch together examples with similar lengths and minimize padding. \n",
    " sort_within_batch=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The `sort_within_batch` argument, when set to True, sorts the data within each minibatch in decreasing order according to the sort_key.\n",
    " \n",
    "The [`BucketIterator`](https://torchtext.readthedocs.io/en/latest/data.html#torchtext.data.BucketIterator) automatically shuffles and buckets the input sequences into sequences of similar length. In each batch,  we need to pad the input sequences to be of the same length to enable batch processing. The amount of padding necessary is determined by the longest sequence in the batch. Therefore, padding is most efficient when the sequences are of similar lengths."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Iterator\n",
    "We use for-loop to load batch in a iterator. The iterator returns a custom datatype called `torchtext.data.batch.Batch`. The Batch object is a similar API to generate a batch samples from each field in the dataset as attributes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[torchtext.data.batch.Batch of size 4]\n",
      "\t[.tweet]:[torch.LongTensor of size 7x4]\n",
      "\t[.label]:[torch.LongTensor of size 4]\n"
     ]
    }
   ],
   "source": [
    "for batch in train_iter:\n",
    "    print(batch)\n",
    "    tweets = batch.tweet\n",
    "    labels = batch.label\n",
    "    break  #we use first batch as an example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`tweets` and `labels` are tensors, you can use them to train any machine model with [PyTorch](https://pytorch.org/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tweets tensor size is `[maximal sequence length * batch_size]`.\n",
    "\n",
    "The label tensor size is `[batch_size]`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let us see how `tweet` tensor looks like. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 20,   0,   4,   0],\n",
       "        [ 21,   0,   0,  39],\n",
       "        [ 86,   0, 105,   0],\n",
       "        [  2,  53,   0,   4],\n",
       "        [  0,   0,   0, 112],\n",
       "        [  1,  38,   3,   0],\n",
       "        [  1,   1,   1,   0]])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and `label` tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4, 0, 2, 0])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We also can convert indexes to text tokens.**\n",
    "\n",
    "We convert these four samples back to text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed sentence: \n",
      "0  smaple: ['do', \"n't\", 'anyone', '#', '<unk>', '<pad>', '<pad>']\n",
      "1  smaple: ['<unk>', '<unk>', '<unk>', 'never', '<unk>', ':(', '<pad>']\n",
      "2  smaple: ['i', '<unk>', 'hate', '<unk>', '<unk>', '.', '<pad>']\n",
      "3  smaple: ['<unk>', 'as', '<unk>', 'i', 'm', '<unk>', '<unk>']\n"
     ]
    }
   ],
   "source": [
    "print(\"processed sentence: \")\n",
    "for j in range(tweets.shape[1]): # sample loop\n",
    "    tmp = [] # create a output container\n",
    "    for i in range(tweets.shape[0]): # token loop\n",
    "        tmp.append(TEXT.vocab.itos[tweets[i,j]])\n",
    "    print(j,\" smaple:\",tmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see the first 3 samples are padded to length of 7. The out-of-vocabulary tokens are replaced with `<unk>` token."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How do the orignal samples look like?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = []\n",
    "for i in range(len(train)):\n",
    "    samples.append(train.examples[i].tweet)\n",
    "samples.sort(key=len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['do', \"n't\", 'anyone', '#', 'onefreeadvice'],\n",
       " ['long', 'distance', 'relationships', 'never', 'work', ':('],\n",
       " ['i', 'absolutely', 'hate', 'burger', 'king', '.'],\n",
       " ['feels', 'as', 'though', 'i', 'm', 'slowly', 'dyeing']]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples[0:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exceise: \n",
    "\n",
    "I provide a corpus from the [CL-Aff shared task](https://sites.google.com/view/affcon2019/cl-aff-shared-task?authuser=0). The `happy_db_10k.csv` includes all the labeled data of this task. Please `happy_db_10k.csv` to complete following questions.\n",
    "\n",
    "1. Load this `csv` file which delimiter is `,`. \n",
    "2. What columns are in this file?\n",
    "3. Split this file into train (80%), validation (10%), and test (10%).\n",
    "4. Create a new directory called `happydb_split`, and save train, validation, and test as `train.tsv`, `val.tsv`, and `test.tsv` respectively. \n",
    "5. Define `Field(s)` to process datasets. Use `tokenize_en` function as tokenizer. Note: there are two labels in this dataset (i.e., agency and social).\n",
    "6. Use your defined Field(s) and ` TabularDataset` to load the train, validation, and test of `happydb` corpus.\n",
    "7. What the keys are in each Example object?\n",
    "8. Create the vocabulary for each defined `Field(s)` with train set. Only include the top 5,000 most frequent tokens.  \n",
    "9. How many elements are in your each Field?\n",
    "10. Use `BucketIterator` to create `Iterators` for train, validation, and test set with same batch size of 32. Sort by the length of samples, also sort samples within each batch. \n",
    "11. What elements are in the first batch of train set? What shape are they?\n",
    "12. How many batch in Iterator of train set, validation set, and test set?\n",
    "13. Can you covert the tensor of first sample to text?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References:\n",
    "* https://mlexplained.com/2018/02/08/a-comprehensive-tutorial-to-torchtext/\n",
    "* https://medium.com/@adam.wearne/lets-get-sentimental-with-pytorch-dcdd9e1ea4c9\n",
    "* https://spacy.io/\n",
    "* https://www.aclweb.org/anthology/P17-1067.pdf\n",
    "* https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n",
    "* https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html\n",
    "* https://pytorch.org/tutorials/beginner/text_sentiment_ngrams_tutorial.html\n",
    "* https://pytorch.org/tutorials/index.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
