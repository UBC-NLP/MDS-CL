{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSCI 572 Quiz 1 Review Sheet\n",
    "\n",
    "### Quiz Overview\n",
    "\n",
    "Quiz 1 will test your understanding of the content covered thus far in both the lecture and the lab. You can prepare for the quiz by going through the lecture slides and the lab tutorials.   \n",
    "\n",
    "Questions may be:  \n",
    "(1) simple multiple choice,  \n",
    "(2) simple conceptual questions that ask for a very short answer (e.g., what does a function from the code covered do), or  \n",
    "(3) simple completions you need to provide to a code snippet.  \n",
    "\n",
    "Overall, the questions are designed to be straightforward. The Quiz will be held via Canvas. It will be available between 7:00am-1:00pm PT (Vancouver time). Once you start the Quiz, you will have a total of 40 mins to complete it. If you have any questions beforehand, please do get in touch over course Slack. We do not anticipate people will have questions during the Quiz time itself, but if you do, please feel free to ask over class Slack as well. We cannot guarantee being available for the whole 7-1 period, but we will try to get back to you as soon as we can. For example, Peter will possibly be available 7:00-9:00am PT and Muhammad and/or Ganesh will possibly be available 9:00-10:30. From 10:30-1:00pm PT, if you take the Quiz during this slot, someone will likely be able to answer questions as well (either Ganesh or Peter). Good luck with Quiz 1, and we wish you the best.\n",
    "\n",
    "\n",
    "## Conceptual Topics\n",
    "\n",
    "### Hyper Parameter Optimization\n",
    "\n",
    "- Grid Search vs. Manual Search vs. Random Search\n",
    "- Sampling from distributions vs. Setting a Parameter Grid\n",
    "\n",
    "### Neural Network Concepts\n",
    "- Feed Forward Neural Networks\n",
    "- Hidden Units vs. Layers\n",
    "- Weights and Bias\n",
    "- Activation functions\n",
    " - ReLU\n",
    " - Sigmoid\n",
    " - Tanh\n",
    "- Dropout\n",
    "- Initialization\n",
    "- Word Embeddings\n",
    "\n",
    "### Optimizers\n",
    "- Stochastic Gradient Descent (vs. Gradient Descent)\n",
    "- Learning Rate\n",
    "\n",
    "## Coding Related\n",
    "\n",
    "### TorchText\n",
    "- Loading and tokenizing corpora\n",
    "- Bucketiteraor\n",
    "\n",
    "### Pytorch \n",
    "- Permute() vs. View()\n",
    "- Squeeze() and Unsqueeze()\n",
    "- Matmul() vs. Mul()\n",
    "- nn.Sequential() vs. nn.Modulelist()\n",
    "- Intialization of weights and bias of a model  (e.g. layer_name.bias.data.normal_(0,1) )\n",
    "- Counting Parameters of model (e.g. loop through model.parameters() adding up param.numel() )\n",
    "\n",
    "### Model Code\n",
    "\n",
    "Be familiar with (e.g. T1 and T2) model classes there will be questions about something similar to these (completing the class, questions about what parts of the code do).\n",
    "\n",
    "#### FFNN Class Code from Lab 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerNN(nn.Module): #feel free to change the inputs it takes\n",
    "    def __init__(self, input_dim, hidden_units=20, output_classes=20, nonlin=nn.ReLU()):  \n",
    "        super(TwoLayerNN, self).__init__()\n",
    "        self.dense0 = nn.Linear(input_dim, hidden_units)\n",
    "        self.nonlin = nonlin\n",
    "        self.dropout = nn.Dropout(p=.1)\n",
    "        self.dense1 = nn.Linear(hidden_units, hidden_units)\n",
    "        self.output = nn.Linear(hidden_units, output_classes)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, X, **kwargs):\n",
    "        X = self.nonlin(self.dense0(X))\n",
    "        X = self.dropout(X)\n",
    "        X = self.nonlin(self.dense1(X))\n",
    "        X = self.softmax(self.output(X))\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_Seq(nn.Module): #feel free to change the inputs it takes\n",
    "    def __init__(self, input_dim, hidden_units=20, output_classes=20, nonlin=nn.ReLU(), dropout=.5):  \n",
    "        super(MLP_Seq, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_dim,hidden_units),\n",
    "            nonlin,\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(hidden_units,hidden_units),\n",
    "            nonlin,\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(hidden_units,hidden_units),\n",
    "            nonlin,\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(hidden_units,hidden_units),\n",
    "            nonlin,\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(hidden_units,hidden_units),\n",
    "            nonlin,\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(hidden_units,output_classes),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "\n",
    "    def forward(self, X, **kwargs):\n",
    "        return self.layers(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_Mod(nn.Module): \n",
    "    def __init__(self, input_dim,num_layers=5, hidden_units=20, output_classes=20, nonlin=nn.ReLU(), dropout=.5):  \n",
    "        super(MLP_Mod, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i in range(num_layers):\n",
    "            if i==0:\n",
    "                self.layers.append(nn.Linear(input_dim,hidden_units))\n",
    "            else:\n",
    "                self.layers.append(nn.Linear(hidden_units,hidden_units))\n",
    "            self.layers.append(nonlin)\n",
    "            self.layers.append(nn.Dropout(p=dropout))\n",
    "        self.layers.append(nn.Linear(hidden_units,output_classes))\n",
    "        self.layers.append(nn.Softmax(dim=-1))\n",
    "\n",
    "    def forward(self, X, **kwargs):\n",
    "        for layer in self.layers:\n",
    "            X = layer(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### T2 from Lab 2   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DimensionTestModel(nn.Module):\n",
    "  \n",
    "  def __init__(self, input_size, filters, hidden, output_size):\n",
    "    super(DimensionTestModel, self).__init__()\n",
    "    self.cnn = nn.Conv1d(input_size, filters, kernel_size=3, padding =1) \n",
    "    self.activation = nn.ReLU()\n",
    "    self.rnn = nn.RNN(filters, hidden)  \n",
    "    self.linear_layer = nn.Linear(hidden, output_size) \n",
    "    self.softmax_layer = nn.LogSoftmax(dim=1)\n",
    "  \n",
    "  def forward(self, x):   #(Assumes input is [N,L,H])\n",
    "    x = self.cnn(x.permute(0,2,1))\n",
    "    x = self.activation(x)\n",
    "    x, _ = self.rnn(x.permute(2,0,1)) \n",
    "    x = self.linear_layer(x.permute(1,0,2))\n",
    "    x = self.softmax_layer(x)\n",
    "    return x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
