{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSCI 572 Quiz 2 Review Sheet\n",
    "\n",
    "### Quiz Overview\n",
    "\n",
    "Quiz 2 will primarily test your understanding of the content covered thus far in both the lecture and the lab. You can prepare for the quiz by going through the lecture slides and the lab tutorials.   \n",
    "\n",
    "Questions may be:  \n",
    "(1) simple multiple choice, or\n",
    "(2) simple conceptual questions that ask for a very short answer (e.g., what does a function from the code covered do)  \n",
    "\n",
    "Note that unlike Quiz 1 there will be no code-writing segments.\n",
    "\n",
    "Overall, the questions are designed to be straightforward. The Quiz will be held via Canvas. It will be available between 5:00am-1:00pm PT (Vancouver time). Once you start the Quiz, you will have a total of 40 mins to complete it. If you have any questions beforehand, please do get in touch over course Slack. We do not anticipate people will have questions during the Quiz time itself, but if you do, please feel free to ask over class Slack as well. We cannot guarantee being available for the whole 7-1 period, but we will try to get back to you as soon as we can. For example, Ganesh will possibly be available 7:00-9:00am PT and Muhammad and/or Ganesh will possibly be available 9:00-10:30 and Peter will cover from 10:30-1:00pm PT. Good luck with Quiz 2, and we wish you the best.\n",
    "\n",
    "\n",
    "## Conceptual Topics\n",
    "\n",
    "\n",
    "### Neural Network Model Concepts\n",
    "- Feed Forward Neural Networks\n",
    "- Hidden Units vs. Layers\n",
    "- Parameters of a model: Weights, Bias, as well as Model Capacity as a general topic.\n",
    "- Activation functions\n",
    " - ReLU\n",
    " - Sigmoid\n",
    " - Tanh\n",
    "- Dropout\n",
    "- Initialization\n",
    "- Word Embeddings\n",
    " \n",
    "\n",
    "### Optimizers and Training\n",
    "- Stochastic Gradient Descent (vs. Gradient Descent) \n",
    "- Gradient Descent concepts:\n",
    " - Derivative / Gradient (as the slope of the loss surface)\n",
    "- Learning Rate\n",
    "- L2, L1 regularization\n",
    "- Data Augmentation\n",
    "- Loss Functions (Cross Entropy, Negative Log Likelihood)\n",
    "- Backpropagation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
