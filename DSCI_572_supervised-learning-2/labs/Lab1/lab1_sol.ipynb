{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Homework 1 - Supervised Learning II - MDS Computational Linguistics \n",
    "\n",
    "###  Assignment Topics \n",
    "- Operations on Tensor\n",
    "- Linearities, Non-linearities and Loss functions \n",
    "- Linear Regression on single input feature\n",
    "- Linear Regression on multiple input features\n",
    "- Very-short answer questions\n",
    "\n",
    "###  Software Requirements \n",
    "- Python (>=3.6)\n",
    "- PyTorch (>=1.2.0) \n",
    "- Matplotlib (>=3.1.2)\n",
    "- Jupyter (latest)\n",
    "\n",
    "###  Submission Info. \n",
    "- Due Date: January 18, 2020, 18:00:00 (Vancouver time)\n",
    "\n",
    "##  Getting Started "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all necessary imports\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# set the seed (allows reproducibility of the results)\n",
    "manual_seed = 123\n",
    "random.seed(manual_seed) # allows us to reproduce results when using functions in random class\n",
    "np.random.seed(manual_seed) # allows us to reproduce results when using random generation on the numpy\n",
    "torch.manual_seed(manual_seed) # allows us to reproduce results when using random generation on the cpu\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # checks if GPU is there in this system and automatically uses GPU if its available, otherwise uses CPU.\n",
    "n_gpu = torch.cuda.device_count() # gets the number of GPUs in this system\n",
    "if n_gpu > 0:\n",
    "  torch.cuda.manual_seed(manual_seed) # allows us to reproduce results when using random generation on the gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Tidy Submission \n",
    "\n",
    "rubric={mechanics:1}\n",
    "\n",
    "To get the marks for tidy submission:\n",
    "- Submit the assignment by filling in this jupyter notebook with your answers embedded\n",
    "- Be sure to follow the [general lab instructions](https://ubc-mds.github.io/resources_pages/general_lab_instructions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Exercise 1: Operations on Tensor \n",
    "\n",
    "### 1.1 Write code that creates a tensor, **X** of size $5 \\times 5$ containing longs with values initialized to ones. \n",
    "rubric={accuracy:1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here\n",
    "x = torch.tensor((),dtype=torch.long).new_ones((5,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Write code that takes the tensor, **X** (from the previous question 1.1) and sets the values along the diagonal to two.\n",
    "rubric={accuracy:1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2, 1, 1, 1, 1],\n",
       "        [1, 2, 1, 1, 1],\n",
       "        [1, 1, 2, 1, 1],\n",
       "        [1, 1, 1, 2, 1],\n",
       "        [1, 1, 1, 1, 2]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code goes here\n",
    "\n",
    "mask = torch.eye(5,5).bool()  #create a mask of the diagonal\n",
    "x.masked_fill_(mask,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Write code that takes the tensor, **X** (from the previous question 1.2), squares all the values in **X**, sums all the squared values in **X** and prints the square root of this sum? (L2-norm) \n",
    "rubric={accuracy:1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(6.3246, dtype=torch.float64)\n",
      "tensor(6.3246, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# your code goes here\n",
    "#need to convert to a floating point type\n",
    "x = x.double()\n",
    "print(torch.norm(x))\n",
    "\n",
    "print(torch.sqrt(torch.sum(torch.mul(x,x))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Given the following two tensors, **X** $\\in \\mathcal{R}^{4\\times4}$ and $\\textbf{Y} \\in \\mathcal{R}^{4\\times4}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2961, 0.5166, 0.2517, 0.6886],\n",
      "        [0.0740, 0.8665, 0.1366, 0.1025],\n",
      "        [0.1841, 0.7264, 0.3153, 0.6871],\n",
      "        [0.0756, 0.1966, 0.3164, 0.4017]])\n",
      "tensor([[0.1186, 0.8274, 0.3821, 0.6605],\n",
      "        [0.8536, 0.5932, 0.6367, 0.9826],\n",
      "        [0.2745, 0.6584, 0.2775, 0.8573],\n",
      "        [0.8993, 0.0390, 0.9268, 0.7388]])\n"
     ]
    }
   ],
   "source": [
    "X = torch.rand(4,4)\n",
    "print(X)\n",
    "Y = torch.rand(4,4)\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.1 Write code that performs standard matrix multiplication, multiply **X** and **Y** without changing their values and prints the result.\n",
    "rubric={accuracy:1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.1644, 0.7440, 1.1501, 1.4276],\n",
      "        [0.8781, 0.6691, 0.7129, 1.0931],\n",
      "        [1.3464, 0.8175, 1.2572, 1.6133],\n",
      "        [0.6250, 0.4032, 0.6143, 0.8112]])\n",
      "tensor([[1.1644, 0.7440, 1.1501, 1.4276],\n",
      "        [0.8781, 0.6691, 0.7129, 1.0931],\n",
      "        [1.3464, 0.8175, 1.2572, 1.6133],\n",
      "        [0.6250, 0.4032, 0.6143, 0.8112]])\n"
     ]
    }
   ],
   "source": [
    "# your code goes here\n",
    "\n",
    "print(torch.matmul(X,Y))\n",
    "#or...\n",
    "print(X@Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.2 Write code that performs standard addition of two matrices, add **X** and **Y** without changing their values and prints the result.\n",
    "rubric={accuracy:1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4147, 1.3440, 0.6338, 1.3491],\n",
      "        [0.9275, 1.4597, 0.7733, 1.0851],\n",
      "        [0.4586, 1.3848, 0.5928, 1.5444],\n",
      "        [0.9750, 0.2357, 1.2432, 1.1405]])\n"
     ]
    }
   ],
   "source": [
    "# your code goes here\n",
    "print(X + Y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.3 Write code that subtracts matrix **Y** from **X** without changing their values and prints the result.\n",
    "rubric={accuracy:1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1775, -0.3108, -0.1304,  0.0281],\n",
      "        [-0.7796,  0.2734, -0.5001, -0.8802],\n",
      "        [-0.0904,  0.0681,  0.0377, -0.1702],\n",
      "        [-0.8237,  0.1576, -0.6104, -0.3370]])\n"
     ]
    }
   ],
   "source": [
    "# your code goes here\n",
    "\n",
    "print(X - Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.4 Write code that performs standard matrix multiplication, multiply **X** and **Y** and placing the results directly in **X** (modifying **X**) and prints the result.\n",
    "rubric={accuracy:1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2961, 0.5166, 0.2517, 0.6886],\n",
      "        [0.0740, 0.8665, 0.1366, 0.1025],\n",
      "        [0.1841, 0.7264, 0.3153, 0.6871],\n",
      "        [0.0756, 0.1966, 0.3164, 0.4017]])\n",
      "tensor([[1.1644, 0.7440, 1.1501, 1.4276],\n",
      "        [0.8781, 0.6691, 0.7129, 1.0931],\n",
      "        [1.3464, 0.8175, 1.2572, 1.6133],\n",
      "        [0.6250, 0.4032, 0.6143, 0.8112]])\n"
     ]
    }
   ],
   "source": [
    "# your code goes here\n",
    "print(X)\n",
    "X = torch.matmul(X,Y)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Given the following tensor, **X** $\\in \\mathcal{R}^{5\\times3}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7179, 0.7058, 0.9156],\n",
      "        [0.4340, 0.0772, 0.3565],\n",
      "        [0.1479, 0.5331, 0.4066],\n",
      "        [0.2318, 0.4545, 0.9737],\n",
      "        [0.4606, 0.5159, 0.4220]])\n"
     ]
    }
   ],
   "source": [
    "X = torch.rand(5,3)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5.1 Write code to print all the elements in the last row of **X**.\n",
    "rubric={accuracy:1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4606, 0.5159, 0.4220])\n"
     ]
    }
   ],
   "source": [
    "# your code goes here\n",
    "\n",
    "print(X[4,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5.2 Write code to print all the elements in the middle column of **X**.\n",
    "rubric={accuracy:1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.7058, 0.0772, 0.5331, 0.4545, 0.5159])\n"
     ]
    }
   ],
   "source": [
    "# your code goes here\n",
    "\n",
    "print(X[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5.3 Write code to create a 3D tensor of size $1 \\times 5 \\times 3$ using the $5 \\times 3$ values from **X** (unsqueeze operation)\n",
    "rubric={accuracy:1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here\n",
    "\n",
    "y = X.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5.4 Write code that converts the 3D tensor (created in the previous question (c)) back into 2D tensor (of size $5 \\times 3$). (squeeze operation)\n",
    "rubric={accuracy:1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here\n",
    "\n",
    "y = y.squeeze(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Exercise 2: Linearities, Non-linearities, Loss functions\n",
    "\n",
    "Sample question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([168.], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "linear_layer = torch.nn.Linear(5, 1)\n",
    "linear_layer.weight.data[0] = torch.tensor([1, 2, 3, 4, 5]) # sets the weight value\n",
    "linear_layer.bias.data = torch.tensor([3]).float() # sets the bias value\n",
    "model_out = linear_layer(torch.tensor([0, 10, 20, 15, 5]).float())\n",
    "print(model_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the values in **model\\_out** by hand. Show your work.\n",
    "\n",
    "Sample answer: (write it in markdown, not as code. if you don't like markdown, you can write the steps in a piece of paper, take a photo and attach an image in the answer block)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "your answer goes here:\n",
    "\n",
    "$model\\_out = A x + b = [1, 2, 3, 4, 5] * [0, 10, 20, 15, 5] + 3 = (1*0 + 2*10 + 3*20 + 4*15 + 5*5) + 3 = 165 + 3 = 168 $\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([165.], grad_fn=<SqueezeBackward3>)\n"
     ]
    }
   ],
   "source": [
    "linear_layer = torch.nn.Linear(5, 1, bias=False)\n",
    "linear_layer.weight.data[0] = torch.tensor([1, 2, 3, 4, 5]) # sets the weight value\n",
    "model_out = linear_layer(torch.tensor([0, 10, 20, 15, 5]).float())\n",
    "print(model_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the values in **model\\_out** by hand. Show your work.\n",
    "rubric={accuracy:2}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "your answer goes here (double-click this block to edit):\n",
    "\n",
    "(There is small error in the example, which we can fix by just transposing the $x$ vector from a row ($1 \\times 5$) vector to a column ($5 \\times 1$) vector).  I'll also start using the convention of calling our weight matrix $W$, this is the same as the $A$ matrix in the example, but a more standard convention.\n",
    "$model\\_out = W x^T + b = [1, 2, 3, 4, 5] * [0, 10, 20, 15, 5]^T = (1*0 + 2*10 + 3*20 + 4*15 + 5*5)  = 165  = 165 $\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.0000, 0.8808], grad_fn=<SigmoidBackward>)\n"
     ]
    }
   ],
   "source": [
    "linear_layer = torch.nn.Linear(5, 2)\n",
    "linear_layer.weight.data[0] = torch.tensor([1, 2, 3, 4, 5]) # sets the weight value\n",
    "linear_layer.weight.data[1] = torch.tensor([1, 0, 0, 0, 1]) # sets the weight value\n",
    "linear_layer.bias.data = torch.tensor([1,1]).float() # sets the bias value\n",
    "model_out = linear_layer(torch.tensor([0, 10, 20, 15, 1]).float())\n",
    "sigmoid_out = torch.nn.Sigmoid()(model_out)\n",
    "print(sigmoid_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the values in **sigmoid\\_out** by hand. Show your work.\n",
    "rubric={accuracy:2}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "your answer goes here:\n",
    "$W$ is a $2 \\times 5$ matrix with each row consisting of the weights of a particular node.\n",
    "\n",
    "We've defined $x$ as a $1 \\times 5$ row vector, which we'll again need to transpose to match dimensions with $W$.\n",
    "\n",
    "$Wx^T$ after multiplication should have dimensions $2 \\times 1$, which is a column vector, but PyTorch unfortunately adopts a row notation for convenience, so just think of the output as being transposed (in the next problem I'll show how we can re-write things to better align with pytorch)\n",
    "\n",
    "$W x^T + b = [[1, 2, 3, 4, 5],[1,0,0,0,1]] * [0, 10, 20, 15, 1]^T + [1,1]^T $\n",
    "\n",
    "$= [(1*0 + 2*10 + 3*20 + 4*15 + 5*1) + 1,(1*0+0*10+0*20+0*15+1*1)+1]^T = [146,2]^T $  \n",
    "\n",
    "now take the sigmoid of this (applying it element-wise)\n",
    "\n",
    "$sigmoid([146,2]^T) = [\\frac{1}{1+exp(-146)},\\frac{1}{1+exp(-2)}]^T = [1.0,0.88]^T$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000],\n",
      "        [0.9933, 0.0067]], grad_fn=<SoftmaxBackward>)\n"
     ]
    }
   ],
   "source": [
    "linear_layer = torch.nn.Linear(5, 2)\n",
    "linear_layer.weight.data[0] = torch.tensor([1, 2, 3, 4, 5]) # sets the weight value\n",
    "linear_layer.weight.data[1] = torch.tensor([1, 3, 0, 0, 10]) # sets the weight value\n",
    "linear_layer.bias.data = torch.tensor([3]).float() # sets the bias value\n",
    "model_out = linear_layer(torch.tensor([[100, 10, 20, 15, 1], [10, 5, 2, 1, 0]]).float())\n",
    "softmax_out = torch.nn.Softmax(dim=1)(model_out)\n",
    "print(softmax_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the values in **softmax\\_out** by hand. Show your work.\n",
    "rubric={accuracy:2}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "your answer goes here:\n",
    "\n",
    "For convenience, with batches I tend to personally prefer using a row notation for the matrix of $X$. This happens to align with how pytorch does things, but if you ended up getting your output transposed, just note the convention difference.  In order for row matrix form of $X$ to work, the equation is going to change slightly. Instead of $Wx^T + b$ I'll use $XW^T + b$\n",
    "\n",
    "With $X$ as a $2 \\times 5$ matrix consisting of the rows of training samples.\n",
    "$W^T$ $5 \\times 2$ matrix consisting of the weights for each node now transposed into the columns of the matrix.\n",
    "\n",
    "I'll also use a $2 \\times 2$ matrix of ones to deal with the bias being added (since it is the same for each node)\n",
    "\n",
    "our output will thus be a $2 \\times 2$ matrix with each row consisting of the softmax applied to a particular training sample.\n",
    "\n",
    "$XW^T + b = [[100, 10, 20, 15, 1], [10, 5, 2, 1, 0]]\\times [[1,2,3,4,5],[1,3,0,0,10]]^T + 3 \\times OnesMatrix$\n",
    "\n",
    "$= [[ (100*1+10*2 + 20*3 + 15*4 + 1*5 +3), (100*1 + 10*3 + 20*0 + 15 * 0 + 1*10  +3)],[(10*1+5*2+2*3+1*4+0*5 +3), (10*1+5*3+2*0+1*0+0*0  +3)]]$\n",
    "\n",
    "$ = [[248,143],[33,28]]$\n",
    "\n",
    "Now apply softmax to each row:\n",
    "$softmax([[248,143],[33,28]])  = [[\\frac{exp(248)}{exp(248)+exp(143)}, \\frac{exp(143)}{exp(248)+exp(143)}],[\\frac{exp(33)}{exp(33)+exp(28)},\\frac{exp(28)}{exp(33)+exp(28)}]] = [[1.0,0.0],[0.9933,0.0067]]$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(7.7500, grad_fn=<MseLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "linear_layer = torch.nn.Linear(5, 2)\n",
    "linear_layer.weight.data[0] = torch.tensor([1, 2, 3, 4, 5]) # sets the weight value\n",
    "linear_layer.weight.data[1] = torch.tensor([1, 3, 0, 0, 10]) # sets the weight value\n",
    "linear_layer.bias.data = torch.tensor([3]).float() # sets the bias value\n",
    "model_out = linear_layer(torch.tensor([[100, 10, 20, 15, 1], [10, 5, 2, 1, 0]]).float())\n",
    "criterion = torch.nn.MSELoss()\n",
    "loss = criterion(model_out, torch.tensor([[245, 140], [30, 30]]).float())\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the values in **loss** by hand. Show your work.\n",
    "rubric={accuracy:2}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "your answer goes here:\n",
    "\n",
    "Same idea as 2.3, but now we'll apply a loss function, the Mean Squared Error, defined as $\\frac{1}{n}\\sum_i^n(\\tilde{y_i} -y_i)^2$ where $\\tilde{y_i}$ is the prediction for one of our test samples, with $n$ being the number of samples.\n",
    "\n",
    "First let's compute the $modelout$ for our inputs:\n",
    "\n",
    "$XW^T + b  = [[100,10,20,15,1],[10,5,2,1,0]] \\times [[1,2,3,4,5],[1,3,0,0,10]]^T  + 3 OnesMatrix$\n",
    "\n",
    "$= [[(100*1+10*2+20*3+15*4+1*5+3),(100*1+10*3+20*0+15*0+1*10+3)],[(10*1+5*2+2*3+1*4+0*5+3),(10*1+5*3+2*0+1*0+0*10+3)]] $\n",
    "\n",
    "$= [[248,143],[33,28]]$\n",
    "\n",
    "Now let's apply the loss:\n",
    "$\\frac{1}{n}\\sum_i^n(\\tilde{y_i} -y_i)^2$\n",
    "\n",
    "$\\frac{1}{2}(mean([(248-245),(143-140)]^2) + mean([(33-30),(28-30)]^2) = \\frac{1}{2} (\\frac{9+9}{2} +\\frac{9+4}{2}) = 7.75$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(9.)\n",
      "tensor(6.5000)\n",
      "7.75\n"
     ]
    }
   ],
   "source": [
    "#in code:\n",
    "y1 = torch.tensor([248.,143.])\n",
    "y1_t = torch.tensor([245.,140.])\n",
    "y2 = torch.tensor([33.,28.])\n",
    "y2_t = torch.tensor([30.,30.])\n",
    "\n",
    "print (((y1 - y1_t)**2).mean())\n",
    "print (((y2 - y2_t)**2).mean())\n",
    "print((9. + 6.5)/2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Very-Short answer questions \n",
    "\n",
    "(Double-click each question block and place your answer at the end of the question) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 What is NumPy? What are the differences between PyTorch's Tensor and NumPy?\n",
    "rubric={reasoning:2}\n",
    "\n",
    "NumPy is a Python scientific computing package with support for linear algebra and some machine learning tools. The main difference between PyTorch Tensors and Numpy high dimension arrays is  basically the names that they are called, and the fact that PyTorch can load its tensors onto CUDA capable devices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 What is the key difference between ``torch.LongTensor`` and ``torch.cuda.LongTensor``?\n",
    "rubric={reasoning:2}\n",
    "\n",
    "The cuda version has been loaded onto a GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 What is the default data type of a PyTorch tensor?\n",
    "rubric={accuracy:1}\n",
    "\n",
    "float32  You can check this yourself as below by creating a tensor (without data).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(())\n",
    "print(x.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 What is ``autograd`` in PyTorch? How is it related to computational graph?\n",
    "rubric={reasoning:2}\n",
    "\n",
    "Autograd allows PyTorch to automatically calculate the gradient of functions. In our computational graph certain nodes can store the gradient as they are changed, thereby allowing for fast computation of backward propagation to learn the weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 What is SGD? What role SGD plays in building machine learning models?\n",
    "rubric={reasoning:2}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stoachastic Gradient Descent is a technique to train a machine learning model by sampling from the training set, calculating the loss of a sample and slightly update the weights of the model based on the loss/gradient from that sample. Generally in SGD you slow down as you see more samples, eventually converging on a stable set of training weights. It's an extremely useful tool in training many machine learning models, particularly neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
