{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Neural Networks - Supervised Learning II - MDS Computational Linguistics\n",
    "\n",
    "### Goal of this tutorial\n",
    "- Introduce Recurrent Neural Networks (RNNs)\n",
    "- Implement RNN for sentiment analysis\n",
    "- Implement Long-Short Term Memories (LSTMs) for sentiment analysis\n",
    "- Implement Gated Recurrent Units (GRUs) for sentiment analysis\n",
    "\n",
    "### General\n",
    "- This notebook was last tested on Python 3.6.9 and PyTorch 1.2.0\n",
    "\n",
    "We would like to acknowledge the following materials which helped as a reference in preparing this tutorial:\n",
    "- https://github.com/UBC-NLP/dlnlp2019/blob/master/slides/RNN.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torchtext\n",
    "from torchtext.data import Field, LabelField\n",
    "from torchtext.data import TabularDataset\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ``Embedding`` layer in Pytorch is where we pass our vocabulary to get get back a word vector for each word in the vocabulary. We need to know what the input and output of this layer look like. Let's first\n",
    "do this on a dummy example where we have two sentences ``x_1`` and ``x_2``. Let's assume we have the two sentences as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_1 = \"He is very nice\"\n",
    "x_2 = \"She is very kind\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's convert the two sentences into indexes (each word is replaced with its index in the vocabulary).\n",
    "Let's assume our ``vocabulary size`` is set to 100. Remember, vocabulary size is a hyper-parameter.\n",
    "Let's also store that ``vocabulary size`` in a variable ``VOCAB_SIZE`` now as we will need to pass it to the ``Embedding`` layer later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_1 = [1, 25, 40, 5]\n",
    "x_2 = [4, 25, 40, 99]\n",
    "VOCAB_SIZE = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Max sequence length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One last thing we need to think about is the ``length of each sequence``. The two examples above are nicely set to equal length = 4. This does not need to be the case, as we can have sequences of varying lengths. We will be passing a batch of sentences to Pytorch and the max sequence length will be set to the length of the longest sentence in that batch. The rest of sentences (shorter ones) will be padded with zeros. Now, do we need to explicitly provide the max sequence length to Pytorch? And how do we know the max seq length for each batch, if different batches have sequences of varying lengths and each batch is set to the max sent in that batch? Well, rest assured, we don't really need to worry about that. Pytorch will assign a max seq length for each batch. We will be able to inspect the max seq length for a given batch using output of the ``Embedding`` layer. (We will see that soon)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Size of word vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ``Embedding`` layer will give us a vector for each word in the vocabulary.\n",
    "Now, we will need to tell it what size we want for that vector. Popular values for a vector size are usally between 100-300 for many tasks (e.g., sentiment analysis\"). Let's set it to 200 dimensions. (You are encouraged to play with this value as practice). All words in the vocabulary will have the same embedding size. Let's put that hyper-parameter in a variable ``WORD_VEC_SIZE``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORD_VEC_SIZE= 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to call the ``Embedding`` class to construct an embeddings tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-1.0173, -0.2822, -0.6141,  ..., -0.8959,  1.1190, -0.4438],\n",
      "         [ 0.0602, -0.2505, -1.2976,  ..., -1.3709,  0.2254,  0.9211],\n",
      "         [-0.3167, -0.0642, -1.6407,  ...,  0.8592, -1.3123,  1.8021],\n",
      "         [-1.7252, -0.6556,  0.6987,  ...,  0.9736,  0.5480,  0.9017]],\n",
      "\n",
      "        [[ 0.2088,  1.5250,  0.3692,  ..., -1.1283,  1.0551,  1.5862],\n",
      "         [ 0.0602, -0.2505, -1.2976,  ..., -1.3709,  0.2254,  0.9211],\n",
      "         [-0.3167, -0.0642, -1.6407,  ...,  0.8592, -1.3123,  1.8021],\n",
      "         [-0.6979,  0.1417, -2.3944,  ...,  0.9338, -0.6353,  0.4065]]],\n",
      "       grad_fn=<EmbeddingBackward>)\n"
     ]
    }
   ],
   "source": [
    "# Constructing an embedding Layer:\n",
    "embedding = nn.Embedding(VOCAB_SIZE, WORD_VEC_SIZE)\n",
    "input = torch.LongTensor([ x_1, x_2 ])\n",
    "embedded=embedding(input)\n",
    "print(embedded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's print the shape of this tensor:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4, 300])\n"
     ]
    }
   ],
   "source": [
    "print(embedded.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is telling us:\n",
    "- We have ``2 examples`` (that is, our ``x_1`` and ``x_2``). (Note: We will be paqssing a whole batch to the ``Embedding`` class and so this first dimension will be equal to the ``batch size``.\n",
    "- For each of the two examples, we have a ``max sequence length`` = 4 (x_1 and x_2 each had 4 indexes).\n",
    "- The ``word vector dimension`` is set to 300."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Max sequence length: Another note"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall from above we mentioned Pytorch automatically infers the max sequence length for each batch. \n",
    "For the example above (as you can see from the second dimension returned by ``embedded.size()``, Pytorch \n",
    "inferred the max seq length for this batch of two sentences is 4.\n",
    "Let's just adjust the second example, **adding two more words** (the string \"and kind\"). Note, both our ``VOCAB_SIZE`` and ``WORD_VEC_SIZE`` stay the same as before. We assign the word \"and\" an index of \"7\" and the word \"considerate\" and index of \"60\". Note that we have to pad the first example ``x_1`` with zeros in the end. (Try removing the zero padding. What do you observe when you run your code with the ``Embedding`` class? Hint: You will get an error.):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_1 = \"He is very nice\"\n",
    "x_2 = \"She is very kind and considerate\"\n",
    "x_1 = [1, 25, 40, 5, 0, 0]\n",
    "x_2 = [4, 25, 40, 99, 7, 60]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's create a new ``embedding`` layer by creating a new instance of the ``Embedding`` class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.6740,  0.9941,  0.4321,  ..., -1.7173,  0.6479,  0.2987],\n",
      "         [-2.4354, -0.2429,  1.6961,  ..., -0.0715, -0.0265,  0.1339],\n",
      "         [ 0.9536, -1.1079, -1.2589,  ..., -0.0185,  0.6623, -1.3210],\n",
      "         [-0.1270, -0.6810,  0.6304,  ...,  1.8238, -0.0749, -1.9014],\n",
      "         [ 1.0998,  1.2457,  1.6403,  ..., -0.3129, -0.1513,  1.5226],\n",
      "         [ 1.0998,  1.2457,  1.6403,  ..., -0.3129, -0.1513,  1.5226]],\n",
      "\n",
      "        [[ 0.9448,  0.6914, -0.6147,  ..., -1.4866,  0.3536,  1.0880],\n",
      "         [-2.4354, -0.2429,  1.6961,  ..., -0.0715, -0.0265,  0.1339],\n",
      "         [ 0.9536, -1.1079, -1.2589,  ..., -0.0185,  0.6623, -1.3210],\n",
      "         [ 0.0598,  0.9716, -1.0782,  ..., -0.6347, -0.7087,  0.7179],\n",
      "         [-2.0151, -0.2159,  3.1345,  ..., -0.5492, -1.7209, -0.5222],\n",
      "         [ 0.1852,  0.4160, -1.2750,  ..., -0.1951,  1.0492, -0.4819]]],\n",
      "       grad_fn=<EmbeddingBackward>)\n"
     ]
    }
   ],
   "source": [
    "# Constructing an embedding Layer:\n",
    "embedding = nn.Embedding(VOCAB_SIZE, WORD_VEC_SIZE)\n",
    "input = torch.LongTensor([ x_1, x_2 ])\n",
    "embedded=embedding(input)\n",
    "print(embedded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If we inspect the shape of the new tensor ``embedded``, we will see the second dimension now changed to 6, to match the max sequence length:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 6, 300])\n"
     ]
    }
   ],
   "source": [
    "print(embedded.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How does Pytorch initialize word vector dimensions/weights?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that Pytorch initializes the word vectors with initialized from a **normal distribution** $ \\mathcal{N}(0, 1) $. The word embedding weights are by default learnable parameters in Pytorch and so they will be adjusted during training. (Note: These weights can be initialized from an external word embedding tool such as Word2vec, Fasttext, or Glove. Also, the weights can be frozen, which is a reasonable option when initialized from an external tool. You can choose to keep learning them within the model with your training data). Below we show the ones initialized from a normal distribution by Pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 1.0998e+00,  1.2457e+00,  1.6403e+00,  ..., -3.1286e-01,\n",
       "         -1.5131e-01,  1.5226e+00],\n",
       "        [ 6.7396e-01,  9.9413e-01,  4.3211e-01,  ..., -1.7173e+00,\n",
       "          6.4793e-01,  2.9870e-01],\n",
       "        [-1.3629e+00, -1.1976e+00,  2.4966e-04,  ...,  1.7985e+00,\n",
       "         -2.6385e-02, -7.9377e-01],\n",
       "        ...,\n",
       "        [ 2.2782e-01,  9.5682e-03,  3.1846e-01,  ..., -1.4852e+00,\n",
       "         -9.7588e-01, -4.9308e-01],\n",
       "        [-1.0949e+00,  2.2144e-01,  1.7491e-01,  ..., -1.0084e+00,\n",
       "          1.3034e+00,  1.3666e+00],\n",
       "        [ 5.9778e-02,  9.7155e-01, -1.0782e+00,  ..., -6.3473e-01,\n",
       "         -7.0874e-01,  7.1787e-01]], requires_grad=True)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More information about the ``Embedding`` class can be found [here](https://pytorch.org/docs/stable/_modules/torch/nn/modules/sparse.html#Embedding)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Neural Networks\n",
    "Recurrent Neural Networks (RNNs) are used to model sequences of arbitrary length (e.g., sequence of words in a sentence, sequence of sentences in a document, sequence of frames in a video). RNNs typically use their internal state (memory) to process sequence of inputs. At each time-step, RNNs output a prediction and hidden state, feeding its previous hidden state into each next step. RNNs are applied in a wide range of NLP applications:\n",
    "- language modeling, where RNN can condition on **all** previous words in the corpus unlike n-gram language model\n",
    "- text classification, where the states act as features (we will see sentiment analysis in this tutorial)\n",
    "- machine translation, where a RNN is used to process a sentence in source language and another RNN is used to decode the sentence in target language (we will see this in the \"Machine Translation\" course)\n",
    "- sequence labeling, where the states in RNN are used to predict a category for each item in the sequence (we might see named entity recognition in the next tutorial)\n",
    "\n",
    "Recommended reading for understanding the theory of RNNs: https://github.com/UBC-NLP/dlnlp2019/blob/master/slides/RNN.pdf \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grabbing few tweets using torchtext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us follow **torchtext** tutorial to read few tweets from the [sentiment analysis dataset](http://alt.qcri.org/semeval2016/task4/) used in the previous tutorial on feedforward neural networks. The preprocessed (tokenization, removing URLs, mentions, hashtags and so on) tweets are placed under ``data/sentiment-twitter-2016-task4`` folder in three files as ``train.tsv``, ``dev.tsv`` and ``test.tsv``.  \n",
    "\n",
    "Let us view few tweets from ``train.tsv`` using pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dear &lt;&lt;&lt;MENTION&gt;&gt;&gt; the newooffice for mac is g...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;&lt;&lt;MENTION&gt;&gt;&gt; how about you make a system that...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i may be ignorant on this issue but should we ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>thanks to &lt;&lt;&lt;MENTION&gt;&gt;&gt; i just may be switchin...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>if i make a game as a &lt;&lt;&lt;HASHTAG&gt;&gt;&gt; universal ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet  label\n",
       "0  dear <<<MENTION>>> the newooffice for mac is g...      2\n",
       "1  <<<MENTION>>> how about you make a system that...      2\n",
       "2  i may be ignorant on this issue but should we ...      2\n",
       "3  thanks to <<<MENTION>>> i just may be switchin...      2\n",
       "4  if i make a game as a <<<HASHTAG>>> universal ...      0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"./data/sentiment-twitter-2016-task4/train.tsv\", sep = '\\t', header=None, names=['tweet','label']) # the separator of tsv file is `\\t`\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We import the relevant packages, define the tokenizer and TorchText's fields.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import related packages\n",
    "import torchtext\n",
    "from torchtext.data import Field, LabelField\n",
    "from torchtext.data import TabularDataset\n",
    "\n",
    "# define the white space tokenizer to get tokens\n",
    "def tokenize_en(tweet):\n",
    "    \"\"\"\n",
    "    Tokenizes English tweet from a string into a list of strings (tokens)\n",
    "    \"\"\"\n",
    "    return tweet.strip().split()\n",
    "\n",
    "# define the TorchText's fields\n",
    "TEXT = Field(sequential=True, tokenize=tokenize_en, lower=True)\n",
    "LABEL = Field(sequential=False, unk_token = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To use the different splits (training, development and testing), we use `TabularDataset` class to load datasets.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val, test = TabularDataset.splits(\n",
    "    path=\"./data/sentiment-twitter-2016-task4/\", # the root directory where the data lies\n",
    "    train='train.tsv', validation=\"dev.tsv\", test=\"test.tsv\", # file names\n",
    "    format='tsv',\n",
    "    skip_header=False, # if your tsv file has a header, make sure to pass this to ensure it doesn't get proceesed as data!\n",
    "    fields=[('tweet', TEXT), ('label', LABEL)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Build our vocabulary to map words to integers.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT.build_vocab(train, min_freq=3) # builds vocabulary based on all the words that occur at least twice in the training set\n",
    "LABEL.build_vocab(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Initialize the iterators for the train, validation, and test data. Note that we set ``sort`` as False so as to not sort examples based on similar lengths which minimizes padding.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data import Iterator, BucketIterator\n",
    "\n",
    "train_iter, val_iter, test_iter = BucketIterator.splits(\n",
    " (train, val, test), # we pass in the datasets we want the iterator to draw data from\n",
    " batch_sizes=(4,64,64),\n",
    " sort_key=lambda x: len(x.tweet), \n",
    " sort=False,\n",
    "# A key to use for sorting examples in order to batch together examples with similar lengths and minimize padding. \n",
    " sort_within_batch=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create a batch of four examples and print them**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed tweets: \n",
      "0  sample: [\"i'd\", 'like', 'to', 'go', 'see', 'eric', 'church', 'tomorrow', 'friends', 'make', 'it', 'happen', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "1  sample: ['<unk>', 'end', 'to', 'the', 'match', 'for', 'isner', \"would've\", 'liked', 'to', 'see', 'a', 'set', 'tiebreak', 'but', 'federer', 'is', 'a', '<unk>', \"he's\", 'ridiculous', '<pad>']\n",
      "2  sample: ['kurt', 'cobain', 'solo', 'album', 'to', 'be', 'released', 'in', 'november', 'no', 'plans', 'currently', 'for', 'a', '<unk>', 'u.s.', 'tour', 'to', 'support', 'it', '<<<hashtag>>>', '<pad>']\n",
      "3  sample: ['a', 'guardian', 'article', 'thinks', 'erdogan', 'is', 'the', 'prime', 'minister', 'of', 'turkey', 'apparently', '<unk>', 'sees', 'all', 'kurds', 'as', 'a', 'threat', 'not', 'pkk', '<<<url>>>']\n"
     ]
    }
   ],
   "source": [
    "# create a single batch and terminate the loop\n",
    "for batch in train_iter:\n",
    "    tweets = batch.tweet\n",
    "    labels = batch.label\n",
    "    break  #we use first batch as an example.\n",
    "\n",
    "# print the four examples with padding \n",
    "print(\"processed tweets: \")\n",
    "for j in range(tweets.shape[1]): # sample loop\n",
    "    tmp = []\n",
    "    for i in range(tweets.shape[0]): # token loop\n",
    "        tmp.append(TEXT.vocab.itos[tweets[i,j]])\n",
    "    print(j,\" sample:\",tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now we set ``sort`` as True so as to sort examples based on similar lengths which minimizes padding.**\n",
    "\n",
    "**Let us initialize the new iterators for the train, validation, and test data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter, val_iter, test_iter = BucketIterator.splits(\n",
    " (train, val, test), # we pass in the datasets we want the iterator to draw data from\n",
    " batch_sizes=(4,64,64),\n",
    " sort_key=lambda x: len(x.tweet), \n",
    " sort=True,\n",
    "# A key to use for sorting examples in order to batch together examples with similar lengths and minimize padding. \n",
    " sort_within_batch=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let us pick up 4 tweets from the training set and convert them to tensors.**\n",
    "\n",
    "**Create a batch of four examples and print them**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed tweets: \n",
      "0  sample: ['ihop', 'is', 'the', 'move', 'tomorrow']\n",
      "1  sample: ['<<<mention>>>', 'make', 'david', 'beckham', 'tomorrow']\n",
      "2  sample: ['bringing', 'the', 'bentley', 'out', 'tomorrow']\n",
      "3  sample: ['new', '<unk>', 'with', 'bentley', 'tomorrow']\n"
     ]
    }
   ],
   "source": [
    "# create a single batch and terminate the loop\n",
    "for batch in train_iter:\n",
    "    tweets = batch.tweet\n",
    "    labels = batch.label\n",
    "    break  #we use first batch as an example.\n",
    "\n",
    "# print the four examples with padding \n",
    "print(\"processed tweets: \")\n",
    "for j in range(tweets.shape[1]): # sample loop\n",
    "    tmp = []\n",
    "    for i in range(tweets.shape[0]): # token loop\n",
    "        tmp.append(TEXT.vocab.itos[tweets[i,j]])\n",
    "    print(j,\" sample:\",tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a single hidden layer RNN\n",
    "\n",
    "PyTorch has ``torch.nn.RNN`` module that implements the vanilla (Elman) RNN with *tanh* or *ReLU* non-linearity. The documentation for this module is [here](https://pytorch.org/docs/stable/nn.html#torch.nn.RNN). Let us use the sample batch of five examples created before to understand this module.\n",
    "\n",
    "In this tutorial, we will represent the input tweet using a sequence of word embeddings (for each word present in the tweet). We will use ``torch.nn.Embedding module`` to store word vectors corresponding to words in the vocabulary.\n",
    "\n",
    "Before implementing the embedding module for our usecase, let us compute the size of the word vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3335\n"
     ]
    }
   ],
   "source": [
    "VOCAB_SIZE = len(TEXT.vocab.stoi)\n",
    "print(VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us implement the embedding module (whose underlying weight matrix shape is (``vocabulary size`` $\\times$ ``word embedding size``) for our usecase:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# an Embedding module containing 10 dimensional tensor for each word in the vocabulary\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "WORD_VEC_SIZE=300\n",
    "# Note, the parameters to Embedding class below are:\n",
    "# num_embeddings (int): size of the dictionary of embeddings\n",
    "# embedding_dim (int): the size of each embedding vector\n",
    "# For more details on Embedding class, see: https://github.com/pytorch/pytorch/blob/master/torch/nn/modules/sparse.py\n",
    "embedding = nn.Embedding(VOCAB_SIZE, WORD_VEC_SIZE, sparse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now feed the tensors of our sample batch to the embedding module and extract the sequence of word embeddings for each tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************************************** \n",
      " Word ids for the first batch (recall, it has 4 sentences, each column representing a sentence): \n",
      " tensor([[ 191,    4, 1273,   49],\n",
      "        [  14,   82,    2,    0],\n",
      "        [   2,   73,  215,   18],\n",
      "        [ 598,  145,   48,  215],\n",
      "        [  21,   21,   21,   21]]) \n",
      " **************************************************\n",
      "************************************************** \n",
      " Tweet input word embeddings size:  torch.Size([5, 4, 300]) \n",
      " **************************************************\n"
     ]
    }
   ],
   "source": [
    "# print tensor containing word ids for our batch\n",
    "print(\"*\"*50, \"\\n Word ids for the first batch (recall, it has 4 sentences, each column representing a sentence): \\n\", tweets.data, \"\\n\",\"*\"*50,)\n",
    "\n",
    "# feed the \"word ids\" tensor to the embedding module\n",
    "tweet_input_embeddings = embedding(tweets)\n",
    "\n",
    "# print the dimensions of the tweet_embeddings\n",
    "print(\"*\"*50, \"\\n Tweet input word embeddings size: \", tweet_input_embeddings.size(), \"\\n\",\"*\"*50,) \n",
    "# first dimension - sequence length: number of words per example (same across the whole batch, after padding) --> max_seq = 22\n",
    "# second dimension -  batch size / number of examples in the batch --> 4\n",
    "# third dimension - number of dimensions in the word vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's actually view the actual word embeddings tensor for this batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************************************** \n",
      " Embeddings for the first batch: \n",
      " tensor([[[-1.2432, -0.2715, -0.0181,  ..., -1.0014, -1.8168, -0.3740],\n",
      "         [-0.6846,  0.6532,  1.5913,  ..., -1.6802,  0.0289,  0.2598],\n",
      "         [ 1.4300,  2.2923, -0.2231,  ...,  1.0980, -0.6193,  0.3053],\n",
      "         [ 1.4549, -1.4750, -0.1812,  ...,  0.2424, -1.3213, -0.4096]],\n",
      "\n",
      "        [[-0.7315, -0.3592,  0.6204,  ..., -1.1071, -1.1912, -0.7095],\n",
      "         [-0.7025, -0.4854,  0.1471,  ...,  1.1662,  1.4169,  0.0226],\n",
      "         [-0.3565,  1.1279, -2.4257,  ..., -0.2974,  1.6922,  0.4019],\n",
      "         [ 0.5930,  2.0758,  0.8884,  ...,  1.7110,  0.1722, -1.8931]],\n",
      "\n",
      "        [[-0.3565,  1.1279, -2.4257,  ..., -0.2974,  1.6922,  0.4019],\n",
      "         [-0.5310,  1.9409, -0.7649,  ...,  0.4448,  0.5486,  0.3169],\n",
      "         [ 0.7153,  1.8802,  1.1964,  ..., -1.1083,  0.1905, -0.6592],\n",
      "         [-1.6032,  1.0348, -0.6736,  ...,  0.0524,  0.3046,  0.3782]],\n",
      "\n",
      "        [[-1.6432,  0.7260, -0.2340,  ..., -1.7118, -0.6208, -0.9128],\n",
      "         [ 0.3954, -1.2081, -1.1395,  ..., -0.8881, -0.0517, -1.0531],\n",
      "         [-1.1929,  1.0379, -0.5464,  ..., -1.9755, -0.2074, -1.6441],\n",
      "         [ 0.7153,  1.8802,  1.1964,  ..., -1.1083,  0.1905, -0.6592]],\n",
      "\n",
      "        [[ 3.4824,  0.1891,  1.3651,  ..., -1.6302,  0.3292,  0.5504],\n",
      "         [ 3.4824,  0.1891,  1.3651,  ..., -1.6302,  0.3292,  0.5504],\n",
      "         [ 3.4824,  0.1891,  1.3651,  ..., -1.6302,  0.3292,  0.5504],\n",
      "         [ 3.4824,  0.1891,  1.3651,  ..., -1.6302,  0.3292,  0.5504]]],\n",
      "       grad_fn=<EmbeddingBackward>) \n",
      " **************************************************\n"
     ]
    }
   ],
   "source": [
    "print(\"*\"*50, \"\\n Embeddings for the first batch: \\n\", tweet_input_embeddings, \"\\n\",\"*\"*50,) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we are seeing is the actual word vectors representing each of the 4 sentences (i.e., whole batch).\n",
    "This is dimension 2 in ``tweet_input_embeddings``. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_input_embeddings.size()[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned, ``max_seq length`` for this batch is ``20``, which is dimension 1 (indexed as 0 in Pytorch, similar to Python) \n",
    "in ``tweet_input_embeddings``: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_input_embeddings.size()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, dimension 3 in ``tweet_input_embeddings`` (indexed as 2) is the size of the word vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_input_embeddings.size()[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the vector for the ``first word`` in the ``first sentence`` in the batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 300])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_input_embeddings[:1, :1, :].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.2432e+00, -2.7152e-01, -1.8090e-02,  4.6624e-01,  1.9280e+00,\n",
       "          -8.5559e-01,  1.1160e-01, -4.5979e-01, -2.4328e-01, -1.8121e-01,\n",
       "           8.6533e-01, -9.2160e-01,  8.7791e-01, -8.6334e-01,  8.6939e-01,\n",
       "           2.1424e+00,  9.6241e-01, -8.9091e-01, -3.6165e-01,  9.8111e-01,\n",
       "           1.3783e+00,  8.7970e-01, -9.0856e-01, -7.0408e-01,  5.3429e-01,\n",
       "          -8.7536e-01, -8.9472e-01, -2.8145e-01,  8.5082e-01,  8.8575e-01,\n",
       "           2.6452e+00,  2.5515e-01,  1.0649e+00,  1.3652e+00, -2.9598e-03,\n",
       "          -1.6033e+00, -9.5273e-01,  7.5412e-01, -5.3374e-01, -8.3862e-01,\n",
       "           8.5092e-01,  1.7674e+00,  1.7836e+00,  5.8056e-01,  6.4203e-01,\n",
       "          -2.5032e-02,  9.3759e-01, -1.5349e-02, -2.6445e-02, -1.1334e-01,\n",
       "          -4.6490e-01, -1.2272e+00, -1.2346e-01, -1.9751e+00, -6.5784e-01,\n",
       "           9.1444e-01,  1.4415e+00, -2.8268e-01,  1.4209e-01,  1.1045e+00,\n",
       "          -2.2767e-01, -1.3139e+00,  5.3333e-02, -4.4247e-01, -9.5348e-01,\n",
       "          -4.1290e-01, -2.2611e+00,  3.7780e-01, -1.1556e+00, -2.0495e+00,\n",
       "          -8.4359e-02, -1.4386e+00,  1.5902e+00, -8.1019e-01,  4.3122e-01,\n",
       "           5.2393e-01,  7.1745e-01, -1.0641e+00,  2.0967e+00, -5.7479e-01,\n",
       "          -1.3914e+00, -8.1958e-01, -8.8745e-01,  1.5997e+00,  1.3046e-01,\n",
       "          -9.3867e-01, -1.2299e-01, -3.0564e-01, -3.3326e-01,  1.2226e+00,\n",
       "          -3.3594e-01,  1.6306e+00,  5.3895e-01, -1.9416e+00,  1.8300e-02,\n",
       "          -1.7071e-02,  2.0133e-01,  1.0415e+00,  2.8766e+00,  1.0414e+00,\n",
       "          -1.0891e+00, -2.7355e-01,  8.3021e-01, -4.5971e-01,  1.4819e-02,\n",
       "           1.1304e+00, -5.2045e-01, -1.4655e-01,  1.3343e+00, -1.1136e+00,\n",
       "           4.2878e-01,  2.8046e-01, -5.3829e-01,  1.9982e+00,  1.9022e+00,\n",
       "           6.1990e-01, -3.7010e-01,  5.4478e-01, -8.8723e-01,  5.1519e-02,\n",
       "          -1.2013e+00, -1.3941e+00, -4.1587e-01,  1.1203e+00,  1.8803e-01,\n",
       "          -6.1908e-01,  4.4689e-01, -7.6103e-01,  7.5122e-01,  7.1435e-01,\n",
       "           2.4641e-01, -1.9123e+00, -1.7411e+00, -2.0725e-01, -5.1718e-01,\n",
       "           3.2759e-01, -1.9864e+00,  1.6243e+00, -1.0368e-01,  1.7767e+00,\n",
       "           1.9791e-02,  7.5851e-01, -2.7615e-01, -6.8646e-01, -4.5279e-02,\n",
       "           1.7525e+00, -6.7747e-01, -6.6501e-01, -1.0209e+00, -1.1013e+00,\n",
       "          -9.7038e-01, -1.2576e+00, -9.9718e-01, -1.2612e+00, -3.7522e-01,\n",
       "          -1.7324e+00, -8.1867e-01,  3.2725e-01, -1.2562e+00, -1.3259e-01,\n",
       "          -2.7807e-01, -1.9746e+00, -2.5276e+00, -4.9318e-01,  1.8070e+00,\n",
       "          -6.5303e-02, -8.2742e-01,  1.8099e+00, -2.5916e-03,  4.3919e-01,\n",
       "           4.8160e-01, -1.0136e-01, -9.5915e-01, -1.8386e+00, -2.5975e-01,\n",
       "          -1.6851e+00, -8.3557e-01, -1.6609e+00,  4.2783e-01,  3.8011e-01,\n",
       "           3.4352e-01, -2.7238e-02,  3.3083e-01, -1.1354e+00, -7.2790e-01,\n",
       "           8.9852e-01, -1.6671e+00,  2.0217e-01,  1.4397e+00, -7.7125e-01,\n",
       "          -7.7025e-01,  1.9362e+00, -5.6935e-01, -1.0336e+00,  2.8104e-01,\n",
       "          -7.4720e-01,  3.0694e-01, -9.5149e-01,  3.8832e-03,  1.5254e-01,\n",
       "           1.4487e-01,  5.4510e-01, -1.0619e+00,  8.0501e-01, -1.9374e-01,\n",
       "           7.8545e-01,  9.0022e-01,  9.3384e-01,  1.4235e+00,  3.7096e-01,\n",
       "          -5.0060e-01, -1.5401e+00,  1.2661e+00,  5.6727e-01,  8.4010e-02,\n",
       "           2.1789e-01, -2.7961e-01,  1.6149e+00, -3.8511e-01, -9.0715e-01,\n",
       "           2.8584e-01,  2.6557e-01,  6.2878e-03, -3.9250e-01, -4.3231e-01,\n",
       "          -4.5934e-01, -9.7123e-01, -8.2620e-01, -1.5473e+00,  1.0004e+00,\n",
       "           6.7004e-01,  1.5814e-01, -1.1661e+00,  5.2224e-01, -4.0321e-01,\n",
       "           4.0385e-01, -1.6656e+00,  1.0259e+00,  3.8061e-01,  2.3311e-01,\n",
       "           1.7799e+00, -6.1616e-01, -9.0810e-01,  1.1231e+00,  3.6682e-01,\n",
       "           1.2592e+00,  1.0512e-01,  1.0536e+00,  1.9628e+00, -1.1674e+00,\n",
       "           8.1398e-01,  1.9098e+00, -2.5828e-01, -4.6435e-01, -1.8913e-01,\n",
       "          -1.6692e-02,  4.7448e-01, -1.2518e+00, -4.1251e-01,  2.1558e-01,\n",
       "           1.1916e+00,  7.0053e-02,  1.6083e+00, -1.1849e-01, -6.0599e-01,\n",
       "          -2.5227e+00,  1.0437e-01, -1.8825e-01, -5.5515e-01, -1.9645e-01,\n",
       "           1.8286e+00, -1.5474e+00,  4.0115e-01,  1.1009e+00, -8.3337e-01,\n",
       "           9.3852e-01, -4.7609e-01,  1.6730e+00,  6.8008e-01, -4.5323e-01,\n",
       "          -4.7471e-01,  3.9259e-01,  1.6747e+00,  4.8995e-01, -1.3560e-01,\n",
       "          -5.6969e-01,  8.6768e-01,  1.2161e-03, -3.2619e+00,  5.4658e-01,\n",
       "           9.1870e-01, -1.5353e+00,  1.8320e+00,  2.6925e-02, -2.6392e-01,\n",
       "           2.1010e-01,  1.0247e+00, -1.0014e+00, -1.8168e+00, -3.7399e-01]]],\n",
       "       grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_input_embeddings[:1, :1, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the ``first 5 dimensions`` of that same ``first word`` of the ``first sentence``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.2432, -0.2715, -0.0181,  0.4662,  1.9280]]],\n",
       "       grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_input_embeddings[:1, :1, :5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following shows you the ``first 5 dimensions`` of the ``first word`` from ``each of the 4 sentences``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.2432, -0.2715, -0.0181,  0.4662,  1.9280],\n",
       "         [-0.6846,  0.6532,  1.5913, -1.2416, -0.8411],\n",
       "         [ 1.4300,  2.2923, -0.2231, -0.7895,  1.4527],\n",
       "         [ 1.4549, -1.4750, -0.1812,  0.0171, -2.3973]]],\n",
       "       grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_input_embeddings[:1, :, :5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following shows you the ``last 7 dimensions`` of the ``last word`` from ``the last sentence``. Enjoy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.5736,  0.1155, -1.7286,  0.4320, -1.6302,  0.3292,  0.5504]]],\n",
       "       grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_input_embeddings[-1:, 3:, -7:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be passing the embedding vector for the first word, Simultaneously for each senetence in the batch, to the RNN. But let's now define an RNN module first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "define the RNN module\n",
    "\"\"\"\n",
    "# first input - number of dimesnions for word vectors for a vector x (300, size of the word embedding)\n",
    "# second input - number of nodes in hidden state h_t (50, size of the hidden layer)\n",
    "# third input - number of recurrent layers (we set it to 1)\n",
    "rnn = nn.RNN(input_size=300, hidden_size=50, num_layers=1) # input_size, hidden_size, num_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now pass the ``tweet_input_embeddings`` (representations of words in our batch) to RNN. Before we do, we need to know RNN also *optionally* takes a parameter for the ``initial hidden state h0`` (that is, the hidden state we will input to the model before it runs. It is just something that we need to start the model. If we don't provide it, Pytorch will just initialize h0 to a tensor of zeros. \n",
    "\n",
    "Let's construct an ``initial hidden state h0``. Note the shape of its tensor, and what each of the 3 parameters it takes mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape as as expected:  torch.Size([1, 4, 50])\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "hidden layer at time-step 0 (h_0)\n",
    "\"\"\"\n",
    "# first dimension - number of RNN layers (1)\n",
    "# second dimension - number of examples/sentences in a batch\n",
    "# third dimension - number of nodes in hidden layer (50, size of the hidden layer)\n",
    "h0 = torch.randn(1, 4, 50)\n",
    "print(\"The shape as as expected: \", h0.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us feed both the hidden representation constructed above and tweet embeddings to our RNN model.\n",
    "We will get back two objects ``output`` and ``hn`` that we will need to understand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "forward propagation over the RNN model\n",
    "\"\"\"\n",
    "output, hn = rnn(tweet_input_embeddings, h0) # h0 is optional input, defaults to tensor of 0's of apprpriate size (num_layers, batch, hidden_size) when not provided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But what is ``output``? Well, let's inspect its shape first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output size:  torch.Size([5, 4, 50])\n"
     ]
    }
   ],
   "source": [
    "# output = seq_len, batch, hidden_size (output features from last layer of RNN)\n",
    "print(\"output size: \", output.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's what we need to know about ``output``:\n",
    "- The first dimension in the ``output`` tensor is the ``max_seq length`` (22). \n",
    "- The second dimension is ``batch_size`` (the number of examples/sentences in our batch = 4).\n",
    "- The third dimension is the ``size of nodes/units`` in our hidden layer (=50). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's what we need to know about ``hn``:\n",
    "- ``hn`` is a tensor of shape (num_layers, batch_size, hidden_size / number of hidden layer nodes) containing the hidden state for the last ``time step`` \n",
    "(``t = max_seq_length``)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last hidden state size:  torch.Size([1, 4, 50])\n"
     ]
    }
   ],
   "source": [
    "# h_n = num_layers, batch, hidden_size (hidden state for t=seq_len or hidden state at last timestep)\n",
    "print(\"last hidden state size: \", hn.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can take the output representation for a tweet after processing the last token (t=seq_len or last timestep) and call the resulting representation as the tweet representation that **\"summarizes\" the information present** in the tweet. This tweet representation can further be used for a useful task like tweet classification (we will try out sentiment analysis later in this tutorial) by adding a classification module on top of the tweet representation.\n",
    "\n",
    "Let us compute the final tweet representation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweet output embeddings size:  torch.Size([4, 50])\n"
     ]
    }
   ],
   "source": [
    "tweet_output_embeddings = output[-1,:,:] # -1 fetches the embeddings from the last timestep\n",
    "print(\"tweet output embeddings size: \", tweet_output_embeddings.size())\n",
    "# first dimension - number of tweets in the batch (4)\n",
    "# second dimension - number of features in hidden state h_t (50, size of the hidden layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multilayered RNN\n",
    "\n",
    "For some applications, we may need more than one hidden layer for RNN to model the information flow. Adding more layers requires fews changes.\n",
    "\n",
    "Firstly, we change the ``num_layers`` argument to reflect the number of layers we want during the RNN module definition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "define the RNN module\n",
    "\"\"\"\n",
    "# first input - number of dimesnions for word vectors for a vector x (300, size of the word embedding)\n",
    "# second input - number of nodes in hidden layer (50, size of the hidden layer)\n",
    "# third input - number of recurrent layers (we set it to 1)\n",
    "rnn = nn.RNN(input_size=300, hidden_size=50, num_layers=2) # input_size, hidden_size, num_layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to single layered RNN, Multilayered RNN module takes two inputs: the ``initial hidden state h0`` for each element in the batch (at ``time step t=0``) and the ``input features`` (``tweet_input_embeddings`` in our case).\n",
    "\n",
    "Let us construct the new initial hidden state for a 2 layered RNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape as as expected:  torch.Size([2, 4, 50])\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "hidden layer at time-step 0 (h_0)\n",
    "\"\"\"\n",
    "# first dimension - number of RNN layers (2)\n",
    "# second dimension - number of examples/sentences in a batch (4)\n",
    "# third dimension - number of nodes in hidden layer (50, size of the hidden layer)\n",
    "h0 = torch.randn(2, 4, 50)\n",
    "print(\"The shape as as expected: \", h0.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us feed both the hidden representation constructed above and tweet embeddings to our RNN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 4, 300])\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "forward propagation over the RNN model\n",
    "\"\"\"\n",
    "print(tweet_input_embeddings.shape)\n",
    "output, hn = rnn(tweet_input_embeddings, h0) # h0 is optional input, defaults to tensor of 0's when not provided"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``output`` tensor contains the output features $h_t$ from the last layer of the RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output size:  torch.Size([5, 4, 50])\n"
     ]
    }
   ],
   "source": [
    "# output = seq_len, batch, hidden_size (output features from last layer of RNN)\n",
    "print(\"output size: \", output.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``hn`` is a tensor of shape (num_layers, batch_size, hidden_size / number of nodes in a hidden layer) containing the hidden state for last time step ``t = max_seq_len`` for the ``2 layered RNN``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last hidden state size:  torch.Size([2, 4, 50])\n"
     ]
    }
   ],
   "source": [
    "# h_n = num_layers, batch, hidden_size (hidden state for t=seq_len or hidden state at last timestep)\n",
    "print(\"last hidden state size: \", hn.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update Discussion \n",
    "\n",
    "#### Update START\n",
    "\n",
    "Detail in the document: https://pytorch.org/docs/stable/nn.html#rnn\n",
    "\n",
    "Actually, `output` is tensor containing the output features (h_t) from the last layer of the RNN, for each t. Namely, `output` returns all the hidden states of all time steps from the last layer of the RNN. Hence, the last element of `output` is `h_n`. \n",
    "Let us print them out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last element of output:\n",
      " tensor([[-0.0088,  0.3960,  0.0280, -0.0443,  0.4824,  0.5359, -0.4994, -0.0123,\n",
      "          0.1963, -0.4505,  0.5593, -0.7822,  0.3600, -0.3049, -0.4489,  0.2069,\n",
      "          0.0067,  0.3375, -0.6773,  0.2435,  0.6068,  0.2218,  0.4962,  0.1942,\n",
      "         -0.3141, -0.6673,  0.1694, -0.2926,  0.6901,  0.6797,  0.4133,  0.0757,\n",
      "          0.1661,  0.5802, -0.1276,  0.1391, -0.4382, -0.1566,  0.3635,  0.2153,\n",
      "          0.6576,  0.1782,  0.5426, -0.3677, -0.5743, -0.5453,  0.5656, -0.4537,\n",
      "          0.0917,  0.3499],\n",
      "        [-0.6214,  0.1181,  0.0932, -0.5903,  0.4482,  0.5844,  0.1375,  0.0546,\n",
      "          0.0710, -0.1835,  0.5116, -0.4337,  0.3386,  0.1007, -0.6376,  0.3560,\n",
      "         -0.0050,  0.4134, -0.6103,  0.6383,  0.6347, -0.5891,  0.3946,  0.5271,\n",
      "         -0.2664, -0.4487, -0.1191,  0.1609,  0.3779,  0.5897,  0.6471, -0.1599,\n",
      "          0.2420,  0.4107, -0.3161,  0.3179, -0.4632, -0.6740,  0.4800, -0.2133,\n",
      "          0.6572,  0.1053,  0.3832,  0.2134, -0.3030, -0.1937, -0.0651, -0.3115,\n",
      "         -0.0591, -0.1481],\n",
      "        [-0.7063,  0.7473,  0.1159, -0.1463,  0.7121,  0.3265, -0.1860, -0.3488,\n",
      "          0.4646, -0.5026,  0.4980, -0.1606,  0.0257,  0.3528, -0.4915,  0.2050,\n",
      "         -0.3533,  0.3668, -0.7866,  0.6810,  0.8548, -0.4424,  0.6782,  0.4254,\n",
      "         -0.4946, -0.5659,  0.1543, -0.1453,  0.2826,  0.3587,  0.3240, -0.0187,\n",
      "          0.2639,  0.4793, -0.5862,  0.5512,  0.2137, -0.4688,  0.5769, -0.0153,\n",
      "          0.2074,  0.6534, -0.2192, -0.2267, -0.6755, -0.5496,  0.1890, -0.4728,\n",
      "          0.5088,  0.3695],\n",
      "        [-0.2464,  0.5105,  0.0226, -0.2039,  0.0743,  0.6216, -0.0538,  0.1290,\n",
      "          0.2745, -0.4335,  0.1151, -0.5466, -0.0164, -0.0351, -0.6127,  0.4521,\n",
      "          0.1515,  0.3092, -0.1403,  0.3631,  0.5588, -0.2499, -0.0194, -0.0998,\n",
      "          0.3535, -0.5375, -0.0281, -0.1165,  0.1489, -0.0300,  0.7096, -0.3482,\n",
      "          0.1432,  0.3583,  0.0655,  0.0484, -0.7581, -0.8376,  0.5051, -0.1105,\n",
      "          0.8009,  0.1057,  0.2892,  0.0405, -0.4389, -0.4875,  0.2419, -0.5832,\n",
      "          0.1662,  0.2255]], grad_fn=<SelectBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(\"last element of output:\\n\", output[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then Let us print out the last hidden state of last layer. \n",
    "Notice, we have two RNN layer, we only want to use the hidden state of last layer. \n",
    "You can find the these value are same as `output[-1]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last hidden state h_n:\n",
      " tensor([[-0.0088,  0.3960,  0.0280, -0.0443,  0.4824,  0.5359, -0.4994, -0.0123,\n",
      "          0.1963, -0.4505,  0.5593, -0.7822,  0.3600, -0.3049, -0.4489,  0.2069,\n",
      "          0.0067,  0.3375, -0.6773,  0.2435,  0.6068,  0.2218,  0.4962,  0.1942,\n",
      "         -0.3141, -0.6673,  0.1694, -0.2926,  0.6901,  0.6797,  0.4133,  0.0757,\n",
      "          0.1661,  0.5802, -0.1276,  0.1391, -0.4382, -0.1566,  0.3635,  0.2153,\n",
      "          0.6576,  0.1782,  0.5426, -0.3677, -0.5743, -0.5453,  0.5656, -0.4537,\n",
      "          0.0917,  0.3499],\n",
      "        [-0.6214,  0.1181,  0.0932, -0.5903,  0.4482,  0.5844,  0.1375,  0.0546,\n",
      "          0.0710, -0.1835,  0.5116, -0.4337,  0.3386,  0.1007, -0.6376,  0.3560,\n",
      "         -0.0050,  0.4134, -0.6103,  0.6383,  0.6347, -0.5891,  0.3946,  0.5271,\n",
      "         -0.2664, -0.4487, -0.1191,  0.1609,  0.3779,  0.5897,  0.6471, -0.1599,\n",
      "          0.2420,  0.4107, -0.3161,  0.3179, -0.4632, -0.6740,  0.4800, -0.2133,\n",
      "          0.6572,  0.1053,  0.3832,  0.2134, -0.3030, -0.1937, -0.0651, -0.3115,\n",
      "         -0.0591, -0.1481],\n",
      "        [-0.7063,  0.7473,  0.1159, -0.1463,  0.7121,  0.3265, -0.1860, -0.3488,\n",
      "          0.4646, -0.5026,  0.4980, -0.1606,  0.0257,  0.3528, -0.4915,  0.2050,\n",
      "         -0.3533,  0.3668, -0.7866,  0.6810,  0.8548, -0.4424,  0.6782,  0.4254,\n",
      "         -0.4946, -0.5659,  0.1543, -0.1453,  0.2826,  0.3587,  0.3240, -0.0187,\n",
      "          0.2639,  0.4793, -0.5862,  0.5512,  0.2137, -0.4688,  0.5769, -0.0153,\n",
      "          0.2074,  0.6534, -0.2192, -0.2267, -0.6755, -0.5496,  0.1890, -0.4728,\n",
      "          0.5088,  0.3695],\n",
      "        [-0.2464,  0.5105,  0.0226, -0.2039,  0.0743,  0.6216, -0.0538,  0.1290,\n",
      "          0.2745, -0.4335,  0.1151, -0.5466, -0.0164, -0.0351, -0.6127,  0.4521,\n",
      "          0.1515,  0.3092, -0.1403,  0.3631,  0.5588, -0.2499, -0.0194, -0.0998,\n",
      "          0.3535, -0.5375, -0.0281, -0.1165,  0.1489, -0.0300,  0.7096, -0.3482,\n",
      "          0.1432,  0.3583,  0.0655,  0.0484, -0.7581, -0.8376,  0.5051, -0.1105,\n",
      "          0.8009,  0.1057,  0.2892,  0.0405, -0.4389, -0.4875,  0.2419, -0.5832,\n",
      "          0.1662,  0.2255]], grad_fn=<SelectBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(\"last hidden state h_n:\\n\", hn[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us compute the final tweet representation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweet output embeddings size:  torch.Size([4, 50])\n"
     ]
    }
   ],
   "source": [
    "tweet_output_embeddings = output[-1,:,:] # -1 fetches the embeddings from the last timestep\n",
    "print(\"tweet output embeddings size: \", tweet_output_embeddings.size())\n",
    "# first dimension - number of tweets in the batch (4)\n",
    "# second dimension - number of features in hidden state h_t (50, size of the hidden layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.4750, -0.0382, -0.3033, -0.6008,  0.0755, -0.5770,  0.2852,  0.5443,\n",
       "         -0.1630,  0.3962, -0.1597,  0.0470,  0.0747,  0.2451,  0.2394,  0.2970,\n",
       "          0.1088,  0.1043, -0.1865,  0.4555,  0.0305, -0.4840,  0.1716, -0.0013,\n",
       "         -0.6560, -0.4392, -0.0558, -0.0386,  0.2135, -0.0366, -0.2997,  0.4148,\n",
       "         -0.4572,  0.1798,  0.3508, -0.1521,  0.3993, -0.2273,  0.2632,  0.4344,\n",
       "         -0.6013,  0.6624, -0.1212, -0.2163, -0.3276,  0.3365,  0.3889,  0.2173,\n",
       "          0.4296, -0.5430],\n",
       "        [-0.5174, -0.0487, -0.3424, -0.5970,  0.0584, -0.5848,  0.3159,  0.5250,\n",
       "         -0.1840,  0.4092, -0.2140,  0.0445,  0.0902,  0.2211,  0.2354,  0.2855,\n",
       "          0.1305,  0.1047, -0.1948,  0.4687,  0.0277, -0.4609,  0.1987, -0.0165,\n",
       "         -0.6700, -0.4564, -0.0676, -0.0252,  0.2396, -0.0109, -0.2977,  0.3746,\n",
       "         -0.4520,  0.1945,  0.3467, -0.1671,  0.3785, -0.2013,  0.2746,  0.4617,\n",
       "         -0.5988,  0.6594, -0.1220, -0.2084, -0.3203,  0.3613,  0.4038,  0.2139,\n",
       "          0.4086, -0.5347],\n",
       "        [-0.2831, -0.6447,  0.0701,  0.3379,  0.1345,  0.1090,  0.1479, -0.1653,\n",
       "          0.4404,  0.1913,  0.0392, -0.2340,  0.0986, -0.2735,  0.2569,  0.1629,\n",
       "         -0.6266,  0.0612,  0.2654,  0.0838, -0.4318,  0.1201,  0.5075,  0.0196,\n",
       "         -0.4185, -0.7226, -0.3859,  0.1619,  0.3950,  0.0565, -0.6048,  0.5378,\n",
       "          0.0262,  0.5344,  0.4607, -0.3480,  0.5869, -0.2069,  0.0387,  0.2953,\n",
       "         -0.2933, -0.1380, -0.5021,  0.3977,  0.1337,  0.1001,  0.5376,  0.4259,\n",
       "          0.2199, -0.6592],\n",
       "        [-0.3122, -0.0542, -0.0075, -0.5962,  0.0412, -0.7115,  0.3424,  0.6110,\n",
       "         -0.1094,  0.4935, -0.3251,  0.1155, -0.0940, -0.0353,  0.2099,  0.2694,\n",
       "         -0.2784,  0.2728, -0.0078,  0.3932, -0.0387, -0.1980,  0.2216,  0.1325,\n",
       "         -0.5353, -0.4795, -0.1510, -0.1208,  0.2749, -0.1976, -0.3145,  0.2719,\n",
       "         -0.5346,  0.0641,  0.1632, -0.1128,  0.2304, -0.3027,  0.0448,  0.3827,\n",
       "         -0.5940,  0.7198, -0.1543, -0.3336, -0.3103,  0.2698,  0.4119,  0.1981,\n",
       "          0.1757, -0.6053]], grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_output_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Update END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN for Sentiment Analysis\n",
    "\n",
    "In this section we will implement RNN for classifying the sentiment of the tweet (same task used in our previous feedforward neural networks tutorial).\n",
    "\n",
    "We will pick up most of the functions from our feedforward neural networks code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all the necessary imports\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "\n",
    "# set the seed\n",
    "manual_seed = 123\n",
    "torch.manual_seed(manual_seed)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_gpu = torch.cuda.device_count()\n",
    "if n_gpu > 0:\n",
    "  torch.cuda.manual_seed(manual_seed)\n",
    "\n",
    "# hyperparameters\n",
    "MAX_EPOCHS = 5\n",
    "LEARNING_RATE = 0.3\n",
    "NUM_CLASSES = 3\n",
    "EMBEDDING_SIZE = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can define the full RNN model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "create a model for RNN\n",
    "\"\"\"\n",
    "class RNNmodel(nn.Module):\n",
    "  \n",
    "  def __init__(self, embedding_size, vocab_size, output_size, hidden_size, num_layers):\n",
    "    # In the constructor we define the layers for our model\n",
    "    super(RNNmodel, self).__init__()\n",
    "    # word embedding lookup table\n",
    "    self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_size, sparse=True)\n",
    "    # core RNN module\n",
    "    self.rnn_layer = nn.RNN(input_size=embedding_size, hidden_size=hidden_size, num_layers=num_layers) \n",
    "    # activation function\n",
    "    self.activation_fn = nn.ReLU()\n",
    "    # classification related modules\n",
    "    self.linear_layer = nn.Linear(hidden_size, output_size) \n",
    "    self.softmax_layer = nn.LogSoftmax(dim=0)\n",
    "  \n",
    "  def forward(self, x):\n",
    "    # In the forward function we define the forward propagation logic\n",
    "    out = self.embedding(x)\n",
    "    out, _ = self.rnn_layer(out) # since we are not feeding h_0 explicitly, h_0 will be initialized to zeros by default\n",
    "    # classify based on the hidden representation after RNN processes the last token\n",
    "    out = out[-1]\n",
    "    out = self.activation_fn(out)\n",
    "    out = self.linear_layer(out)\n",
    "    out = self.softmax_layer(out) # accepts 2D or more dimensional inputs\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some additional hyperparameters for RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters of RNN\n",
    "HIDDEN_SIZE = 50\n",
    "NUM_LAYERS = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rest of the pipeline looks similar to our feedforward neural networks code (except that we are using **torchtext** instead of **DataLoader**):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "def train(loader):\n",
    "    total_loss = 0.0\n",
    "    # iterate throught the data loader\n",
    "    num_sample = 0\n",
    "    for batch in loader:\n",
    "        # load the current batch\n",
    "        batch_input = batch.tweet\n",
    "        batch_output = batch.label\n",
    "        \n",
    "        batch_input = batch_input.to(device)\n",
    "        batch_output = batch_output.to(device)\n",
    "        # forward propagation\n",
    "        # pass the data through the model\n",
    "        model_outputs = model(batch_input)\n",
    "        # compute the loss\n",
    "        cur_loss = criterion(model_outputs, batch_output)\n",
    "        total_loss += cur_loss.item()\n",
    "\n",
    "        # backward propagation (compute the gradients and update the model)\n",
    "        # clear the buffer\n",
    "        optimizer.zero_grad()\n",
    "        # compute the gradients\n",
    "        cur_loss.backward()\n",
    "        # update the weights\n",
    "        optimizer.step()\n",
    "\n",
    "        num_sample += batch_output.shape[0]\n",
    "    return total_loss/num_sample\n",
    "\n",
    "# evaluation logic based on classification accuracy\n",
    "def evaluate(loader):\n",
    "    all_pred=[]\n",
    "    all_label = []\n",
    "    with torch.no_grad(): # impacts the autograd engine and deactivate it. reduces memory usage and speeds up computation\n",
    "        for batch in loader:\n",
    "             # load the current batch\n",
    "            batch_input = batch.tweet\n",
    "            batch_output = batch.label\n",
    "\n",
    "            batch_input = batch_input.to(device)\n",
    "            # forward propagation\n",
    "            # pass the data through the model\n",
    "            model_outputs = model(batch_input)\n",
    "            # identify the predicted class for each example in the batch\n",
    "            probabilities, predicted = torch.max(model_outputs.cpu().data, 1)\n",
    "            # put all the true labels and predictions to two lists\n",
    "            all_pred.extend(predicted)\n",
    "            all_label.extend(batch_output)\n",
    "            \n",
    "    accuracy = accuracy_score(all_label, all_pred)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us define the RNN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model\n",
    "model = RNNmodel(EMBEDDING_SIZE, VOCAB_SIZE, NUM_CLASSES, HIDDEN_SIZE, NUM_LAYERS) \n",
    "model.to(device)\n",
    "# define the loss function (last node of the graph)\n",
    "criterion = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We need to create a new directory 'ckpt/' to store our model checkpoint.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.mkdir(\"./ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let us perform the training. We will save our model and optimizer at end of each epoch.**\n",
    "\n",
    "\n",
    "You can find more information of saving and loading model [here](https://pytorch.org/tutorials/beginner/saving_loading_models.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 0.3452, Training Accuracy: 0.4048, Validation Accuracy: 0.3392\n",
      "Epoch [2/5], Loss: 0.3448, Training Accuracy: 0.4015, Validation Accuracy: 0.3972\n",
      "Epoch [3/5], Loss: 0.3449, Training Accuracy: 0.3438, Validation Accuracy: 0.3657\n",
      "Epoch [4/5], Loss: 0.3448, Training Accuracy: 0.3568, Validation Accuracy: 0.3442\n",
      "Epoch [5/5], Loss: 0.3450, Training Accuracy: 0.3432, Validation Accuracy: 0.3847\n"
     ]
    }
   ],
   "source": [
    "# create an instance of SGD with required hyperparameters\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# start the training\n",
    "for epoch in range(MAX_EPOCHS):\n",
    "    # train the model for one pass over the data\n",
    "    train_loss = train(train_iter)  \n",
    "    # compute the training accuracy\n",
    "    train_acc = evaluate(train_iter)\n",
    "    # compute the validation accuracy\n",
    "    val_acc = evaluate(val_iter)\n",
    "    \n",
    "    # print the loss for every epoch\n",
    "    print('Epoch [{}/{}], Loss: {:.4f}, Training Accuracy: {:.4f}, Validation Accuracy: {:.4f}'.format(epoch+1, MAX_EPOCHS, train_loss, train_acc, val_acc))\n",
    "    \n",
    "    # save model, optimizer, and number of epoch to a dictionary\n",
    "    model_save = {\n",
    "            'epoch': epoch,  # number of epoch\n",
    "            'model_state_dict': model.state_dict(), # model parameters \n",
    "            'optimizer_state_dict': optimizer.state_dict(), # save optimizer \n",
    "            'loss': train_loss # training loss\n",
    "            }\n",
    "    \n",
    "    # use torch.save to store \n",
    "    torch.save(model_save, \"./ckpt/model_{}.pt\".format(epoch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We trained the network only for 5 epochs, but it already overfits on validation set after epoch 2. \n",
    "In the coming sessions, we will look at methods to ``regularize`` the network (this will help us deal with overfitting)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load model checkpoint** \n",
    "\n",
    "When we have a trained model checkpint, we can load it using `torch.load()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNNmodel(\n",
      "  (embedding): Embedding(4876, 300, sparse=True)\n",
      "  (rnn_layer): RNN(300, 50, num_layers=2)\n",
      "  (activation_fn): ReLU()\n",
      "  (linear_layer): Linear(in_features=50, out_features=3, bias=True)\n",
      "  (softmax_layer): LogSoftmax()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# define a new model\n",
    "model2 = RNNmodel(EMBEDDING_SIZE, VOCAB_SIZE, NUM_CLASSES, HIDDEN_SIZE, NUM_LAYERS) \n",
    "# load checkpoint \n",
    "checkpoint = torch.load(\"./ckpt/model_1.pt\")\n",
    "# assign the parameters of checkpoint to this new model\n",
    "model2.load_state_dict(checkpoint['model_state_dict'])\n",
    "model2.to(device)\n",
    "\n",
    "print(model2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRUs\n",
    "\n",
    "Gated Recurrent Units (GRUs) are a variant of RNNs that use more complex units for activation. They are created to have more persistent memory thereby making them easier for RNNs to capture long-term dependencies. To learn the theory behind GRUs, we recommend: https://github.com/UBC-NLP/dlnlp2019/blob/master/slides/RNN.pdf \n",
    "\n",
    "GRU is defined by ``torch.nn.GRU`` module and its documentation can be fetched [here](https://pytorch.org/docs/stable/nn.html#torch.nn.GRU). Now let us define the GRU module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "define the GRU module\n",
    "\"\"\"\n",
    "# first input - number of word vector dimensions/embeddings\n",
    "# second input - number of nodes in hidden layer (50, size of the hidden layer)\n",
    "# third input - number of recurrent layers (2)\n",
    "gru_rnn = nn.GRU(input_size=300, hidden_size=50, num_layers=2) # input_size, hidden_size, num_layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to RNN, GRU module takes two inputs: *the initial hidden state for each element in the batch* (t=0) and the *input features* (``tweet_input_embeddings`` in our case).\n",
    "\n",
    "Let us feed both the initial hidden state and tweet embeddings to our GRU model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "forward propagation over the GRU model\n",
    "\"\"\"\n",
    "output, hn = gru_rnn(tweet_input_embeddings, h0) # h0 is optional input, defaults to tensor of 0's when not provided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``output`` tensor contqains the output features $h_t$ from the last layer of the GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output size:  torch.Size([20, 4, 50])\n"
     ]
    }
   ],
   "source": [
    "# output = seq_len, batch, hidden_size (output features from last layer of GRU)\n",
    "print(\"output size: \", output.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``hn`` is a tensor of shape (num_layers, batch_size, hidden_size / number of nodes in a hidden layer) containing the hidden state for last time step ``t = max_seq_len`` for the ``2 layered RNN``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last hidden state size:  torch.Size([2, 4, 50])\n"
     ]
    }
   ],
   "source": [
    "# h_n = num_layers, batch, hidden_size (hidden state for t=seq_len or hidden state at last timestep)\n",
    "print(\"last hidden state size: \", hn.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to RNN, you can compute the final tweet representation (representation from last hidden state for each tweet) as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweet output embeddings size:  torch.Size([4, 50])\n"
     ]
    }
   ],
   "source": [
    "tweet_output_embeddings = output[-1,:,:] # -1 fetches the embeddings from the last timestep\n",
    "print(\"tweet output embeddings size: \", tweet_output_embeddings.size())\n",
    "# first dimension - number of tweets in the batch (5)\n",
    "# second dimension - number of features in hidden state h_t (20, size of the hidden layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTMs\n",
    "\n",
    "Long short-term memory (LSTMs) are a variant of RNNs that use more complex units for activation. Similar to the spirit of GRU, they are created to have more persistent memory thereby making them easier for RNNs to capture long-term dependencies. To learn the theory behind GRUs, we recommend: https://github.com/UBC-NLP/dlnlp2019/blob/master/slides/RNN.pdf \n",
    "\n",
    "LSTM is defined by ``torch.nn.LSTM`` module and its documentation can be fetched [here](https://pytorch.org/docs/stable/nn.html#torch.nn.LSTM). Now let us define the LSTM module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "define the LSTM module\n",
    "\"\"\"\n",
    "# first input - number of features in x (300, size of the word embedding)\n",
    "# second input - number of number of nodes in a hidden layer (50)\n",
    "# third input - number of recurrent layers (2)\n",
    "lstm_rnn = nn.LSTM(input_size=300, hidden_size=50, num_layers=2) # input_size, hidden_size, num_layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike RNN and GRU, LSTM module takes three inputs: the initial hidden state for each element in the batch (t=0), the input features (tweet_input_embeddings in our case) and initial cell state for each element in the batch.\n",
    "\n",
    "Let us construct the initial cell state (this construction is similar to that of initial hidden state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "cell state at time-step 0 (h_0)\n",
    "\"\"\"\n",
    "# first dimension - number of LSTM layers (2)\n",
    "# second dimension - batch_size (# of tweets/examples/sentences)\n",
    "# third dimension - hidden_size / number of nodes in a hidden layer (50)\n",
    "c0 = torch.randn(2, 4, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us feed the initial hidden state, initial cell state and tweet embeddings to our LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "forward propagation over the LSTM model\n",
    "\"\"\"\n",
    "output, (hn, cn) = lstm_rnn(tweet_input_embeddings, None) # h0 and c0 is optional input, defaults to tensor of 0's when not provided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``output`` tensor contains the output features $h_t$ from the last layer of the LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output size:  torch.Size([20, 4, 50])\n"
     ]
    }
   ],
   "source": [
    "# output = seq_len, batch_size, hidden_size (output features from last layer of LSTM)\n",
    "print(\"output size: \", output.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``hn`` is a tensor of shape (num_layers, batch, hidden_size) containing the hidden state for t = seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last hidden state size:  torch.Size([2, 5, 20])\n"
     ]
    }
   ],
   "source": [
    "# h_n = num_layers, batch, hidden_size (hidden state for t=seq_len or hidden state at last timestep)\n",
    "print(\"last hidden state size: \", hn.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``cn`` is a tensor of shape (num_layers, batch, hidden_size) containing the cell state for t = seq_len."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last cell state size:  torch.Size([2, 4, 50])\n"
     ]
    }
   ],
   "source": [
    "# c_n = num_layers, batch_size, hidden_size (cell state for t=seq_len or cell state at last timestep)\n",
    "print(\"last cell state size: \", hn.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to RNN and GRU, you can compute the final tweet representation (representation from last hidden state for each tweet) as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweet output embeddings size:  torch.Size([4, 50])\n"
     ]
    }
   ],
   "source": [
    "tweet_output_embeddings = output[-1,:,:] # -1 fetches the embeddings from the last timestep\n",
    "print(\"tweet output embeddings size: \", tweet_output_embeddings.size())\n",
    "# first dimension - number of tweets in the batch (4)\n",
    "# second dimension - number of features in hidden state h_t (50, size of the hidden layer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
