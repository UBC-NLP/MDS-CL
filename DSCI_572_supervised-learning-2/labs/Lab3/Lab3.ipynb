{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework 3 - Supervised Learning II - MDS Computational Linguistics\n",
    "\n",
    "### Assignment Topics\n",
    "- Recurrent Neural Networks\n",
    "- Long Short-Term Memory\n",
    "- Saving and Loading NN models using Pytorch\n",
    "- Very-short answer questions\n",
    "\n",
    "### Software Requirements\n",
    "- Python (>=3.6)\n",
    "- PyTorch (>=1.2.0) \n",
    "- Jupyter (latest)\n",
    "\n",
    "### Submission Info.\n",
    "- Due Date: 1/30/21 11:59 PM PST\n",
    "\n",
    "## Getting Started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all the necessary imports\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torchtext\n",
    "from torchtext.data import Field, LabelField\n",
    "from torchtext.data import TabularDataset\n",
    "from torchtext.data import Iterator, BucketIterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the seed\n",
    "manual_seed = 572\n",
    "torch.manual_seed(manual_seed)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tidy Submission\n",
    "\n",
    "rubric={mechanics:1}\n",
    "\n",
    "To get the marks for tidy submission:\n",
    "- Submit the assignment by filling in this jupyter notebook with your answers embedded\n",
    "- Be sure to follow the [general lab instructions](https://ubc-mds.github.io/resources_pages/general_lab_instructions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T3 - CNN's for text classification\n",
    "\n",
    "Convolutional Neural Networks (CNNs) are essentially a special case of a normal feed forward network, instead of being \"densely\" connected (note you may see people refer to Feed Forward networks as \"dense layers\"), nodes in CNNs connect to a smaller set of nodes, defined by the \"filters size\" of the network. CNNs thus have a smaller local windows to look at data, but make up for it by generally using many additional filters, which might be able to learn different aspects of the data. These networks turn out to be extremely useful for processing images, audio, and anything with some sort of spatial properties to the data.\n",
    "\n",
    "It turns out they can also be used for any sentence classification task. Words in a sentence it turns out have a sort of 1D spatial ordering, which means some classification tasks can benefit from this CNNs ability to operate over the length of the sequence. In addition, because of the sparsity of the connections, you end up being able to make much smaller networks that retain a great deal of power.\n",
    "\n",
    "<img src=\"images/cnn.png\" alt=\"cnn\" title=\"CNN vs FF-net comparison\" />\n",
    "\n",
    "<p style=\"text-align: center;\">Top: A CNN with filter size of 3, Bottom: A Feedforward neural net. (From Goodfellow et al. 2016)</p>\n",
    "\n",
    "This illustrates a single size 3 filter, your network might have many additional filters that would all pass over the length of the data, each one producing an output channel to be passed to the next part of the network.  \n",
    "\n",
    "<img src=\"images/1dcnn.png\" alt=\"1dcnn\" title=\"Pytorch 1DCNN overview\" />\n",
    "\n",
    "*We'll get into 2D CNNs later (as an additional topic in COLX 585) but here is a quick taste for how you could use a 1D CNN in the context of text-based NLP tasks.*\n",
    "\n",
    "\n",
    "#### Pytorch 1D CNNs and max-pooling\n",
    "\n",
    "Here's a quick example of how these networks function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.2694, 0.2258, 0.3027, 0.2242, 0.4921, 0.1119, 0.2653, 0.3390,\n",
      "          0.0732, 0.0784],\n",
      "         [0.4174, 0.8640, 0.8319, 0.8304, 0.7679, 0.7038, 0.6223, 0.8163,\n",
      "          1.0691, 0.5059],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0248, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0957],\n",
      "         [0.1202, 0.0854, 0.2238, 0.0000, 0.1766, 0.0630, 0.0000, 0.1541,\n",
      "          0.0000, 0.0000]],\n",
      "\n",
      "        [[0.1818, 0.1428, 0.0944, 0.2333, 0.3378, 0.2724, 0.0255, 0.0448,\n",
      "          0.3605, 0.5355],\n",
      "         [0.4958, 1.0880, 0.7523, 0.4226, 0.7082, 0.8051, 1.0217, 0.5980,\n",
      "          0.6185, 0.8972],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0901,\n",
      "          0.0000, 0.0000],\n",
      "         [0.1728, 0.0000, 0.0000, 0.0410, 0.0566, 0.0718, 0.0000, 0.0000,\n",
      "          0.0000, 0.1017]]], grad_fn=<ReluBackward0>)\n",
      "Take the highest value in each window using max pool\n",
      "tensor([[[0.2694, 0.4921, 0.3390, 0.0784],\n",
      "         [0.8640, 0.8319, 0.8163, 1.0691],\n",
      "         [0.0000, 0.0248, 0.0000, 0.0957],\n",
      "         [0.1202, 0.2238, 0.1541, 0.0000]],\n",
      "\n",
      "        [[0.1818, 0.3378, 0.2724, 0.5355],\n",
      "         [1.0880, 0.7523, 1.0217, 0.8972],\n",
      "         [0.0000, 0.0000, 0.0901, 0.0000],\n",
      "         [0.1728, 0.0566, 0.0718, 0.1017]]], grad_fn=<SqueezeBackward1>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "x = torch.rand((2,5,10))   # batch size 2 with length 10 and 5 dim embedding.\n",
    "\n",
    "in_dim = 5\n",
    "filters = 4\n",
    "\n",
    "cnn1d = nn.Conv1d(in_dim,filters,kernel_size=3,padding=1) \n",
    "max_pool = nn.MaxPool1d(kernel_size=3, padding=1)\n",
    "activation = nn.ReLU()\n",
    "x = cnn1d(x)\n",
    "x = activation(x)\n",
    "print(x)\n",
    "print(\"Take the highest value in each window using max pool\")\n",
    "print(max_pool(x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we've used a 1D CNN and max pooling to \"summarize\" our data, boiling a length 10 series to only 4 items. One could repeat combinations of CNNs, activation functions, with and without pooling layers to build up a network in the same way that we have worked with FF nets.\n",
    "\n",
    "But first one quick check about 1D CNNs\n",
    "\n",
    "#### 1D CNN parameters\n",
    "rubric={reasoning:1}  \n",
    "  \n",
    "Including bias weights how many trainable parameters are in our example CNN (think about how many weights would be needed to operate over your kernel size and each segment of data)?  Explain your reasoning in your answer, and consider checking it by printing out those weights/biases.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your Answer Here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Size through CNNs\n",
    "rubric={accuracy:1, reasoning:1}\n",
    "\n",
    "CNNs can be a little tricky because as the data passes through them the dimensions might change based on number of filters, padding, and two other things we won't talk about now: stride and dilation. MaxPooling also quickly can decrease the size of the data. This is useful especially as a way to \"feature extract\" or \"dimensionality reduction\" but it's important to make sure we get the right output dimensions. \n",
    "\n",
    "For an tensor with batch size of 10, embedding dimension of 25, and length 30 that is fed to a 1D CNN + Maxpool combination, what parameters of the 1D CNN (kernelsize, number of filters, padding) and MaxPool (kernel_size, padding) would we need to get a output that has an output embedding dimension of 5 and a length of 10?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your Answer Here*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test your answer by making a random tensor of the appropriate size, \n",
    "## passing it through your proposed CNN+Maxpool and printing out the final size.\n",
    "\n",
    "## Your Code Here ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1D CNN Model for Sentiment Analysis\n",
    "rubric={accuracy:3, quality:1}\n",
    "\n",
    "Based on our tutorial code that we've used for RNNs, modify our network to use 1D cnns instead of FF networks. We'll search over the architecture in the next question, but for now just make it as a basic 2 layer CNN network with 5 filters kernel size of 3 and using ReLU activation (no max Pooling). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext\n",
    "from torchtext.data import Field, LabelField\n",
    "from torchtext.data import TabularDataset\n",
    "\n",
    "# define the white space tokenizer to get tokens\n",
    "def tokenize_en(tweet):\n",
    "    \"\"\"\n",
    "    Tokenizes English tweet from a string into a list of strings (tokens)\n",
    "    \"\"\"\n",
    "    return tweet.strip().split()\n",
    "\n",
    "# define the TorchText's fields\n",
    "TEXT = Field(sequential=True, tokenize=tokenize_en, lower=True)\n",
    "LABEL = Field(sequential=False, unk_token = None)\n",
    "\n",
    "\n",
    "train, val, test = TabularDataset.splits(\n",
    "    path=\"./data/sentiment-twitter-2016-task4/\", # the root directory where the data lies\n",
    "    train='train.tsv', validation=\"dev.tsv\", test=\"test.tsv\", # file names\n",
    "    format='tsv',\n",
    "    skip_header=False, # if your tsv file has a header, make sure to pass this to ensure it doesn't get proceesed as data!\n",
    "    fields=[('tweet', TEXT), ('label', LABEL)])\n",
    "\n",
    "TEXT.build_vocab(train, min_freq=3) # builds vocabulary based on all the words that occur at least twice in the training set\n",
    "LABEL.build_vocab(train)\n",
    "\n",
    "train_iter, val_iter, test_iter = BucketIterator.splits(\n",
    " (train, val, test), # we pass in the datasets we want the iterator to draw data from\n",
    " batch_sizes=(4,64,64),\n",
    " sort_key=lambda x: len(x.tweet), \n",
    " sort=True,\n",
    "# A key to use for sorting examples in order to batch together examples with similar lengths and minimize padding. \n",
    " sort_within_batch=True\n",
    ")\n",
    "\n",
    "VOCAB_SIZE = len(TEXT.vocab.stoi)\n",
    "\n",
    "WORD_VEC_SIZE=300\n",
    "# Note, the parameters to Embedding class below are:\n",
    "# num_embeddings (int): size of the dictionary of embeddings\n",
    "# embedding_dim (int): the size of each embedding vector\n",
    "# For more details on Embedding class, see: https://github.com/pytorch/pytorch/blob/master/torch/nn/modules/sparse.py\n",
    "\n",
    "class ConvNet(nn.Module):\n",
    "  \n",
    "  def __init__(self, inputs_size, output_size, VOCAB_SIZE,  WORD_VEC_SIZE):  #feel free to add additional parameters\n",
    "    self.embedding = nn.Embedding(VOCAB_SIZE, WORD_VEC_SIZE, sparse=True)\n",
    "    #your code here\n",
    "  \n",
    "  def forward(self, x):\n",
    "    #your code here\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1D CNN Performance\n",
    "rubric={accuracy:2, quality:1}\n",
    "\n",
    "Based on our initial network we'd like to compare how depth matters vs number of filters in a given layer. For this section (consider spliting up the trials similar to Lab 1, with group members running a set on different random start states), perform a random search (total trials: 20) over the parameters space (number of layers, kernel size, number of filters, activation, etc.) and report your results (ranges searched, best performance) on the sentiment task. Hint: Feel free to use Skorch + sklearn + scipy.stats (for distributions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1D CNN summary\n",
    "rubric={reasoning:1}\n",
    "\n",
    "Based on the performance on the task how does the CNN compare to the Feed Forward network from Lab 2? What factors seemed to be most important to the performance of the model (number of filters, depth, size of filters...? ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* You answer here *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Building a Recurrent Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, we provide a corpus from the [CL-Aff shared task](https://sites.google.com/view/affcon2019/cl-aff-shared-task?authuser=0). HappyDB is a dataset of about 100,000 `happy moments` crowd-sourced via Amazon’s Mechanical Turk where each worker was asked to describe in a complete sentence `what made them happy in the past 24 hours`. Each user was asked to describe three such moments. \n",
    "In this exercise, we focus on sociality classification. We only use labelled dataset which include 10,562 labelled samples. \n",
    "\n",
    "We have already preprocessed (tokenization, removing URLs, mentions, hashtags and so on) the tweets and placed it under ``data/happy_db`` folder in three files as ``train.tsv``, ``dev.tsv`` and ``test.tsv``."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Write code to  define a `whitespace_tokenizer` whose input is a tweet text.\n",
    "\n",
    "rubric={accuracy:1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def whitespace_tokenize(text):\n",
    "    # your code goes here "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Wrote code to define `TorchText's Fields` handle how data should be processed.  \n",
    "Hint: You need 2 Fields `TEXT` and `LABEL` for tweet text and label respectively. \n",
    "\n",
    "* The tokenizer is the whitespace_tokenizer in 1.1.\n",
    "* Use the truecase of words. \n",
    "\n",
    "rubric={accuracy:2} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 Write code to use `TabularDataset class`  and `Fields` to process the tsv files (train, dev, and test). \n",
    "\n",
    "Hint 1: `Fields` will call tokenizer, and convert tokens to numerical index. \n",
    "\n",
    "Hint 2: Use `TabularDataset.splits(...)` to load train, dev, and test sets.\n",
    "\n",
    "rubric={accuracy:2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4  Write code to build your vocabulary to map words and labels to integers. The maximum size of vocabulary is 5,000 (not count `<pad>` and `<unk>` in).\n",
    "Hint: Use `build_vocab` to build vocabularies for `TEXT` field and `LABEL` field respectively. \n",
    "\n",
    "rubric={accuracy:2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.5 Write code to print the sizes of your two vocabularies (i.e., `TEXT` and `LABEL`) individually. \n",
    "\n",
    "rubric={accuracy:2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.6 Write code to construct the Iterators to get the train, dev, and test splits. Use `BucketIterator` to initialize the Iterators for the train, dev, and test data.\n",
    "* apply same batch size of 32 on train, dev, and test set.\n",
    "* Samples are sorted by length.\n",
    "* Sort samples within each batch\n",
    "\n",
    "Hint: Use `BucketIterator.splits(...)`\n",
    "\n",
    "rubric={accuracy:2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.7.1 Write code to create a class called `LSTMmodel` to define a classifier of the task. In this model, you should have:\n",
    "\n",
    "1. An embedding layer with input dimention equal to the size of your `TEXT` vocabulary, and that represent each token in a 300-dimentional vector.  The parameters of this embedding layer should be randomly initialized with numbers sampled from a normal distribution (mean 0 and variance 0.05).\n",
    "\n",
    "2. Two uni-directional `LSTM` layers. Each layer has 500 hidden unites.\n",
    "\n",
    "3. Pass the last hidden of last layer into a `Tanh` activation function, then feed the output of `Tanh` to a linear layer whose dimensionality is equal to the number of classes in the dataset (i.e., 2 in our case).\n",
    "\n",
    "4. Then, a `LogSoftmax` layer on top of the outcome of `linear layer`.\n",
    "\n",
    "5. Return the output of `LogSoftmax` layer.\n",
    "\n",
    "Hint: `Tanh` might not be the ideal function to use, but we want you to explore it. (Usually `ReLU` works well).\n",
    "\n",
    "rubric={accuracy:8, quality:4}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.7.2 Write code to Instantiate the model class with aforementioned hyper-parameters and print out the model architecture.\n",
    "rubric={accuracy:3}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint: Your output should generally look like the following (with correct names of the model, functions, etc.):\n",
    "```\n",
    "modelName (\n",
    "  (embedding): Embedding(xxx, xxx)\n",
    "  (lstm_rnn): MODEL(xxx, xxx, num_layers=xxx)\n",
    "  (activation_fn): Tanh()\n",
    "  (linear_layer): Linear(in_features=xxx, out_features=xxx, bias=True)\n",
    "  (softmax_layer): somethingRelatedToSoftmax()\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create an optimizer for training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.1\n",
    "criterion = nn.NLLLoss()\n",
    "# create an instance of SGD with required hyperparameters\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.8.1 How many learnable (or updatable) parameters are present in the model defined in 1.7. Compute the result by writing code.\n",
    "rubric={accuracy:1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.8.2 OPTIONAL QUESTION: How many megabyte memory will this model use? Please show your work. \n",
    "\n",
    "Hint1: Each parameter is a `torch.float32` tensor which is 32-bit floating point. \n",
    "\n",
    "Hint2: All the parameters of this model are learnable (or updatable) parameters.\n",
    "\n",
    "rubric={spark:2}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write your answer here.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To facilitate your work for the next questions (1.9 and 1.10), we provide two functions for training and evaluation.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def train(loader):\n",
    "    total_loss = 0.0\n",
    "    # iterate throught the data loader\n",
    "    num_sample = 0\n",
    "    for batch in loader:\n",
    "        # load the current batch\n",
    "        batch_input = batch.tweet\n",
    "        batch_output = batch.label\n",
    "        \n",
    "        batch_input = batch_input.to(device)\n",
    "        batch_output = batch_output.to(device)\n",
    "        # forward propagation\n",
    "        # pass the data through the model\n",
    "        model_outputs = model(batch_input)\n",
    "        # compute the loss\n",
    "        cur_loss = criterion(model_outputs, batch_output)\n",
    "        total_loss += cur_loss.item()\n",
    "\n",
    "        # backward propagation (compute the gradients and update the model)\n",
    "        # clear the buffer\n",
    "        optimizer.zero_grad()\n",
    "        # compute the gradients\n",
    "        cur_loss.backward()\n",
    "        # update the weights\n",
    "        optimizer.step()\n",
    "\n",
    "        num_sample += batch_output.shape[0]\n",
    "    return total_loss/num_sample\n",
    "\n",
    "# evaluation logic based on classification accuracy\n",
    "def evaluate(loader):\n",
    "    all_pred=[]\n",
    "    all_label = []\n",
    "    with torch.no_grad(): # impacts the autograd engine and deactivate it. reduces memory usage and speeds up computation\n",
    "        for batch in loader:\n",
    "             # load the current batch\n",
    "            batch_input = batch.tweet\n",
    "            batch_output = batch.label\n",
    "\n",
    "            batch_input = batch_input.to(device)\n",
    "            # forward propagation\n",
    "            # pass the data through the model\n",
    "            model_outputs = model(batch_input)\n",
    "            # identify the predicted class for each example in the batch\n",
    "            probabilities, predicted = torch.max(model_outputs.cpu().data, 1)\n",
    "            # put all the true labels and predictions to two lists\n",
    "            all_pred.extend(predicted)\n",
    "            all_label.extend(batch_output)\n",
    "            \n",
    "    accuracy = accuracy_score(all_label, all_pred)\n",
    "    f1score = f1_score(all_label, all_pred, average='macro') \n",
    "    return accuracy,f1score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.9 Please read [Pytorch documentation](https://pytorch.org/tutorials/beginner/saving_loading_models.html) and write code to load the trained model checkpoint from directory: `./ckpt/model_24.pt`.\n",
    "\n",
    "Note this ckpt is a dictionary which includes three keys: epoch, model_state_dict, and optimizer_state_dict.\n",
    "The model parameters are the values of \"model_state_dictm\".\n",
    "\n",
    "\n",
    "\n",
    "rubric={accuracy:2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.10 Write code to evaluate the trained model on test set and report the test accuracy and F-score. \n",
    "Report the performance of the trained model on the test set is XX.XXX% on accuracy (fill in your accuracy).\n",
    "\n",
    "Report the performance of the trained model on the test set is XX.XXX on F1_score (fill in your F1_score).\n",
    "\n",
    "rubric={accuracy:2,quality:1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Short Answer Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Non-Linearity Review\n",
    "rubric={reasoning:2}\n",
    "\n",
    "There are a number of different non-linearities that can be used in our neural networks, for instance: **Sigmoid, tanh, and ReLU**. Different variations and tweaks to these non-linearities get introduced fairly frequently, and it pays to have a sense of why you might pick one non-linearity for your network instead of another. **Explain how these three (Sigmoid, tanh, and ReLU) are different and what might be some pros and cons of using them in a neural network**. (There are some great blog/quora posts talking about this topic, but **if you are going to be summarizing from a post or other online resource please include a link to the resource**).\n",
    "\n",
    "\n",
    "Hint: Your answer should be a maximum of 2-3 paragraphs. Short answers are just fine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 ReLU Variations\n",
    "rubric={reasoning:1}\n",
    "\n",
    "PyTorch supports several other non-linearities, find two variations on ReLU that PyTorch implements (see https://pytorch.org/docs/stable/nn.html) and explain how they are different from standard ReLU. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write your answer here.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Wait why are we using Logs?\n",
    "rubric={reasoning:1}\n",
    "\n",
    "What is the purpose of taking logs of probabilities, as in the case of NLLLoss and LogSoftmax?\n",
    "\n",
    "Hint: A short answer is just fine.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Softmax\n",
    "rubric={reasoning:2}\n",
    "\n",
    "For a multiclass problem with a Softmax layer why might we care about values corresponding to more than one of these classes?\n",
    "\n",
    "Hint: Sometimes knowing what the top n most likely classes is of interest. Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 RNNs\n",
    "rubric={reasoning:1}\n",
    "\n",
    "RNNs are quite powerful for dealing with certain types of data (such as sequences), however, they have some drawbacks. What are some of these drawbacks? (List two drawbacks)\n",
    "\n",
    "Hint: Look at RNN slides, including the last few slides.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 LSTMs vs RNNs\n",
    "\n",
    "rubric={reasoning:2}\n",
    "\n",
    "LSTMs help alleviate one of the issues with RNNs (see 2.5). What do they alleviate? Do you see any problems that the model might still have (just reason generally about the model based on your understanding).\n",
    "\n",
    "Hint: Short answer is good.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write your answer here."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
