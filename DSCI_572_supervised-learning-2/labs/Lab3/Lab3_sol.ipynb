{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework 3 - Supervised Learning II - MDS Computational Linguistics\n",
    "\n",
    "### Assignment Topics\n",
    "- Recurrent Neural Networks\n",
    "- Long Short-Term \n",
    "- Saving and Loading NN models using Pytorch\n",
    "- Very-short answer questions\n",
    "\n",
    "### Software Requirements\n",
    "- Python (>=3.6)\n",
    "- PyTorch (>=1.2.0) \n",
    "- Jupyter (latest)\n",
    "\n",
    "### Submission Info.\n",
    "- Due Date: February 1, 2020, 18:00:00 (Vancouver time)\n",
    "\n",
    "## Getting Started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all the necessary imports\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch\n",
    "import torchtext\n",
    "from torchtext.data import Field, LabelField\n",
    "from torchtext.data import TabularDataset\n",
    "from torchtext.data import Iterator, BucketIterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the seed\n",
    "manual_seed = 123\n",
    "torch.manual_seed(manual_seed)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_gpu = torch.cuda.device_count()\n",
    "if n_gpu > 0:\n",
    "    torch.cuda.manual_seed(manual_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tidy Submission\n",
    "\n",
    "rubric={mechanics:1}\n",
    "\n",
    "To get the marks for tidy submission:\n",
    "- Submit the assignment by filling in this jupyter notebook with your answers embedded\n",
    "- Be sure to follow the [general lab instructions](https://ubc-mds.github.io/resources_pages/general_lab_instructions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Building a Recurrent Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, we provide a corpus from the [CL-Aff shared task](https://sites.google.com/view/affcon2019/cl-aff-shared-task?authuser=0). HappyDB is a dataset of about 100,000 `happy moments` crowd-sourced via Amazonâ€™s Mechanical Turk where each worker was asked to describe in a complete sentence `what made them happy in the past 24 hours`. Each user was asked to describe three such moments. \n",
    "In this exercise, we focus on sociality classification. We only use labelled dataset which include 10,562 labelled samples. \n",
    "\n",
    "We have already preprocessed (tokenization, removing URLs, mentions, hashtags and so on) the tweets and placed it under ``data/happy_db`` folder in three files as ``train.tsv``, ``dev.tsv`` and ``test.tsv``."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Write code to  define a `whitespace_tokenizer` whose input is a tweet text.\n",
    "\n",
    "rubric={accuracy:1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def whitespace_tokenize(text):\n",
    "    # your code goes here \n",
    "    \"\"\"\n",
    "    Tokenizes English text from a string into a list of strings (tokens)\n",
    "    \"\"\"\n",
    "    return text.strip().split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Wrote code to define `TorchText's Fields` handle how data should be processed.  \n",
    "Hint: You need 2 Fields `TEXT` and `LABEL` for tweet text and label respectively. \n",
    "\n",
    "* The tokenizer is the whitespace_tokenizer in 1.1.\n",
    "* Use the truecase of words. \n",
    "\n",
    "rubric={accuracy:2} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here \n",
    "TEXT = Field(sequential=True, tokenize=whitespace_tokenize, lower=False)\n",
    "LABEL = Field(sequential=False, unk_token = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 Write code to use `TabularDataset class`  and `Fields` to process the tsv files (train, dev, and test). \n",
    "\n",
    "Hint 1: `Fields` will call tokenizer, and convert tokens to numerical index. \n",
    "\n",
    "Hint 2: Use `TabularDataset.splits(...)` to load train, dev, and test sets.\n",
    "\n",
    "rubric={accuracy:2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here \n",
    "train, val, test = TabularDataset.splits(\n",
    "               path=\"./data/happy_db/\", # the root directory where the data lies\n",
    "               train='train.tsv', validation=\"dev.tsv\", test=\"test.tsv\", # file names\n",
    "               format='tsv',\n",
    "               skip_header=True, # if your tsv file has a header, make sure to pass this to ensure it doesn't get proceesed as data!\n",
    "               fields=[('tweet', TEXT), ('label', LABEL)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4  Write code to build your vocabulary to map words and labels to integers. The maximum size of vocabulary is 5,000 (not count `<pad>` and `<unk>` in).\n",
    "Hint: Use `build_vocab` to build vocabularies for `TEXT` field and `LABEL` field respectively. \n",
    "\n",
    "rubric={accuracy:2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here \n",
    "TEXT.build_vocab(train, max_size=5000)\n",
    "LABEL.build_vocab(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.5 Write code to print the sizes of your two vocabularies (i.e., `TEXT` and `LABEL`) individually. \n",
    "\n",
    "rubric={accuracy:2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here \n",
    "print(\"Vocabulary size of TEXT:\",len(TEXT.vocab.stoi))\n",
    "print(\"Vocabulary size of LABEL:\",len(LABEL.vocab.stoi))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.6 Write code to construct the Iterators to get the train, dev, and test splits. Use `BucketIterator` to initialize the Iterators for the train, dev, and test data.\n",
    "* apply same batch size of 32 on train, dev, and test set.\n",
    "* Samples are sorted by length.\n",
    "* Sort samples within each batch\n",
    "\n",
    "Hint: Use `BucketIterator.splits(...)`\n",
    "\n",
    "rubric={accuracy:2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here \n",
    "\n",
    "train_iter, val_iter, test_iter = BucketIterator.splits(\n",
    " (train, val, test), # we pass in the datasets we want the iterator to draw data from\n",
    " batch_sizes=(32,32,32),\n",
    " sort_key=lambda x: len(x.tweet), \n",
    " sort=True,\n",
    "# A key to use for sorting examples in order to batch together examples with similar lengths and minimize padding. \n",
    " sort_within_batch=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.7.1 Write code to create a class called `LSTMmodel` to define a classifier of the task. In this model, you should have:\n",
    "\n",
    "1. An embedding layer with input dimention equal to the size of your `TEXT` vocabulary, and that represent each token in a 300-dimentional vector.  The parameters of this embedding layer should be randomly initialized with numbers sampled from a normal distribution (mean 0 and variance 0.05).\n",
    "\n",
    "2. Two uni-directional `LSTM` layers. Each layer has 500 hidden unites.\n",
    "\n",
    "3. Pass the last hidden of last layer into a `Tanh` activation function, then feed the output of `Tanh` to a linear layer whose dimensionality is equal to the number of classes in the dataset (i.e., 2 in our case).\n",
    "\n",
    "4. Then, a `LogSoftmax` layer on top of the outcome of `linear layer`.\n",
    "\n",
    "5. Return the output of `LogSoftmax` layer.\n",
    "\n",
    "Hint: `Tanh` might not be the ideal function to use, but we want you to explore it. (Usually `ReLU` works well).\n",
    "\n",
    "rubric={accuracy:8, quality:4}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here \n",
    "\n",
    "class LSTMmodel(nn.Module):\n",
    "  \n",
    "  def __init__(self, embedding_size, vocab_size, output_size, hidden_size, num_layers):\n",
    "    # In the constructor we define the layers for our model (same as our previous RNN)\n",
    "    super(LSTMmodel, self).__init__()\n",
    "    # word embedding lookup table\n",
    "    self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_size)\n",
    "    self.embedding.weight.data.normal_(0.0,0.05) # mean=0.0, mu=0.05\n",
    "    # core LSTM module\n",
    "    self.lstm_rnn = nn.LSTM(input_size=embedding_size, hidden_size=hidden_size, num_layers=num_layers) # input_size, hidden_size, num_layers\n",
    "    self.activation_fn = nn.Tanh()\n",
    "    self.linear_layer = nn.Linear(hidden_size, output_size) \n",
    "    self.softmax_layer = nn.LogSoftmax(dim=0)\n",
    "  \n",
    "  def forward(self, x):\n",
    "    # In the forward function we define the forward propagation logic\n",
    "    out = self.embedding(x)\n",
    "    out, (h_state, c_state) = self.lstm_rnn(out) # h_0 initialized to zeros by default\n",
    "    # classify based on the hidden representation at the last token\n",
    "    out = out[-1] # unsqueeze converts 1D input (D dimension) into 2D input (1xD) \n",
    "    out = self.linear_layer(out)\n",
    "    out = self.softmax_layer(out) # accepts 2D or more dimensional inputs\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.7.2 Write code to Instantiate the model class with aforementioned hyper-parameters and print out the model architecture.\n",
    "rubric={accuracy:3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here \n",
    "\n",
    "EMBEDDING_SIZE = 300 \n",
    "VOCAB_SIZE = 5002\n",
    "NUM_CLASSES = 2\n",
    "HIDDEN_SIZE = 500\n",
    "NUM_LAYERS = 2\n",
    "model = LSTMmodel(EMBEDDING_SIZE, VOCAB_SIZE, NUM_CLASSES, HIDDEN_SIZE, NUM_LAYERS)\n",
    "model = model.to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create an optimizer for training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.1\n",
    "criterion = nn.NLLLoss()\n",
    "# create an instance of SGD with required hyperparameters\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.8.1 How many learnable (or updatable) parameters are present in the model defined in 1.7. Compute the result by writing code.\n",
    "rubric={accuracy:1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here \n",
    "count = 0\n",
    "for p in model.parameters():\n",
    "    count += p.numel()\n",
    "print(\"the number of parameters:\",count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.8.2 OPTIONAL QUESTION: How many megabyte memory will this model use? Please show your work. \n",
    "\n",
    "Hint1: Each parameter is a `torch.float32` tensor which is 32-bit floating point. \n",
    "\n",
    "Hint2: All the parameters of this model are learnable (or updatable) parameters.\n",
    "\n",
    "rubric={spark:2}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$5109602 * 32 bits = 163507264 bits$\n",
    "\n",
    "$1bit = 1.25e-7 Megabyte$\n",
    "\n",
    "$163507264 bits = 20.438408 Megabyte$ \n",
    "\n",
    "Actully, you can find the size of the checkpoint (i.e., model_23.pt) is 20.4 MB."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To facilitate your work, we provide two function for training and evaluation.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def train(loader):\n",
    "    total_loss = 0.0\n",
    "    # iterate throught the data loader\n",
    "    num_sample = 0\n",
    "    for batch in loader:\n",
    "        # load the current batch\n",
    "        batch_input = batch.tweet\n",
    "        batch_output = batch.label\n",
    "        \n",
    "        batch_input = batch_input.to(device)\n",
    "        batch_output = batch_output.to(device)\n",
    "        # forward propagation\n",
    "        # pass the data through the model\n",
    "        model_outputs = model(batch_input)\n",
    "        # compute the loss\n",
    "        cur_loss = criterion(model_outputs, batch_output)\n",
    "        total_loss += cur_loss.item()\n",
    "\n",
    "        # backward propagation (compute the gradients and update the model)\n",
    "        # clear the buffer\n",
    "        optimizer.zero_grad()\n",
    "        # compute the gradients\n",
    "        cur_loss.backward()\n",
    "        # update the weights\n",
    "        optimizer.step()\n",
    "\n",
    "        num_sample += batch_output.shape[0]\n",
    "    return total_loss/num_sample\n",
    "\n",
    "# evaluation logic based on classification accuracy\n",
    "def evaluate(loader):\n",
    "    all_pred=[]\n",
    "    all_label = []\n",
    "    with torch.no_grad(): # impacts the autograd engine and deactivate it. reduces memory usage and speeds up computation\n",
    "        for batch in loader:\n",
    "             # load the current batch\n",
    "            batch_input = batch.tweet\n",
    "            batch_output = batch.label\n",
    "\n",
    "            batch_input = batch_input.to(device)\n",
    "            # forward propagation\n",
    "            # pass the data through the model\n",
    "            model_outputs = model(batch_input)\n",
    "            # identify the predicted class for each example in the batch\n",
    "            probabilities, predicted = torch.max(model_outputs.cpu().data, 1)\n",
    "            # put all the true labels and predictions to two lists\n",
    "            all_pred.extend(predicted)\n",
    "            all_label.extend(batch_output)\n",
    "            \n",
    "    accuracy = accuracy_score(all_label, all_pred)\n",
    "    f1score = f1_score(all_label, all_pred, average='macro') \n",
    "    return accuracy,f1score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Here is the code I used to train the model on GPU. The best model on validation set was trained with 23 epochs.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an instance of SGD with required hyperparameters\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# start the training\n",
    "for epoch in range(30):\n",
    "    # train the model for one pass over the data\n",
    "    train_loss = train(train_iter)  \n",
    "    # compute the training accuracy\n",
    "    train_acc,f1t = evaluate(train_iter)\n",
    "    # compute the validation accuracy\n",
    "    val_acc,f1v = evaluate(val_iter)\n",
    "    \n",
    "    # print the loss for every epoch\n",
    "    print('Epoch [{}/{}], Loss: {:.4f}, Training Accuracy: {:.4f}, Validation Accuracy: {:.4f}'.format(epoch+1, 30, train_loss, train_acc, val_acc))\n",
    "    \n",
    "    # save model, optimizer, and number of epoch to a dictionary\n",
    "    model_save = {\n",
    "            'epoch': epoch,  # number of epoch\n",
    "            'model_state_dict': model.state_dict(), # model parameters \n",
    "            'optimizer_state_dict': optimizer.state_dict(), # save optimizer \n",
    "            'loss': train_loss # training loss\n",
    "            }\n",
    "    \n",
    "    # use torch.save to store \n",
    "    torch.save(model_save, \"./ckpt/model_{}.pt\".format(epoch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.9 Please read [Pytorch documentation](https://pytorch.org/tutorials/beginner/saving_loading_models.html) and write code to load the trained model checkpoint from directory: `./ckpt/model_24.pt`.\n",
    "\n",
    "Note this ckpt is a dictionary which includes three keys: epoch, model_state_dict, and optimizer_state_dict.\n",
    "The model parameters are the values of \"model_state_dictm\".\n",
    "\n",
    "\n",
    "\n",
    "rubric={accuracy:2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here \n",
    "checkpoint = torch.load(\"./ckpt/model_23.pt\")\n",
    "model.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.10 Write code to evaluate the trained model on test set and report the test accuracy and F-score. \n",
    "Report the performance of the trained model on the test set is 84.186% on accuracy (fill in your accuracy).\n",
    "\n",
    "Report the performance of the trained model on the test set is 84.021 on F1_score (fill in your F1_score).\n",
    "\n",
    "rubric={accuracy:2,quality:1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(evaluate(test_iter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Short Answer Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Non-Linearity Review\n",
    "rubric={reasoning:3}\n",
    "\n",
    "There are a number of different non-linearities that can be used in our neural networks, for instance: **Sigmoid, tanh, and ReLU**. Different variations and tweaks to these non-linearities get introduced fairly frequently, and it pays to have a sense of why you might pick one non-linearity for your network instead of another. **Explain how these three (Sigmoid, tanh, and ReLU) are different and what might be some pros and cons of using them in a neural network**. (There are some great blog/quora posts talking about this topic, but if you are going to be summarizing from a post please include a link to the resource).\n",
    "\n",
    "\n",
    "Hint: Your answer should be a maximum of 2-3 paragraphs. Short answers are just fine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write your answer here.\n",
    "Examples:\n",
    "- Sigmoid and Tanh can both run into issues where the gradients get very small when input gets close to (+inf or -inf), this is the vanishing gradient problem.\n",
    "\n",
    "- Tanh is between (-1,1) which makes training a little bit nice than sigmoid.\n",
    "\n",
    "- ReLU is super fast to compute and doesn't have the vanishing gradient issue.\n",
    "\n",
    "There are of course other things you could have found in your research, mainly looking that you are able to look through some resources and weigh the pros and cons of the different non-linearities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 ReLU Variations\n",
    "rubric={reasoning:1}\n",
    "\n",
    "PyTorch supports several other non-linearities, find two variations on ReLU that PyTorch implements (see https://pytorch.org/docs/stable/nn.html) and explain how they are different from standard ReLU.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write your answer here.\n",
    "\n",
    "Picking one as an example: \n",
    "\n",
    "- Leaky ReLU is like ReLU but allows for some small (tiny!) value when the input is negative. This might be important if you still want to differentiate between negative input values and could potentially be slightly more powerful than ReLU at the expense of being slightly more expensive to compute. \n",
    "\n",
    "There are a lot of other choices, but just consider what might be some benefits for these other \"advanced\" activation functions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Wait why are we using Logs?\n",
    "rubric={reasoning:1}\n",
    "\n",
    "What is the purpose of taking logs of probabilities, as in the case of NLLLoss and LogSoftmax?\n",
    "\n",
    "Hint: A short answer is just fine.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write your answer here.\n",
    "\n",
    "You can run into underflow issues with probabilities. likewise logs of probabilities allow you to calculate things in terms of sums (cheap) as opposed to multiplication, potentially allowing for faster computation of values. If you just care about the biggest probability, the biggest log probability is still going to be larger than the rest, so we can do everything with logs that we could with non-logs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Softmax\n",
    "rubric={reasoning:2}\n",
    "\n",
    "For a multiclass problem with a Softmax layer, give an example of a hypothetical Softmax output with 3 classes (hint, think back to Lab1 for what that output might look like). Generally, which of the three classes in your example should your classifier pick? Why might we care about values corresponding to more than one of these classes?\n",
    "\n",
    "Hint: Sometimes knowing what the top $n$ most likely classes is of interest. Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write your answer here.\n",
    "\n",
    "Say for instance your output from Softmax looks like: $[.3,.6,.1]$ first it should be a probablity distribution (the elements should sum to 1), second you should generally pick the second class here (.6) since it is the highest probability given. There are a couple of reasons why you might care about the other numbers though, one reason is if you want to say handle ambiguous situations slightly differently, say you have the top two classes close together, but both fairly low, in some situations you might want to handle things without just predicting the top class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 RNNs\n",
    "rubric={reasoning:1}\n",
    "\n",
    "RNNs are quite powerful for dealing with certain types of data (such as sequences), however, they have some drawbacks. What are some of these drawbacks? (List two drawbacks)\n",
    "\n",
    "Hint: Look at RNN slides, including the last few slides.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write your answer here.\n",
    "\n",
    "Clear issues include dealing with long sequences and the runtime of the algorithm (it can't be parallelized)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 LSTMs vs RNNs\n",
    "\n",
    "rubric={reasoning:2}\n",
    "\n",
    "LSTMs help alleviate one of the issues with RNNs (see 2.5). What do they alleviate? Do you see any problems that the model might still have (just reason generally about the model based on your understanding).\n",
    "\n",
    "Hint: Short answer is good.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write your answer here.\n",
    "\n",
    "LSTMs are better at sequence length issues, [although attention based methods which we will get to later in the semester turn out to be much better. They will still run into issues with very very long sequence lengths, although now they are able to learn to pay attention to certain things over the course of the sequence. They still have the same weakness in terms of speed, as they can't be parallelized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
