{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feedforward Neural Networks - Supervised Learning II - MDS Computational Linguistics\n",
    "\n",
    "### Goal of this tutorial\n",
    "- Implement single layer and multilayer neural networks for sentiment analysis\n",
    "- Implement word embeddings based on word2vec\n",
    "\n",
    "### General\n",
    "- This notebook was last tested on Python 3.6.9, PyTorch 1.2.0 and sklearn 0.21.3\n",
    "\n",
    "We would like to acknowledge the following materials which helped as a reference in preparing this tutorial:\n",
    "- https://github.com/UBC-NLP/dlnlp2019/blob/master/slides/feedforward_nets.pdf\n",
    "- https://github.com/UBC-NLP/dlnlp2019/blob/master/slides/word2vec.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feedforward neural networks\n",
    "\n",
    "Neural networks are a family of classifiers which can model **non-linear decision boundaries**. \n",
    "\n",
    "Recommended reading for the theory behind neural networks: https://github.com/UBC-NLP/dlnlp2019/blob/master/slides/feedforward_nets.pdf\n",
    "\n",
    "We will first define the practical task that we are going to deal with in this tutorial.\n",
    "\n",
    "In this tutorial, we will focus only on sentiment analysis task. Specifically, we focus on classifying the sentiment of the tweet. We make use of the dataset provided by ``SemEval-2016 Task 4 on Sentiment Analysis on Twitter`` (http://alt.qcri.org/semeval2016/task4/). We focus on the subtask A which is coined as **message polarity classification task**. In this task, given a tweet, we need to predict whether the tweet is of **positive, negative or neutral sentiment**. We have 6,000, 1,999 and 20,632 tweets in train set, validation set and test set respectively. We have already preprocessed (tokenization, removing URLs, mentions, hashtags and so on) the tweets and placed it under ``data/sentiment-twitter-2016-task4`` folder in three files as ``train.tsv``, ``dev.tsv`` and ``test.tsv``."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all the necessary imports\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "\n",
    "# set the seed\n",
    "manual_seed = 123\n",
    "torch.manual_seed(manual_seed)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_gpu = torch.cuda.device_count()\n",
    "if n_gpu > 0:\n",
    "    torch.cuda.manual_seed(manual_seed)\n",
    "\n",
    "# hyperparameters\n",
    "BATCH_SIZE = 5\n",
    "MAX_EPOCHS = 15\n",
    "LEARNING_RATE = 0.5\n",
    "MAX_FEATURES = 5000   # x_j, the number of j\n",
    "NUM_CLASSES = 3\n",
    "\n",
    "# dataset\n",
    "DATA_FOLDER = \"data/sentiment-twitter-2016-task4\"\n",
    "TRAIN_FILE = DATA_FOLDER + \"/train.tsv\"\n",
    "VALID_FILE = DATA_FOLDER + \"/dev.tsv\"\n",
    "TEST_FILE = DATA_FOLDER + \"/test.tsv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extractor\n",
    "The training example is a **tweet**, which contains an ordered list of terms (e.g., words). We need to **vectorize** the tweet, that is, convert the tweet to a fixed-length vector to be fed as input to a ML model. We also call this vector as a **feature vector**. In this tutorial, we will utilize simple features based on **term frequency-inverse document frequency (tf-idf)** to represent a tweet. In our case a *document* is a *tweet* and a *word* is a *term*. There are two functions:\n",
    "- tf: A function for **'term frequency'** (basically counts, i.e., how many times the term occurred in our document) \n",
    "- idf:  A function for **'inverse document frequency'** (the total number of documents divided by the number of documents in which the term occurred).\n",
    "\n",
    "Tf-idf of a tweet $d$ is a $|V|$ dimensional feature vector, where each component corresponds to a term $t$ in the vocabulary V whose size is $|V|$. Tf-idf is formulated as,\n",
    "\n",
    "tf-idf(d,t) $ = (1 + \\log f_{d,t}) * \\log(1 + \\frac{N}{n_t})$\n",
    "\n",
    "where,\n",
    "- $f_{d,t}$ corresponds to the raw frequency of the term $t$ in tweet $d$\n",
    "- $N$ corresponds to the number of tweets in train set\n",
    "- $n_t$ corresponds to the number of tweets in train set that has the term $t$\n",
    "\n",
    "Note: There are variations of tf-idf, but here:\n",
    "- We take the log and **add one** to idf to avoid dividing by zero if the term never occurs in any document.\n",
    "- We also similarly add 1 and take the log of tf.\n",
    "Ultimately, we will let sklearn take care of [tf-idf construction](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html). \n",
    "\n",
    "Essentially, **high weight** in tf–idf feature vector is attained by a **high term frequency (in the given tweet) and a low tweet frequency of the term in the whole collection of tweets**. Hence, the weights tend to filter out common terms.\n",
    "\n",
    "Let us construct the tf-idf feature vectorizer from the tweets in train set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aa', 'aapl', 'abc', 'abigail', 'able', 'about', 'above', 'absence', 'absolute', 'absolutely']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#function for loading file\n",
    "def read_corpus(file):\n",
    "    corpus = [] \n",
    "    for line in open(file):\n",
    "        content, label = line.strip().split(\"\\t\")\n",
    "        corpus.append(content)\n",
    "    return corpus\n",
    "\n",
    "# reads the train corpus\n",
    "train_corpus = read_corpus(TRAIN_FILE) \n",
    "\n",
    "# define the vectorizer\n",
    "# builds a vocabulary that only considering the **top max_features** ordered by **term frequency** across the corpus.\n",
    "vectorizer = TfidfVectorizer(max_features=MAX_FEATURES)\n",
    "\n",
    "# fit the vectorizer on train set\n",
    "vectorizer.fit(train_corpus)\n",
    "\n",
    "# print feature names (term) corresponding to first 10 features\n",
    "print(vectorizer.get_feature_names()[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataLoader\n",
    "\n",
    "Next, let us construct the dataloader for sentiment dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input (sample batch) vector size:  torch.Size([5, 5000]) \n",
      "Golden label (sample batch) vector size:  torch.Size([5])\n",
      "Number of batches:  1200\n"
     ]
    }
   ],
   "source": [
    "# create a new class inheriting torch.utils.data.Dataset\n",
    "class TweetSentimentDataset(Dataset):\n",
    "    \"\"\" sentiment-twitter-2016-task4 dataset.\"\"\"\n",
    "    def __init__(self, file, vectorizer):\n",
    "    # read the corpus\n",
    "        corpus, labels = [], []\n",
    "        for line in open(file):\n",
    "            content, label = line.strip().split(\"\\t\")\n",
    "            corpus.append(content)\n",
    "            labels.append(int(label))\n",
    "\n",
    "        # set the size of the corpus\n",
    "        self.n = len(corpus)\n",
    "\n",
    "        # vectorize all the tweets\n",
    "        features = vectorizer.transform(corpus)\n",
    "\n",
    "        # convert features and labels to torch.tensor\n",
    "        self.features = torch.from_numpy(features.toarray()).float()\n",
    "        self.features.to(device)\n",
    "        self.labels = torch.tensor(labels, device=device, requires_grad=False)\n",
    "\n",
    "    # return input and output of a single example\n",
    "    # Input: Feature vectors, where each vector corresponds to a tweet. \n",
    "    # Output: Labels, where each label is one index for each of our tags in the set {positive, negative, neutral}\n",
    "    def __getitem__(self, index):\n",
    "        return self.features[index], self.labels[index]\n",
    "\n",
    "    # return the total number of examples\n",
    "    def __len__(self):\n",
    "        return self.n\n",
    "\n",
    "# create the dataloader object\n",
    "train_loader = DataLoader(dataset=TweetSentimentDataset(TRAIN_FILE, vectorizer), batch_size=BATCH_SIZE, shuffle=True, num_workers=2) \n",
    "valid_loader = DataLoader(dataset=TweetSentimentDataset(VALID_FILE, vectorizer), batch_size=BATCH_SIZE, shuffle=False, num_workers=1)\n",
    "test_loader = DataLoader(dataset=TweetSentimentDataset(TEST_FILE, vectorizer), batch_size=BATCH_SIZE, shuffle=False, num_workers=1) \n",
    "\n",
    "# iterate over the dataset for one epoch\n",
    "num_batches = 0\n",
    "for i, data in enumerate(train_loader, 0):\n",
    "    input, output = data\n",
    "    if i == 0:\n",
    "        print(\"Input (sample batch) vector size: \",input.size(), \"\\nGolden label (sample batch) vector size: \",output.size())\n",
    "    num_batches += 1\n",
    "print(\"Number of batches: \", num_batches) # prints the number of batches per epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **computational graph** for a single layer neural network for classification can look like this:\n",
    "\n",
    "<img src=\"images/sl2_textclassification_neuralnets_cg.jpg\" alt=\"MLP\" title=\"Single Layer neural network - Computational Graph\" width=\"650\" height=\"450\"/>\n",
    "\n",
    "Let us implement this single layer neural network in PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let me explain NLLLoss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "create a custom model class inheriting torch.nn.Module\n",
    "\"\"\"\n",
    "class SingleLayerNeuralNetworkModel(nn.Module):\n",
    "  \n",
    "  def __init__(self, num_inputs, hidden_layers, num_outputs):\n",
    "    # In the constructor we define the layers for our model\n",
    "    super(SingleLayerNeuralNetworkModel, self).__init__()\n",
    "    self.input_to_hidden = nn.Linear(num_inputs, hidden_layers[0]) # includes W and c \n",
    "    self.sigmoid_layer = nn.Sigmoid()  \n",
    "    self.hidden_to_output = nn.Linear(hidden_layers[0], num_outputs) # includes W and b\n",
    "    self.softmax_layer = nn.LogSoftmax(dim=1)\n",
    "  \n",
    "  def forward(self, x):\n",
    "    # In the forward function we define the forward propagation logic\n",
    "    out = self.input_to_hidden(x) # Wx + c\n",
    "    out = self.sigmoid_layer(out) # h = g(Wx + c)\n",
    "    out = self.hidden_to_output(out) # W^T h + b\n",
    "    out = self.softmax_layer(out) # out = softmax(W^T h + b)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us define the training and evaluation logic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train logic (similar to that of linear regression model)\n",
    "def train(loader):\n",
    "    total_loss = 0.0\n",
    "    # iterate throught the data loader\n",
    "    num_batches = 0\n",
    "    for batch in loader:\n",
    "        # load the current batch\n",
    "        batch_input, batch_output = batch\n",
    "\n",
    "        # forward propagation\n",
    "        # pass the data through the model\n",
    "        model_outputs = model(batch_input)\n",
    "        # compute the loss\n",
    "        cur_loss = criterion(model_outputs, batch_output)\n",
    "        total_loss += cur_loss.item()\n",
    "\n",
    "        # backward propagation (compute the gradients and update the model)\n",
    "        # clear the buffer\n",
    "        optimizer.zero_grad()\n",
    "        # compute the gradients\n",
    "        cur_loss.backward()\n",
    "        # update the weights\n",
    "        optimizer.step()\n",
    "\n",
    "        num_batches += 1\n",
    "    return total_loss/num_batches\n",
    "\n",
    "# evaluation logic based on classification accuracy\n",
    "def evaluate(loader):\n",
    "    accuracy, num_examples = 0.0, 0\n",
    "    with torch.no_grad(): # impacts the autograd engine and deactivate it. reduces memory usage and speeds up computation\n",
    "        for batch in loader:\n",
    "            # load the current batch\n",
    "            batch_input, batch_output = batch\n",
    "            # forward propagation\n",
    "            # pass the data through the model\n",
    "            model_outputs = model(batch_input)\n",
    "            # identify the predicted class for each example in the batch (row-wise)\n",
    "            _, predicted = torch.max(model_outputs.data, 1) # Returns a (values, indices) \n",
    "            # compare with batch_output (gold labels) to compute accuracy\n",
    "            accuracy += (predicted == batch_output).sum().item()\n",
    "            num_examples += batch_output.size(0)\n",
    "    return accuracy/num_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us start the training of our first neural network model: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SingleLayerNeuralNetworkModel(\n",
      "  (input_to_hidden): Linear(in_features=5000, out_features=50, bias=True)\n",
      "  (sigmoid_layer): Sigmoid()\n",
      "  (hidden_to_output): Linear(in_features=50, out_features=3, bias=True)\n",
      "  (softmax_layer): LogSoftmax()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# hyperparameter of neural network\n",
    "hidden_layers = [50]  # set number of hidden features  of the first layer to 50\n",
    "\n",
    "# define the loss function (last node of the graph)\n",
    "model = SingleLayerNeuralNetworkModel(MAX_FEATURES, hidden_layers, NUM_CLASSES)\n",
    "model.to(device)\n",
    "print(model)\n",
    "criterion = nn.NLLLoss() #The negative log likelihood loss\n",
    "# create an instance of SGD with required hyperparameters\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/15], Loss: 1.1222, Training Accuracy: 0.5157,  Validation Accuracy: 0.4217\n",
      "Epoch [2/15], Loss: 1.0186, Training Accuracy: 0.5157,  Validation Accuracy: 0.4217\n",
      "Epoch [3/15], Loss: 0.9626, Training Accuracy: 0.6305,  Validation Accuracy: 0.5033\n",
      "Epoch [4/15], Loss: 0.8864, Training Accuracy: 0.5432,  Validation Accuracy: 0.4302\n",
      "Epoch [5/15], Loss: 0.8240, Training Accuracy: 0.5955,  Validation Accuracy: 0.4577\n",
      "Epoch [6/15], Loss: 0.7746, Training Accuracy: 0.7260,  Validation Accuracy: 0.5248\n",
      "Epoch [7/15], Loss: 0.7328, Training Accuracy: 0.4887,  Validation Accuracy: 0.4212\n",
      "Epoch [8/15], Loss: 0.6813, Training Accuracy: 0.7442,  Validation Accuracy: 0.4952\n",
      "Epoch [9/15], Loss: 0.6452, Training Accuracy: 0.8222,  Validation Accuracy: 0.5063\n",
      "Epoch [10/15], Loss: 0.5984, Training Accuracy: 0.8393,  Validation Accuracy: 0.5043\n",
      "Epoch [11/15], Loss: 0.5608, Training Accuracy: 0.7920,  Validation Accuracy: 0.4777\n",
      "Epoch [12/15], Loss: 0.5247, Training Accuracy: 0.8703,  Validation Accuracy: 0.5083\n",
      "Epoch [13/15], Loss: 0.4880, Training Accuracy: 0.6860,  Validation Accuracy: 0.4137\n",
      "Epoch [14/15], Loss: 0.4527, Training Accuracy: 0.8330,  Validation Accuracy: 0.4762\n",
      "Epoch [15/15], Loss: 0.4312, Training Accuracy: 0.8655,  Validation Accuracy: 0.4857\n"
     ]
    }
   ],
   "source": [
    "# start the training\n",
    "for epoch in range(MAX_EPOCHS):\n",
    "    # train the model for one pass over the data\n",
    "    train_loss = train(train_loader)\n",
    "    # compute the training accuracy\n",
    "    train_acc = evaluate(train_loader)\n",
    "    # compute the validation accuracy \n",
    "    val_acc = evaluate(valid_loader)\n",
    "    # print the loss for every epoch\n",
    "    \n",
    "    print('Epoch [{}/{}], Loss: {:.4f}, Training Accuracy: {:.4f},  Validation Accuracy: {:.4f}'.format(epoch+1, MAX_EPOCHS, train_loss, train_acc, val_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding more hidden layers\n",
    "\n",
    "Single layer neural network uses **one hidden layer**. A deep neural network typically stacks multiple hidden layers leading to good performance on many tasks. And the computational graph for a **2 layer neural network** for classification can look like this:\n",
    "\n",
    "<img src=\"images/sl2_textclassification_neuralnets_multi_cg.jpg\" alt=\"MLP\" title=\"2 Layer neural network - Computational Graph\" />\n",
    "\n",
    "Let us implement this two layer neural network in PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiLayerNeuralNetworkModel(\n",
      "  (model): Sequential(\n",
      "    (0): Linear(in_features=5000, out_features=50, bias=True)\n",
      "    (1): Sigmoid()\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Sigmoid()\n",
      "    (4): Linear(in_features=50, out_features=3, bias=True)\n",
      "    (5): LogSoftmax()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "create a custom model class inheriting torch.nn.Module\n",
    "\"\"\"\n",
    "class MultiLayerNeuralNetworkModel(nn.Module):\n",
    "  \n",
    "  def __init__(self, num_inputs, hidden_layers, num_outputs):\n",
    "    # In the constructor we define the layers for our model\n",
    "    super(MultiLayerNeuralNetworkModel, self).__init__()\n",
    "    \n",
    "    modules = [] # stores all the layers for the neural network\n",
    "    input_dim = num_inputs\n",
    "    # add input layer followed by hidden layers\n",
    "    for hidden_layer in hidden_layers:\n",
    "      # add one layer followed by non-linearity (nn.Sigmoid)\n",
    "      modules.append(nn.Linear(input_dim, hidden_layer))\n",
    "      modules.append(nn.Sigmoid())\n",
    "      input_dim = hidden_layer\n",
    "    # add the classification module (output layer)\n",
    "    modules.append(nn.Linear(input_dim, num_outputs))\n",
    "    modules.append(nn.LogSoftmax(dim=1))\n",
    "    \n",
    "    # create the model from all the modules\n",
    "    self.model = nn.Sequential(*modules) # container of layers, for more details: https://pytorch.org/docs/stable/nn.html#torch.nn.Sequential\n",
    "  \n",
    "  def forward(self, x):\n",
    "    # In the forward function we define the forward propagation logic\n",
    "    out = self.model(x)\n",
    "    return out\n",
    "\n",
    "# hyperparameter of neural network\n",
    "hidden_layers = [50, 50]  # [num. of hidden units in first layer, num. of hidden units in second layer]\n",
    "\n",
    "# define the loss function (last node of the graph)\n",
    "model = MultiLayerNeuralNetworkModel(MAX_FEATURES, hidden_layers, NUM_CLASSES)\n",
    "model.to(device)\n",
    "print(model)\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "# create an instance of SGD with required hyperparameters\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/15], Loss: 1.0212, Training Accuracy: 0.5157, Validation Accuracy: 0.4217\n",
      "Epoch [2/15], Loss: 1.0044, Training Accuracy: 0.3405, Validation Accuracy: 0.3827\n",
      "Epoch [3/15], Loss: 1.0032, Training Accuracy: 0.5157, Validation Accuracy: 0.4217\n",
      "Epoch [4/15], Loss: 1.0043, Training Accuracy: 0.5157, Validation Accuracy: 0.4217\n",
      "Epoch [5/15], Loss: 1.0019, Training Accuracy: 0.5157, Validation Accuracy: 0.4217\n",
      "Epoch [6/15], Loss: 1.0019, Training Accuracy: 0.5157, Validation Accuracy: 0.4217\n",
      "Epoch [7/15], Loss: 1.0002, Training Accuracy: 0.5157, Validation Accuracy: 0.4217\n",
      "Epoch [8/15], Loss: 0.9952, Training Accuracy: 0.5157, Validation Accuracy: 0.4217\n",
      "Epoch [9/15], Loss: 0.9736, Training Accuracy: 0.5157, Validation Accuracy: 0.4217\n",
      "Epoch [10/15], Loss: 0.9281, Training Accuracy: 0.5567, Validation Accuracy: 0.4362\n",
      "Epoch [11/15], Loss: 0.8724, Training Accuracy: 0.6172, Validation Accuracy: 0.4937\n",
      "Epoch [12/15], Loss: 0.8304, Training Accuracy: 0.6478, Validation Accuracy: 0.5063\n",
      "Epoch [13/15], Loss: 0.7931, Training Accuracy: 0.5803, Validation Accuracy: 0.4432\n",
      "Epoch [14/15], Loss: 0.7615, Training Accuracy: 0.6943, Validation Accuracy: 0.5258\n",
      "Epoch [15/15], Loss: 0.7377, Training Accuracy: 0.7193, Validation Accuracy: 0.5103\n"
     ]
    }
   ],
   "source": [
    "# start the training\n",
    "for epoch in range(MAX_EPOCHS):\n",
    "  # train the model for one pass over the data\n",
    "  train_loss = train(train_loader)\n",
    "  # compute the training accuracy\n",
    "  train_acc = evaluate(train_loader)\n",
    "  # compute the validation accuracy \n",
    "  val_acc = evaluate(valid_loader)\n",
    "  # print the loss for every epoch\n",
    "  print('Epoch [{}/{}], Loss: {:.4f}, Training Accuracy: {:.4f}, Validation Accuracy: {:.4f}'.format(epoch+1, MAX_EPOCHS, train_loss, train_acc, val_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us shift gears and take a look at word embeddings. \n",
    "\n",
    "## Word Embeddings\n",
    "\n",
    "In this tutorial, we will focus on **word embeddings (or vectors)** generated by a [word2vec model](https://code.google.com/archive/p/word2vec/). Specifically, we will focus on **continuous bag of words (CBOW)** model. At its core, word2vec relies on the hypothesis that words which occur in **similar contexts** (defined by a select set of words before and after the word in the sentences in which the target word appears) tend to have **similar meaning** and therefore must have similar embeddings. CBOW model learns word embedding (of a certain size called **embedding size** which is a hyperparameter) by setting up an auxiliary task. The auxiliary task is to predict the word (target) given the surrounding words of the target word in a sentence as input. We typically consider **window size** (another hyperparameter) words before and after the target word as surrounding words (or context).\n",
    "\n",
    "The model architecture of CBOW based word2vec model from the [original paper](https://arxiv.org/pdf/1301.3781.pdf) is:\n",
    "\n",
    "<img src=\"images/sl2_textclassification_neuralnets_cbow.png\" alt=\"word2vec\" title=\"CBOW model - Model Architecture\" width=\"250\" height=\"100\" />\n",
    "\n",
    "For instance, consider our corpus has only one sentence **\"I had so much fun in my semester break\"**. If the window size is set to 2, then the train data for CBOW model corresponding to the auxiliary classification task looks like this:\n",
    "\n",
    "| example no. | train input | train label |\n",
    "| ----------------- | ----------- |-------------|\n",
    "| 1  | 'I', 'had', 'much', 'fun'   | 'so' |\n",
    "| 2  | 'had', 'so', 'fun', 'in' | 'much' |\n",
    "| 3  | 'so', 'much', 'in', 'my' | 'fun' |\n",
    "| 4  | 'much', 'fun', 'my', 'semester' | 'in' |\n",
    "| 5  | 'fun', 'in', 'semester', 'break' | 'my' |\n",
    "\n",
    "Our vocabulary has 9 words and CBOW model learns a word embedding for every word in the vocabulary. These word embeddings are typically stored in a giant matrix $W_{input} \\in 9\\times3$, that is **vocabulary size $\\times$ embedding size** (assuming embedding size is 3). Each row in this giant matrix corresponds to a unique word in the vocabulary. The feature vector for the train input is constructed by **averaging the word embeddings** corresponds to the tokens in the train input. For instance, the feature vector for the first training example is given by $\\frac{1}{4} （W_{(I,:)} + W_{(had,:)} + W_{(much,:)} + W_{(fun,:)}）$. \n",
    "\n",
    "This input is fed to a classification layer to identify the target word. Note that the CBOW model does not have bias term in the affine transformation module (``nn.Linear``) corresponding to the classification layer in the neural networks model. The number of categories ($K$) in CBOW model is equivalent to the size of the vocabulary ($9$ in our case).\n",
    "\n",
    "The word embeddings are **intialized** with small numbers sampled randomly **from Gaussian distribution** before training. During training, the word embeddings are jointly learned along with the parameters of the classification layer to predict the target word well given the context. Once the training is done, the word embeddings can capture some semantic features of the word. Note that post training, we do not care for the auxiliary task, whose main goal is to induce meaningful word embeddings.\n",
    "\n",
    "Recommended reading for word embeddings: https://github.com/UBC-NLP/dlnlp2019/blob/master/slides/word2vec.pdf, https://arxiv.org/pdf/1301.3781.pdf and https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html\n",
    "\n",
    "\n",
    "We will use [``torch.nn.Embedding``](https://pytorch.org/docs/stable/nn.html#torch.nn.Embedding) module to simulate the giant matrix, $W_{input}$. Let us look at an example now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# an Embedding module containing 9 tensors of size 3 to represent W_{input}\n",
    "\n",
    "# Note, the parameters to Embedding class below are:\n",
    "# num_embeddings (int): size of the dictionary of embeddings\n",
    "# embedding_dim (int): the size of each embedding vector\n",
    "# For more details on Embedding class, see: https://github.com/pytorch/pytorch/blob/master/torch/nn/modules/sparse.py\n",
    "embedding = nn.Embedding(9, 3, sparse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us construct the word to index mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************  word2id dictionary  ********************\n",
      "{'I': 0, 'had': 1, 'so': 2, 'much': 3, 'fun': 4, 'in': 5, 'my': 6, 'semester': 7, 'break': 8}\n"
     ]
    }
   ],
   "source": [
    "# let us construct the word to index mapping\n",
    "word2id = {}\n",
    "for word in 'I had so much fun in my semester break'.split():\n",
    "    if word not in word2id:\n",
    "        word2id[word] = len(word2id)\n",
    "print(\"*\" *20 ,\" word2id dictionary \", \"*\" *20)\n",
    "print(word2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************  train_input  ********************\n",
      "tensor([0, 1, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "# construct the train input for example 1\n",
    "train_input = torch.LongTensor([word2id['I'], word2id['had'], word2id['much'], word2id['fun']])\n",
    "print(\"*\" *20 ,\" train_input \", \"*\" *20)\n",
    "print(train_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************  word_embeddings  ********************\n",
      "tensor([[-0.5321, -0.3469, -1.2427],\n",
      "        [-0.6171, -1.3775,  0.8161],\n",
      "        [ 0.5846, -1.0117,  1.0847],\n",
      "        [ 1.0463, -1.3028, -0.9977]], grad_fn=<EmbeddingBackward>)\n",
      "torch.Size([4, 3])\n"
     ]
    }
   ],
   "source": [
    "# obtain the word embeddings corresponding to the words in the train input\n",
    "word_embeddings = embedding(train_input)\n",
    "print(\"*\" *20 ,\" word_embeddings \", \"*\" *20)\n",
    "print(word_embeddings)\n",
    "print(word_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************  feature_input  ********************\n",
      "tensor([ 0.1204, -1.0097, -0.0849], grad_fn=<MeanBackward1>)\n"
     ]
    }
   ],
   "source": [
    "# If we want to do text classification on the sentence, we can take the mean of the vectors of all the words in the sentence,\n",
    "# thus acquiring a single vector:\n",
    "# construct the feature input (mean of word embeddings) to be fed to the classification module\n",
    "feature_input = word_embeddings.mean(0) # column-wise, dimension-wise\n",
    "print(\"*\" *20 ,\" feature_input \", \"*\" *20)\n",
    "print(feature_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, the computational graph for the CBOW model will look like this:\n",
    "\n",
    "<img src=\"images/sl2_textclassification_word2vec.jpg\" alt=\"MLP\" title=\"Word2vec - Computational Graph\" />\n",
    "\n",
    "Let us train a CBOW model from scratch on our one-sentence corpus.\n",
    "\n",
    "Before that, we will define the dataset reader for our one-sentence dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "create a dataset reader\n",
    "\"\"\"\n",
    "class CBOWDataset(Dataset):\n",
    "  \"\"\" one-sentence dataset.\"\"\"\n",
    "  def __init__(self, window_size=2):\n",
    "    # read the corpus\n",
    "    corpus = ['I had so much fun in my semester break']\n",
    "\n",
    "    window = 2 * window_size + 1\n",
    "    token2id = {}\n",
    "    # populate the word to id mapping and generate train inputs/targets\n",
    "    for sentence in corpus:\n",
    "      tokens = sentence.strip().split()\n",
    "      for token in tokens:\n",
    "        if token not in token2id.keys(): # add new word\n",
    "          token2id[token] = len(token2id) # new word index = length of token2id\n",
    "    \n",
    "      # map to index\n",
    "      tokens = [token2id[token] for token in tokens]\n",
    "    \n",
    "      # generate all training samples for this sentence\n",
    "      train_inputs, train_outputs = [], []\n",
    "      for num_win in range(len(tokens) - window + 1):\n",
    "        cur_tokens = tokens[num_win:num_win + window]\n",
    "        tgt_token = cur_tokens[window_size]\n",
    "        train_inputs.append(cur_tokens[0:window_size]+cur_tokens[window_size+1:])\n",
    "        train_outputs.append(tgt_token)\n",
    "    self.token2id = token2id\n",
    "    \n",
    "    # set the vocab. size\n",
    "    self.vocab_size = len(token2id)\n",
    "    \n",
    "    # set the total number of training examples\n",
    "    self.n = len(train_inputs)\n",
    "    \n",
    "    # convert features and labels to torch.tensor\n",
    "    self.features = torch.LongTensor(train_inputs, device=device)\n",
    "    self.labels = torch.LongTensor(train_outputs, device=device)\n",
    "    \n",
    "  # return input and output of a single example\n",
    "  # Input: Feature vectors, where each vector corresponds to a tweet. \n",
    "  # Output: Labels, where each label is one index for each of our tags in the set {positive, negative, neutral}\n",
    "  def __getitem__(self, index):\n",
    "    return self.features[index], self.labels[index]\n",
    "  \n",
    "  # return the total number of examples\n",
    "  def __len__(self):\n",
    "    return self.n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let us define the CBOW model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "create a model for CBOW\n",
    "\"\"\"\n",
    "class CBOWmodel(nn.Module):\n",
    "  \n",
    "  def __init__(self, embedding_size, vocab_size, output_size):\n",
    "    # In the constructor we define the layers for our model\n",
    "    super(CBOWmodel, self).__init__()\n",
    "    self.embedding = nn.Embedding(vocab_size, embedding_size, sparse=True)\n",
    "    \n",
    "    self.embedding.weight.data.normal_(0.0,0.05) # mean=0.0, mu=0.05\n",
    "    \n",
    "    self.linear_layer = nn.Linear(embedding_size, output_size, bias=False) # the layer will not learn an additive bias\n",
    "    self.softmax_layer = nn.LogSoftmax(dim=1)\n",
    "  \n",
    "  def forward(self, x):\n",
    "    # In the forward function we define the forward propagation logic\n",
    "    out = self.embedding(x).mean(1)\n",
    "    out = self.linear_layer(out)\n",
    "    out = self.softmax_layer(out)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the hyperparameters of the CBOW model will be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameter of CBOW model\n",
    "EMBEDDING_SIZE = 3 # size of the word embedding\n",
    "LEARNING_RATE = 0.5 # learning rate of gradient descent\n",
    "WINDOW_SIZE = 2  # number of words to be considerd before (or after) the target word for making the context\n",
    "MAX_EPOCHS = 5 # number of passes over the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now train the CBOW model on our one-sentence dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of samples in the dataset: 5\n",
      "feature matrix: tensor([[0, 1, 3, 4],\n",
      "        [1, 2, 4, 5],\n",
      "        [2, 3, 5, 6],\n",
      "        [3, 4, 6, 7],\n",
      "        [4, 5, 7, 8]])\n",
      "label matrix: tensor([2, 3, 4, 5, 6])\n"
     ]
    }
   ],
   "source": [
    "# load the dataset reader, corpus: 'I had so much fun in my semester break'\n",
    "dataset = CBOWDataset(window_size = WINDOW_SIZE)\n",
    "print(\"number of samples in the dataset:\", dataset.n)\n",
    "print(\"feature matrix:\", dataset.features)\n",
    "print(\"label matrix:\", dataset.labels)\n",
    "# create batch\n",
    "train_loader = DataLoader(dataset=dataset, batch_size=1, shuffle=True, num_workers=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CBOWmodel(\n",
      "  (embedding): Embedding(9, 3, sparse=True)\n",
      "  (linear_layer): Linear(in_features=3, out_features=9, bias=False)\n",
      "  (softmax_layer): LogSoftmax()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# define the loss function (last node of the graph)\n",
    "model = CBOWmodel(EMBEDDING_SIZE, dataset.vocab_size, dataset.vocab_size)\n",
    "model.to(device)\n",
    "print(model)\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "# create an instance of SGD with required hyperparameters\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 2.2022\n",
      "Epoch [2/5], Loss: 2.1647\n",
      "Epoch [3/5], Loss: 2.1254\n",
      "Epoch [4/5], Loss: 2.0798\n",
      "Epoch [5/5], Loss: 2.0200\n"
     ]
    }
   ],
   "source": [
    "# start the training\n",
    "for epoch in range(MAX_EPOCHS):\n",
    "  # train the model for one pass over the data\n",
    "  train_loss = train(train_loader)   \n",
    "  # print the loss for every epoch\n",
    "  print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, MAX_EPOCHS, train_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We trained CBOW model on a toyish corpus possible. In general, the model is trained on a **large corpus** of raw text and the quality of resulting word embeddings tend to get better with increase in the size of training corpus. Several researchers have released their word embeddings trained on large amounts of data. We can directly use their embeddings in the future work. For example, Google offers a [pre-trained model](https://code.google.com/archive/p/word2vec/) which was trained on Google News dataset containing about 100 billion words.\n",
    "\n",
    "You can access the word embedding in the following way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding for the word 'I': tensor([ 3.3001e-05, -2.8894e-01, -2.9169e-01])\n"
     ]
    }
   ],
   "source": [
    "# prints the embedding of word 'I'\n",
    "print(\"embedding for the word 'I':\", model.embedding.weight.data[dataset.token2id['I']]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding for the word 'semester': tensor([-0.0789,  0.0040,  0.1631])\n"
     ]
    }
   ],
   "source": [
    "# prints the embedding of word 'semester'\n",
    "print(\"embedding for the word 'semester':\", model.embedding.weight.data[dataset.token2id['semester']]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
