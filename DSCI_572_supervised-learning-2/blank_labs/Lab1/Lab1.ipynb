{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework 1 - Supervised Learning II - MDS Computational Linguistics\n",
    "\n",
    "### Assignment Topics\n",
    "- Introduction to Neural Nets, (Feed Forward-NNs / Multi-layer Perceptrons)\n",
    "- Neural Net Hyperparameter Tuning\n",
    "- Operations on tensor\n",
    "- Linearities, non-linearities and loss functions \n",
    "- Very-short answer questions\n",
    "\n",
    "### Software Requirements\n",
    "- Python (>=3.6)\n",
    "- PyTorch (>=1.2.0) \n",
    "- Jupyter (latest)\n",
    "- Scikit Learn (>=0.23.2)\n",
    "- Skorch (>=0.9)\n",
    "\n",
    "### Submission Info. \n",
    "- Due Date: 1/16/21 11:59pm (Pacific Time)\n",
    "\n",
    "## Getting Started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all necessary imports\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import sklearn\n",
    "\n",
    "# set the seed (allows reproducibility of the results)\n",
    "manual_seed = 572\n",
    "torch.manual_seed(manual_seed) # allows us to reproduce results when using random generation on the cpu\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # checks if GPU is there in this system and automatically uses GPU if its available, otherwise uses CPU.\n",
    "torch.backends.cudnn.deterministic=True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tidy Submission\n",
    "\n",
    "rubric={mechanics:1}\n",
    "\n",
    "To get the marks for tidy submission:\n",
    "- Submit the assignment by filling in this jupyter notebook with your answers embedded\n",
    "- Make sure code that is randomly initialized is set correctly with the manual_seed=572\n",
    "- Be sure to follow the [general lab instructions](https://ubc-mds.github.io/resources_pages/general_lab_instructions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise T1: Simple Neural Net Optimization\n",
    "\n",
    "For this group assignment we will be applying a simple 2-layer Feed Forward Neural Net classifier for a classification task. we'll use a newsgroup data set, which consists of the words in posts from a set of different forums, with the post data represented in Tfidf format, can our classifiers predict which forum each post came from? In essense this is a quick recap of 571, but applying it to Neural Networks, which end up having much more hyperparameters to deal with than the algorithms you encountered in 571. For your sanity, we will limit how many different trials we expect you to run in each section. \n",
    "\n",
    "Our comparison will be a SVM classifier with a simple linear kernel.\n",
    "\n",
    "### Data Loading "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups_vectorized \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_data = fetch_20newsgroups_vectorized(subset='train', remove=('headers', 'footers', 'quotes'))\n",
    "test_data =fetch_20newsgroups_vectorized(subset='test', remove=('headers', 'footers', 'quotes'))\n",
    "\n",
    "X_train = train_data.data\n",
    "y_train = train_data.target\n",
    "X_test = test_data.data\n",
    "y_test = test_data.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM Baseline\n",
    "\n",
    "Let's run a quick test to see how well an SVM does on this dataset (might take a couple minutes):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5624004248539565\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn import svm\n",
    "\n",
    "clf = svm.SVC(kernel='linear') #Use a linear kernel with default regularization for simplicity\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "pred = clf.predict(X_test)\n",
    "\n",
    "acc = metrics.accuracy_score(y_test, pred)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like SVMs and other ML classifiers, neural nets have flexibility in their configuration, the so called hyperparameters that control how the network is structured and how it learns. And like SVMs it turns out that this tuning is extremely important for appropriate performance.\n",
    "\n",
    "### Example 2 Layer NN\n",
    "\n",
    "We'll use Skorch (see https://github.com/skorch-dev/skorch for conda install instructions) to allow us to use Sklearn alongside Pytorch. Note: in later assignments in this course we'll just focus on Pytorch directly, but it's always a good tool to have in your back pocket (if you want to use say Sklearn's cross validation tools)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m3.0029\u001b[0m       \u001b[32m0.0486\u001b[0m        \u001b[35m3.0022\u001b[0m  4.1832\n",
      "      2        \u001b[36m3.0018\u001b[0m       0.0486        \u001b[35m3.0012\u001b[0m  3.6511\n",
      "      3        \u001b[36m3.0007\u001b[0m       0.0486        \u001b[35m3.0002\u001b[0m  3.5746\n",
      "      4        \u001b[36m2.9998\u001b[0m       0.0486        \u001b[35m2.9993\u001b[0m  3.4552\n",
      "      5        \u001b[36m2.9990\u001b[0m       0.0486        \u001b[35m2.9986\u001b[0m  3.4531\n",
      "      6        \u001b[36m2.9982\u001b[0m       0.0486        \u001b[35m2.9978\u001b[0m  3.3736\n",
      "      7        \u001b[36m2.9975\u001b[0m       0.0486        \u001b[35m2.9972\u001b[0m  3.4266\n",
      "      8        \u001b[36m2.9969\u001b[0m       0.0486        \u001b[35m2.9966\u001b[0m  3.4702\n",
      "      9        \u001b[36m2.9963\u001b[0m       0.0486        \u001b[35m2.9960\u001b[0m  3.4065\n",
      "     10        \u001b[36m2.9957\u001b[0m       \u001b[32m0.0499\u001b[0m        \u001b[35m2.9955\u001b[0m  3.4624\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<class 'skorch.classifier.NeuralNetClassifier'>[initialized](\n",
       "  module_=TwoLayerNN(\n",
       "    (dense0): Linear(in_features=101631, out_features=20, bias=True)\n",
       "    (nonlin): ReLU()\n",
       "    (dropout): Dropout(p=0.1)\n",
       "    (dense1): Linear(in_features=20, out_features=20, bias=True)\n",
       "    (output): Linear(in_features=20, out_features=20, bias=True)\n",
       "    (softmax): Softmax()\n",
       "  ),\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from skorch import NeuralNetClassifier\n",
    "\n",
    "\n",
    "##Define our NN\n",
    "class TwoLayerNN(nn.Module): #feel free to change the inputs it takes\n",
    "    def __init__(self, input_dim, hidden_units=20, output_classes=20, nonlin=nn.ReLU()):  \n",
    "        super(TwoLayerNN, self).__init__()\n",
    "        self.dense0 = nn.Linear(input_dim, hidden_units)\n",
    "        self.nonlin = nonlin\n",
    "        self.dropout = nn.Dropout(p=.1)\n",
    "        self.dense1 = nn.Linear(hidden_units, hidden_units)\n",
    "        self.output = nn.Linear(hidden_units, output_classes)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, X, **kwargs):\n",
    "        X = self.nonlin(self.dense0(X))\n",
    "        X = self.dropout(X)\n",
    "        X = self.nonlin(self.dense1(X))\n",
    "        X = self.softmax(self.output(X))\n",
    "        return X\n",
    "\n",
    "\n",
    "#Make sure you run these each time before you (re)initialize the network!!! \n",
    "torch.manual_seed(manual_seed)\n",
    "np.random.seed(manual_seed)\n",
    "torch.cuda.manual_seed(manual_seed)\n",
    "\n",
    "net = NeuralNetClassifier(\n",
    "    TwoLayerNN(input_dim=X_train.shape[1]),\n",
    "    max_epochs=10,\n",
    "    lr=0.01,\n",
    "    optimizer=torch.optim.SGD,   \n",
    "    optimizer__weight_decay=0.001,  #roughly equivalent to L2 regularization\n",
    "    iterator_train__shuffle=True,\n",
    "    device=device  #'cpu' or 'cuda'\n",
    ")\n",
    "\n",
    "net.fit(torch.from_numpy(X_train.todense()).float(), torch.tensor(y_train,dtype=torch.long))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05018587360594796\n"
     ]
    }
   ],
   "source": [
    "#when you're ready to run on TEST:\n",
    "pred = net.predict(torch.from_numpy(X_test.todense()).float())\n",
    "acc = metrics.accuracy_score(y_test, pred)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not great! Let's look at improving our network.\n",
    "\n",
    "### Hyperparameter Optimization\n",
    "\n",
    "It turns out there are quite a few choices that we can make to optimize our NN: the architecture of our net (number of hidden layers, number of units in each layer, choice of nonlinearity (sigmoid, ReLU etc.)) being a major factor. We also care about our choice of training procedure such as which optimizer (SGD, Adam etc.) learning rate and number of epochs. Last but not least regularization is also important (L2 regularization can be thought of as weight decay in Pytorch).   \n",
    "\n",
    "*NOTE 1*: One thing we won't touch yet, but is extremely important for good optimization is how model weights are initialized, we'll talk about this later.  \n",
    "\n",
    "*NOTE 2*: Keep it to 2 layers, you'll be doing a multilayer model in question 2 (there are tricks to make it easier to implement).\n",
    "\n",
    "#### Brief recap of these concepts\n",
    "  \n",
    "*Learning Rate*: Multiplication factor to set how far your model parameters move per each update.    \n",
    "*Optimizer*: Which algorithm is used to update the model parameters (Stochastic Gradient Descent, Adam, Adadelta etc. we'll talk about these later).  \n",
    "*Epoch*: A number/count to represent times that the model has cycled through the entire training set during training.  \n",
    "*Nonlinearity*: A non-linear function that allows for neural nets to 'learn' to solve non-linearly seperable problems. Common ones include sigmoid, tanh, ReLU.  \n",
    "*Regularization*: Penalty based on magnitude of the weighs to the model, encouraging your model to not overfit. Pytorch uses weight decay which is roughly equal to L2 regularization (but subtly differs). Other regularization types (L0, L1, L_inf etc.) might need to be manually calculated and added to loss directly.  \n",
    "*Dropout*: A regularization technique used with Neural networks, randomly zeros values (based on a specified percent) passed through the dropout layer to encourage the model to generalize learning between nodes (don't rely on just some single node for something). \n",
    "\n",
    "\n",
    "#### For these sections it is advised to split up the work with teammates.\n",
    "\n",
    "First let's do a quick manual search of the hyper parameters (you'll be graded here on just following a logical process and explaining it), second try a grid search, and finally a random search.\n",
    "\n",
    "### T1 Manual Search\n",
    "rubric={reasoning:1}\n",
    "\n",
    "As a reminder, manual search is basically just using trial-and-error to find the best combination, starting with an educated guess as to the best parameters, and then making adjustments as you go. For this part, you are only allowed to try *5* different combinations of hyperparameters.\n",
    "\n",
    "Document your starting point, and how you adjusted the hyperparameters along the way, reporting the accuracy for each round. Explain your reasoning for some of the choices you made.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Documentation Goes Here\n",
    "\n",
    "ex:  \n",
    "Round 1  \n",
    "\n",
    "Regularization(L2=.001)  \n",
    "Optimizer(SGD)  \n",
    "Max_epochs(20)  \n",
    "Learning_rate(0.01)  \n",
    "NN_layers(2)  \n",
    "NN_hidden_units(20 for all layers)  \n",
    "Dropout(.1 on output of dense0)  \n",
    "Nonlinearity(ReLU)  \n",
    "~Anything Else~    \n",
    "ACC = 0.05  Awful!  No better than chance!\n",
    "\n",
    "*YOUR WORK*\n",
    "\n",
    "*YOUR EXPLANATION*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T1 Grid Search\n",
    "rubric={accuracy:1, quality:1}\n",
    "\n",
    "Another approach is to try a number of different values by setting intervals to check and then covering all possibilities. Use Scikit-learn's grid search functionality to check a total of around 20 different possible configurations of hyperparameters. **With very large epoch/parameter values it might take 30 minutes to run a trial (depending on your CPU/GPU) so plan accordingly and potentially split up the grid between teammates** (Teammates who didn't run the code can copy outputs into a rawNB convert box)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Your code to run the grid search here\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "###Copy of outputs from teammates if you didn't run this section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T1 Random Search\n",
    "rubric={accuracy:1, quality:1}\n",
    "\n",
    "Finally, use scikit-learn's random search functionality to check a total of around 20 different possible configurations of hyperparameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Your code to run the grid search here\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "###Copy of outputs from teammates if you didn't run this section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T1 Hyperparameter Optimization Reflection\n",
    "rubric={reasoning:2}  \n",
    "Reflect on the process to get this basic Neural Network to work. Were you able to get it to perform better than our baseline SVM? How \"easy\"/\"hard\" was it to get to work well? What might you also consider (but perhaps didn't have time) to try to improve it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Tensor Operations (Pytorch Intro)\n",
    "\n",
    "We'll primarily be using Pytorch for coding neural nets in this class (as well as COLX 531 and some others), this exercise is here to give an introduction to the basic operations that are performed on the tensors that pytorch uses. Pytorch tensors are basically numpy arrays, with the advantage that they can be loaded onto GPU to perform extremely fast parallel calculations. Remember in DSCI 512 the speedup from parallelizing code? Deep learning utilizes this in spades!\n",
    "\n",
    "### 1.1 Write code that creates a tensor, **X** of size $5 \\times 5$ containing longs with values initialized to ones. \n",
    "rubric={accuracy:1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Write code that takes the tensor, **X** (from the previous question 1.1) and sets the values along the diagonal to two.\n",
    "rubric={accuracy:1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Write code that takes the tensor, **X** (from the previous question 1.2), squares all the values in **X**, sums all the squared values in **X** and prints the square root of this sum? (L2-norm) \n",
    "rubric={accuracy:1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Given the following two tensors, **X** $\\in \\mathcal{R}^{4\\times4}$ and $\\textbf{Y} \\in \\mathcal{R}^{4\\times4}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2961, 0.5166, 0.2517, 0.6886],\n",
      "        [0.0740, 0.8665, 0.1366, 0.1025],\n",
      "        [0.1841, 0.7264, 0.3153, 0.6871],\n",
      "        [0.0756, 0.1966, 0.3164, 0.4017]])\n",
      "tensor([[0.1186, 0.8274, 0.3821, 0.6605],\n",
      "        [0.8536, 0.5932, 0.6367, 0.9826],\n",
      "        [0.2745, 0.6584, 0.2775, 0.8573],\n",
      "        [0.8993, 0.0390, 0.9268, 0.7388]])\n"
     ]
    }
   ],
   "source": [
    "X = torch.rand(4,4)\n",
    "print(X)\n",
    "Y = torch.rand(4,4)\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.1 Write code that performs standard matrix multiplication, multiply **X** and **Y** without changing their values and prints the result.\n",
    "rubric={accuracy:1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.2 Write code that performs standard addition of two matrices, add **X** and **Y** without changing their values and prints the result.\n",
    "rubric={accuracy:1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.3 Write code that subtracts matrix **Y** from **X** without changing their values and prints the result.\n",
    "rubric={accuracy:1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.4 Write code that performs standard matrix multiplication, multiply **X** and **Y** and placing the results directly in **X** (modifying **X**) and prints the result.\n",
    "rubric={accuracy:1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Given the following tensor, **X** $\\in \\mathcal{R}^{5\\times3}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.rand(5,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5.1 Write code to print all the elements in the last row of **X**.\n",
    "rubric={accuracy:1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5.2 Write code to print all the elements in the middle column of **X**.\n",
    "rubric={accuracy:1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5.3 Write code to create a 3D tensor of size $1 \\times 5 \\times 3$ using the $5 \\times 3$ values from **X** (unsqueeze operation)\n",
    "rubric={accuracy:1}\n",
    "\n",
    "(You'll often need to \"squeeze\" or \"unsqueeze\" tensors to make sure that the dimensions are correct for certain parts of your model.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5.4 Write code that converts the 3D tensor (created in the previous question (c)) back into 2D tensor (of size $5 \\times 3$). (squeeze operation)\n",
    "rubric={accuracy:1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Putting the \"Multi\" in Multilayer Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In T1 you did hyper parameters optimization on a 2 layer neural network. For this assignment, you'll be using the same dataset (20 Newsgroups) but you'll need to build a model that can support an arbitrary number of layers (rather than just two)! There are two tricks to help out with this, the first is *nn.sequential* which allows you to stack pytorch modules together in a *cascade* fashion (*cascade* here meaning one thing passed to another like a pipeline). The other trick is *nn.modulelist* which will allow us to keep track of lists of modules, which we can then iterate through to build out network and perform *forward* passes through the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### nn.Sequential example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2510, 0.1498, 0.1444, 0.1669, 0.2878],\n",
      "        [0.2776, 0.1573, 0.1573, 0.1692, 0.2386],\n",
      "        [0.2137, 0.2039, 0.1484, 0.1878, 0.2462],\n",
      "        [0.2488, 0.1647, 0.1614, 0.1614, 0.2638],\n",
      "        [0.1998, 0.1814, 0.1532, 0.1532, 0.3123]], grad_fn=<SoftmaxBackward>)\n"
     ]
    }
   ],
   "source": [
    "#Make sure you run these each time before you (re)initialize the network or data!!! \n",
    "torch.manual_seed(manual_seed)\n",
    "np.random.seed(manual_seed)\n",
    "torch.cuda.manual_seed(manual_seed)\n",
    "\n",
    "\n",
    "## Fake data for testing\n",
    "x = torch.rand((5,10))\n",
    "\n",
    "\n",
    "## Building the model.  \n",
    "## Note: to use inside a larger model, set it as:  self.name_of_layer  = nn.Sequential(...)\n",
    "## in the initialization function\n",
    "\n",
    "layers = nn.Sequential(\n",
    "          nn.Linear(10,5),\n",
    "          nn.ReLU(),\n",
    "          nn.Softmax(dim=-1)\n",
    "        )\n",
    "\n",
    "## Forward pass:\n",
    "\n",
    "output = example_model(x)\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 A 5 layer MLP using nn.Sequential\n",
    "rubric={accuracy:2, quality:2}\n",
    "\n",
    "Using nn.Sequential build a network with the following parameters: 5 Hidden Layers, each with 40 hidden units, using a Tanh activation function between layers, 20% dropout on each layer, and finally a softmax output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### My MLP Here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Training and testing our 5 layer MLP\n",
    "rubric={accuracy:1, quality:1}\n",
    "\n",
    "Now train the network on the 20 Newgroups training data, and report the Test set accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### My MLP training / testing code!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### nn.ModuleList example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1650, 0.2360, 0.2021, 0.2144, 0.1825],\n",
      "        [0.1614, 0.2381, 0.2030, 0.2141, 0.1834],\n",
      "        [0.1638, 0.2382, 0.2023, 0.2146, 0.1811],\n",
      "        [0.1626, 0.2370, 0.2024, 0.2158, 0.1822],\n",
      "        [0.1650, 0.2403, 0.2015, 0.2112, 0.1820]], grad_fn=<SoftmaxBackward>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Make sure you run these each time before you (re)initialize the network or data!!! \n",
    "torch.manual_seed(manual_seed)\n",
    "np.random.seed(manual_seed)\n",
    "torch.cuda.manual_seed(manual_seed)\n",
    "\n",
    "\n",
    "## Fake data for testing\n",
    "x = torch.rand((5,10))\n",
    "\n",
    "layers = nn.ModuleList()   ## Again you'll need to assign as: self.layers = nn.ModuleList()   in the initialization function\n",
    "## Two layers\n",
    "for i in range(2):\n",
    "    layers.append(nn.Linear(10,10))\n",
    "    layers.append(nn.ReLU())\n",
    "layers.append(nn.Linear(10,5))  #output layer\n",
    "layers.append(nn.Softmax(dim=-1))\n",
    "\n",
    "## Forward pass:\n",
    "for layer in layers:\n",
    "    x = layer(x)\n",
    "\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Arbitrary depth MLP using nn.ModuleList()\n",
    "rubric={accuracy:2, quality:2}\n",
    "\n",
    "Using nn.ModuleList to build a MLP class that takes input for number of layers, input/output dimensions, hidden units, activation function, and dropout percent and builds the corresponding network.  Output should pass through appropriate layers to a final softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### My MLP Here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Training and testing a 6 layer MLP\n",
    "rubric={accuracy:1, quality:1}\n",
    "\n",
    "Using the class you built in 2.3, build and train a 6 layer MLP network on the 20 Newgroups training data, and report the Test set accuracy. You should set hidden units per layer to 50 units, dropout percent to 20%, and use sigmoid as your activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### My MLP training / testing code!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Very-Short answer questions\n",
    "\n",
    "(Double-click each question block and place your answer at the end of the question) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 What is NumPy? What are the differences between PyTorch's Tensor and NumPy Array?\n",
    "rubric={reasoning:1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 What is the key difference between ``torch.LongTensor`` and ``torch.cuda.LongTensor``?\n",
    "rubric={reasoning:1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 What is the default data type of a PyTorch tensor?\n",
    "rubric={accuracy:1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 What is ``autograd`` in PyTorch? How is it related to computational graph?\n",
    "rubric={reasoning:1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 What is SGD? How is it different from Gradient Descent? Finally what role SGD plays in building machine learning models?\n",
    "rubric={reasoning:1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Not Graded)  Linearities, Non-linearities, Loss functions by Hand\n",
    "\n",
    "This is a quick linear algebra review exercise, if you're a little rusty, or if you haven't taken a linear algebra class, this is a peak under the hood to show the math that Pytorch is doing when it's calculating tensor operations.  (See the separate solutions in the github lab folder to check your work)\n",
    "\n",
    "Sample question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([168.], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "linear_layer = torch.nn.Linear(5, 1)\n",
    "linear_layer.weight.data[0] = torch.tensor([1, 2, 3, 4, 5]) # sets the weight value\n",
    "linear_layer.bias.data = torch.tensor([3]).float() # sets the bias value\n",
    "model_out = linear_layer(torch.tensor([0, 10, 20, 15, 5]).float())\n",
    "print(model_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the values in **model\\_out** by hand. Show your work.\n",
    "\n",
    "Sample answer: (write it in markdown, not as code. if you don't like markdown, you can write the steps in a piece of paper, take a photo and attach an image in the answer block)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "your answer goes here:\n",
    "\n",
    "$model\\_out = A x + b = [1, 2, 3, 4, 5] * [0, 10, 20, 15, 5] + 3 = (1*0 + 2*10 + 3*20 + 4*15 + 5*5) + 3 = 165 + 3 = 168 $\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-f5336c006d6c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlinear_layer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mlinear_layer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# sets the weight value\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mmodel_out\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlinear_layer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m15\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_out\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "linear_layer = torch.nn.Linear(5, 1, bias=False)\n",
    "linear_layer.weight.data[0] = torch.tensor([1, 2, 3, 4, 5]) # sets the weight value\n",
    "model_out = linear_layer(torch.tensor([0, 10, 20, 15, 5]).float())\n",
    "print(model_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the values in **model\\_out** by hand. Show your work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "your answer goes here (double-click this block to edit):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.0000, 0.8808], grad_fn=<SigmoidBackward>)\n"
     ]
    }
   ],
   "source": [
    "linear_layer = torch.nn.Linear(5, 2)\n",
    "linear_layer.weight.data[0] = torch.tensor([1, 2, 3, 4, 5]) # sets the weight value\n",
    "linear_layer.weight.data[1] = torch.tensor([1, 0, 0, 0, 1]) # sets the weight value\n",
    "linear_layer.bias.data = torch.tensor([1]).float() # sets the bias value\n",
    "model_out = linear_layer(torch.tensor([0, 10, 20, 15, 1]).float())\n",
    "sigmoid_out = torch.nn.Sigmoid()(model_out)\n",
    "print(sigmoid_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the values in **sigmoid\\_out** by hand. Show your work.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "your answer goes here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000],\n",
      "        [0.9933, 0.0067]], grad_fn=<SoftmaxBackward>)\n"
     ]
    }
   ],
   "source": [
    "linear_layer = torch.nn.Linear(5, 2)\n",
    "linear_layer.weight.data[0] = torch.tensor([1, 2, 3, 4, 5]) # sets the weight value\n",
    "linear_layer.weight.data[1] = torch.tensor([1, 3, 0, 0, 10]) # sets the weight value\n",
    "linear_layer.bias.data = torch.tensor([3]).float() # sets the bias value\n",
    "model_out = linear_layer(torch.tensor([[100, 10, 20, 15, 1], [10, 5, 2, 1, 0]]).float())\n",
    "softmax_out = torch.nn.Softmax(dim=1)(model_out)\n",
    "print(softmax_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the values in **softmax\\_out** by hand. Show your work.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "your answer goes here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(7.7500, grad_fn=<MseLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "linear_layer = torch.nn.Linear(5, 2)\n",
    "linear_layer.weight.data[0] = torch.tensor([1, 2, 3, 4, 5]) # sets the weight value\n",
    "linear_layer.weight.data[1] = torch.tensor([1, 3, 0, 0, 10]) # sets the weight value\n",
    "linear_layer.bias.data = torch.tensor([3]).float() # sets the bias value\n",
    "model_out = linear_layer(torch.tensor([[100, 10, 20, 15, 1], [10, 5, 2, 1, 0]]).float())\n",
    "criterion = torch.nn.MSELoss()\n",
    "loss = criterion(model_out, torch.tensor([[245, 140], [30, 30]]).float())\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the values in **loss** by hand. Show your work.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "your answer goes here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
