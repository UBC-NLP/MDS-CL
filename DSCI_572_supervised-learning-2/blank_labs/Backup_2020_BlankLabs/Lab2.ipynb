{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework 2 - Supervised Learning II - MDS Computational Linguistics\n",
    "\n",
    "### Assignment Topics\n",
    "- More linearities, non-linearities\n",
    "- Embedding layer\n",
    "- Neural Network for sentiment analysis\n",
    "- Very-short answer questions\n",
    "\n",
    "### Software Requirements\n",
    "- Python (>=3.6)\n",
    "- PyTorch (>=1.2.0) \n",
    "- Jupyter (latest)\n",
    "\n",
    "### Submission Info.\n",
    "- Due Date: January 25, 2020, 18:00:00 (Vancouver time)\n",
    "\n",
    "## Getting Started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all necessary imports\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# set the seed (allows reproducibility of the results)\n",
    "manual_seed = 123\n",
    "torch.manual_seed(manual_seed) # allows us to reproduce results when using random generation on the cpu\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # creates the device object, either GPU (cuda) or CPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tidy Submission\n",
    "\n",
    "rubric={mechanics:1}\n",
    "\n",
    "To get the marks for tidy submission:\n",
    "- Submit the assignment by filling in this jupyter notebook with your answers embedded\n",
    "- Be sure to follow the [general lab instructions](https://ubc-mds.github.io/resources_pages/general_lab_instructions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous assignment, you had computed the values in tensors by hand.\n",
    "\n",
    "Sample question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model loss (loss_out): 8.75\n"
     ]
    }
   ],
   "source": [
    "# the model\n",
    "linear_layer = torch.nn.Linear(5, 2)\n",
    "\n",
    "# set the parameters (weights, biases)\n",
    "linear_layer.weight.data = torch.tensor([[1., 2., 3., 4., 5.], [1., 3., 0., 0., 10.]]) # sets the values of weight matrix (W)\n",
    "linear_layer.bias.data = torch.tensor([3., 1.]) # sets the values of bias vector (b) (note unlike previous assignment, we assign different bias for each output unit)\n",
    "\n",
    "# data (2 examples each with 5 input features and 2 target values)\n",
    "inputs = torch.tensor([[100., 10., 20., 15., 1.], [10., 5., 2., 1., 0.]]) # initialize the inputs (X)\n",
    "targets = torch.tensor([[245., 140.], [30., 30.]]) # initialize the targets (Y)\n",
    "\n",
    "# forward propagation\n",
    "model_out = linear_layer(inputs)\n",
    "criterion = torch.nn.MSELoss()\n",
    "loss_out = criterion(model_out, targets)\n",
    "\n",
    "print(\"model loss (loss_out):\", loss_out.data.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the values in ``loss_out`` by hand. Show your work.\n",
    "\n",
    "Sample answer: (write it in markdown, not as code. if you don't like markdown, you can write the steps in a piece of paper, take a photo and attach an image in the answer block)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "your answer goes here:\n",
    "\n",
    "First let's compute the $model\\_out$ for our inputs:\n",
    "\n",
    "$XW^T + b  = [[100,10,20,15,1],[10,5,2,1,0]] \\times [[1,2,3,4,5],[1,3,0,0,10]]^T  + [[3, 1], [3, 1]]$\n",
    "\n",
    "$= [[(100*1+10*2+20*3+15*4+1*5+3),(100*1+10*3+20*0+15*0+1*10+1)],[(10*1+5*2+2*3+1*4+0*5+3),(10*1+5*3+2*0+1*0+0*10+1)]] $\n",
    "\n",
    "$= [[248,141],[33,26]]$\n",
    "\n",
    "Now let's apply the mean squared error loss to compute $loss\\_out$:\n",
    "$\\frac{1}{n}\\sum_i^n(\\tilde{y_i} -y_i)^2$\n",
    "\n",
    "$\\frac{1}{2}(mean([(248-245),(141-140)]^2) + mean([(33-30),(26-30)]^2) = \\frac{1}{2} (\\frac{9+1}{2} +\\frac{9+16}{2}) = 8.75$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For some questions in this assignment, you will be asked to compute the number of parameters of a model.\n",
    "\n",
    "Sample question: **How many learnable (or updatable) parameters are present in the above model?**\n",
    "\n",
    "Sample answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "linear_layer.weight = $2 \\times 5$ = 10\n",
    "\n",
    "linear_layer.bias = $2 \\times 1$ = 2\n",
    "\n",
    "Thus, the number of parameters is = 10 + 2 = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  1.,   2.,   3.,   4.,   5.],\n",
      "        [  1.,   3.,   0.,   0.,  10.]])\n",
      "tensor([ 3.,  1.])\n",
      "the number of parameters: 12\n"
     ]
    }
   ],
   "source": [
    "# Python code to get the same result\n",
    "modules_in_model = [linear_layer]\n",
    "count = 0\n",
    "for module in modules_in_model:\n",
    "  for p in module.parameters():\n",
    "    print(p.data)\n",
    "    count += p.numel()\n",
    "print(\"the number of parameters:\",count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: More linearities and non-linearities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Logistic Regression (2-class classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model loss (loss_out): 3.500455617904663\n"
     ]
    }
   ],
   "source": [
    "# the model\n",
    "linear_layer = torch.nn.Linear(5, 2)\n",
    "\n",
    "# set the parameters (weights, biases)\n",
    "linear_layer.weight.data = torch.tensor([[1., 2., 3., 4., 5.], [1., 3., 0., 0., 10.]]) # sets the values of weight matrix (W)\n",
    "linear_layer.bias.data = torch.tensor([3., 1.]) # sets the values of bias vector (b) (note unlike previous assignment, we assign different bias for each output unit)\n",
    "\n",
    "# data (2 examples each with 5 input features and 1 target class value)\n",
    "inputs = torch.tensor([[100., 10., 20., 15., 1.], [10., 5., 2., 1., 0.]]) # initialize the inputs (X)\n",
    "targets = torch.tensor([0, 1]) # initialize the target classes (Y)\n",
    "\n",
    "# forward propagation\n",
    "model_out = linear_layer(inputs)\n",
    "softmax_out = nn.LogSoftmax(dim=1)(model_out) # documentation: https://pytorch.org/docs/stable/nn.html#torch.nn.LogSoftmax\n",
    "nll_loss = nn.NLLLoss() # documentation: https://pytorch.org/docs/stable/nn.html#torch.nn.NLLLoss\n",
    "loss_out = nll_loss(softmax_out, targets)\n",
    "\n",
    "print(\"model loss (loss_out):\",loss_out.data.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Compute the values in ``loss_out`` by hand. Show your work. \n",
    "(Hint: It might be easier if you compute ``model_out`` first, followed by ``softmax_out`` and then compute ``loss_out``) \n",
    "\n",
    "rubric={accuracy:3}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**your answer goes here:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Single layer neural network (2-class classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model loss (loss_out): 30.0\n"
     ]
    }
   ],
   "source": [
    "# the model\n",
    "layer_1 = torch.nn.Linear(5, 2) \n",
    "activation_fn = nn.ReLU()\n",
    "layer_2 = torch.nn.Linear(2, 2)\n",
    "softmax_out = nn.LogSoftmax(dim=1)(model_out)\n",
    "nll_loss = nn.NLLLoss()\n",
    "\n",
    "# set the parameters (weights, biases)\n",
    "layer_1.weight.data = torch.tensor([[1., 2., 3., 4., 5.], [1., 3., 0., 0., 10.]]) # sets the values of weight matrix for 1st layer (W_1)\n",
    "layer_1.bias.data = torch.tensor([3., 1.]) # sets the values of bias vector for 1st layer (b_1)\n",
    "layer_2.weight.data = torch.tensor([[2., 2.], [1, 1]]) # sets the values of weight matrix for 2nd layer (W_2)\n",
    "layer_2.bias.data = torch.tensor([2., 1.]) # sets the values of bias vector for 2nd layer (b_2)\n",
    "\n",
    "# data (2 examples each with 5 features and 1 target class value)\n",
    "inputs = torch.tensor([[100., 10., 20., 15., 1.], [10., 5., 2., 1., 0.]]) # initialize the inputs (X)\n",
    "targets = torch.tensor([0, 1]) # initialize the target classes (Y)\n",
    "\n",
    "# forward propagation\n",
    "layer1_out = layer_1(inputs)\n",
    "layer1_hidden = activation_fn(layer1_out)\n",
    "layer2_out = layer_2(layer1_hidden)\n",
    "softmax_out = nn.LogSoftmax(dim=1)(layer2_out)\n",
    "loss_out = nll_loss(softmax_out, targets)\n",
    "\n",
    "print(\"model loss (loss_out):\", loss_out.data.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Compute the values in ``loss_out`` by hand. Show your work. \n",
    "(Hint: It might be easier if you compute ``layer1_out`` first, followed by ``layer1_hidden`` and the rest  ``layer2_out``, ``softmax_out``, ``loss_out`` in order.) \n",
    "\n",
    "rubric={accuracy:4}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**your answer goes here:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Multi layer neural network (2-class classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model loss (loss_out): 143.5\n"
     ]
    }
   ],
   "source": [
    "# the model\n",
    "layer_1 = torch.nn.Linear(5, 4)\n",
    "activation_fn = nn.ReLU()\n",
    "layer_2 = torch.nn.Linear(4, 3)\n",
    "layer_3 = torch.nn.Linear(3, 2)\n",
    "softmax_out = nn.LogSoftmax(dim=1)(model_out)\n",
    "nll_loss = nn.NLLLoss()\n",
    "\n",
    "# set the parameters (weights, biases)\n",
    "layer_1.weight.data = torch.tensor([[1., 2., 3., 4., 5.], [1., 3., 0., 0., 10.], [1., 0., 0., 4., 5.], [1., 3., 1., 0., 0.]]) # sets the values of weight matrix for 1st layer (W_1)\n",
    "layer_1.bias.data = torch.tensor([3., 1., 1., 0.]) # sets the values of bias vector for 1st layer (b_1)\n",
    "layer_2.weight.data = torch.tensor([[2., 2., 2., 2.], [1., 1., 1., 1.], [0., 1., 1., 0.]]) # sets the values of weight matrix for 2nd layer (W_2)\n",
    "layer_2.bias.data = torch.tensor([2., 1., 0.]) # sets the values of bias vector for 2nd layer (b_2)\n",
    "layer_3.weight.data = torch.tensor([[2., 1., 2.], [1., 1., 0.]]) # sets the values of weight matrix for 3rd layer (W_3)\n",
    "layer_3.bias.data = torch.tensor([1., 0.]) # sets the values of bias vector for 3rd layer (b_3)\n",
    "\n",
    "# data (2 examples each with 5 features and 1 target class value)\n",
    "inputs = torch.tensor([[100., 10., 20., 15., 1.], [10., 5., 2., 1., 0.]]) # initialize the inputs (X)\n",
    "targets = torch.tensor([0, 1]) # initialize the target classes (Y)\n",
    "\n",
    "# forward propagation\n",
    "layer1_out = layer_1(inputs)\n",
    "layer1_hidden = activation_fn(layer1_out)\n",
    "layer2_out = layer_2(layer1_hidden)\n",
    "layer2_hidden = activation_fn(layer2_out)\n",
    "layer3_out = layer_3(layer2_hidden)\n",
    "softmax_out = nn.LogSoftmax(dim=1)(layer3_out)\n",
    "loss_out = nll_loss(softmax_out, targets)\n",
    "\n",
    "print(\"model loss (loss_out):\", loss_out.data.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Compute the values in **loss_out** by hand. Show your work.\n",
    "(Hint: It might be easier if you compute ``layer1_out`` first, followed by ``layer1_hidden`` and the rest  ``layer2_out``, ``layer2_hidden``, ``layer3_out``, ``softmax_out``, ``loss_out`` in order.) \n",
    "\n",
    "rubric={accuracy:5}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**your answer goes here:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer_1 weight:\n",
      " tensor([[ 0.1564, -0.3429,  0.3451,  0.1402,  0.3094],\n",
      "        [-0.1759,  0.0948,  0.4367,  0.3008,  0.3587],\n",
      "        [-0.0939,  0.3407, -0.3503,  0.0387, -0.2518],\n",
      "        [-0.1043, -0.1145,  0.0335,  0.4070,  0.2214]])\n"
     ]
    }
   ],
   "source": [
    "layer_1 = torch.nn.Linear(5, 4)\n",
    "print(\"layer_1 weight:\\n\", layer_1.weight.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From what distribution (and range) the default values in ``layer_1.weight.data`` are sampled from? \n",
    "(Hint: You can look at the [documentation](https://pytorch.org/docs/stable/nn.html#torch.nn.Linear) and/or [source code](https://pytorch.org/docs/stable/_modules/torch/nn/modules/linear.html#Linear) of ``nn.Linear``. And the symbols  $\\textit{U}$ and $\\mathcal{N}$ correspond to uniform and normal (Gaussian) distribution.)\n",
    "\n",
    "rubric={accuracy:2}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**your answer goes here:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Write code to initialize parameters (weights and biases) in ``layer_1`` (previous queston, 1.4) with numbers sampled randomly from standard normal distribution (mean 0 and variance 1).\n",
    "\n",
    "Hint: Look at ``torch.randn`` function\n",
    "\n",
    "rubric={accuracy:2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 How many learnable (or updatable) parameters are present in the single layer neural network defined in ``1.2``? Compute the result by writing code or by hand.\n",
    "rubric={accuracy:2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here (if you are writing code, use this block. Otherwise, change this block to a Markdown block.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.7 How many learnable (or updatable) parameters are present in the multi layer neural network defined in ``1.3``? Compute the result by writing code or by hand.\n",
    "\n",
    "rubric={accuracy:2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here (if you are writing code, use this block. Otherwise, change this block to a Markdown block.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Embedding layer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings_avg:\n",
      " tensor([[  2.0000,   1.0000,   1.5000],\n",
      "        [  5.5000,  10.5000,  15.5000]])\n"
     ]
    }
   ],
   "source": [
    "# the model\n",
    "embedding_model = nn.Embedding(4, 3) # 4 embeddings with each of three dimensions\n",
    "\n",
    "# set the weights for each of the four embedding\n",
    "embedding_model.weight.data = torch.tensor([[1., 2., 3.], [1., 1., 1.], [3., 0., 0.], [10., 20., 30.]])\n",
    "\n",
    "# data (2 examples each with two inputs)\n",
    "inputs = torch.tensor([[0, 2], [1, 3]])\n",
    "\n",
    "# forward propagation for computing average of input embeddings\n",
    "embeddings_out = embedding_model(inputs)\n",
    "embeddings_avg = embeddings_out.mean(1)\n",
    "print(\"embeddings_avg:\\n\", embeddings_avg.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Compute the values in **embeddings_avg** by hand. Show your work.\n",
    "rubric={accuracy:2}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**your answer goes here:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Can you reimplement 2.1 by using [``nn.EmbeddingBag``](https://pytorch.org/docs/stable/nn.html#torch.nn.EmbeddingBag) instead of ``nn.Embedding`` by setting the appropriate mode?  (OPTIONAL question)\n",
    "\n",
    "Hint: You can look at the [documentation](https://pytorch.org/docs/stable/nn.html#torch.nn.EmbeddingBag) and/or [source code](https://pytorch.org/docs/stable/_modules/torch/nn/modules/sparse.html#EmbeddingBag) of ``nn.EmbeddingBag``\n",
    "\n",
    "rubric={spark:2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 \n",
    "\n",
    "Toy corpus worth 3 sentences (each row excluding the header corresponds to a sentence)\n",
    "\n",
    "|  sentence no. | sentence text |\n",
    "| --------------- | ------------------------------ |\n",
    "| 1  | UBC’s Master of Data Science in Computational Linguistics is the credential to set you apart. |\n",
    "| 2  | Offered at the Vancouver campus, this unique degree is tailored to those with a passion for language and data.|\n",
    "| 3  | Over 10 months, the program combines foundational data science courses with advanced computational linguistics courses—equipping graduates with the skills to turn language-related data into knowledge and to build AI that can interpret human language. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.1 In the tutorial, we constructed word to index mapping for a one sentence corpus. Write code to build word to index mapping for this toy corpus containing three sentences.\n",
    "\n",
    "rubric={accuracy:2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.2 In the tutorial, we constructed the train data (input and output for each training example) for a one sentence corpus. Write code that outputs the train data for CBOW model created from this toy corpus and prints the number of training examples. \n",
    "\n",
    "Note:\n",
    "- Assume the **window size to be 3**. \n",
    "- Use **truecase** of the words. \n",
    "- Use [white space tokenizer](https://kite.com/python/docs/nltk.WhitespaceTokenizer) to get the words from each sentence and no further preprocessing. \n",
    "- A training example is generated from a sentence and doesn't span across multiple sentences. \n",
    "\n",
    "rubric={accuracy:4}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.3 Write code that\n",
    "- defines the CBOW model\n",
    "- train the CBOW model (update its parameters) with all the training examples (use SGD)\n",
    "- prints the word embedding for one of the word involved in the training before and after training\n",
    "- assumes the hyperparameters: EMBEDDING_SIZE to 3, LEARNING_RATE to 0.5, WINDOW_SIZE to 3 and MAX_EPOCHS to 1.\n",
    "\n",
    "rubric={accuracy:6}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.4 How many learnable (or updatable) parameters are present in the model defined in 2.3.3. Compute the result by writing code or by hand.\n",
    "rubric={accuracy:2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here (if you are writing code, use this block. Otherwise, change this block to a Markdown block.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Neural Network for Sentiment Analysis\n",
    "\n",
    "\n",
    "The multilayer neural network code used in our tutorial is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiLayerNeuralNetworkModel(\n",
      "  (model): Sequential(\n",
      "    (0): Linear(in_features=5000, out_features=50, bias=True)\n",
      "    (1): Sigmoid()\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Sigmoid()\n",
      "    (4): Linear(in_features=50, out_features=3, bias=True)\n",
      "    (5): LogSoftmax()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# all the necessary imports\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "\n",
    "# set the seed\n",
    "manual_seed = 123\n",
    "torch.manual_seed(manual_seed)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_gpu = torch.cuda.device_count()\n",
    "if n_gpu > 0:\n",
    "  torch.cuda.manual_seed(manual_seed)\n",
    "\n",
    "# hyperparameters\n",
    "BATCH_SIZE = 5\n",
    "MAX_EPOCHS = 15\n",
    "LEARNING_RATE = 0.5\n",
    "MAX_FEATURES = 5000 # x_j, the number of j\n",
    "NUM_CLASSES = 3\n",
    "\n",
    "# dataset\n",
    "DATA_FOLDER = \"data/sentiment-twitter-2016-task4\"\n",
    "TRAIN_FILE = DATA_FOLDER + \"/train.tsv\"\n",
    "VALID_FILE = DATA_FOLDER + \"/dev.tsv\"\n",
    "TEST_FILE = DATA_FOLDER + \"/test.tsv\"\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# function for reading tsv file\n",
    "def read_corpus(file):\n",
    "    corpus = [] \n",
    "    for line in open(file):\n",
    "        content, label = line.strip().split(\"\\t\") # first column is tweet, second column is golden label.\n",
    "        corpus.append(content)\n",
    "    return corpus\n",
    "\n",
    "# reads the train corpus\n",
    "train_corpus = read_corpus(TRAIN_FILE)  # get a list of tweets\n",
    "\n",
    "# define the vectorizer\n",
    "# builds a vocabulary that only considering the top max_features ordered by term frequency across the corpus.\n",
    "vectorizer = TfidfVectorizer(max_features=MAX_FEATURES)\n",
    "\n",
    "# fit the vectorizer on train set\n",
    "vectorizer.fit(train_corpus)\n",
    "\n",
    "# create a new class inheriting torch.utils.data.Dataset\n",
    "class TweetSentimentDataset(Dataset):\n",
    "  \"\"\" sentiment-twitter-2016-task4 dataset.\"\"\"\n",
    "  def __init__(self, file, vectorizer):\n",
    "    # read the corpus\n",
    "    corpus, labels = [], []\n",
    "    for line in open(file):\n",
    "      content, label = line.strip().split(\"\\t\")\n",
    "      corpus.append(content)\n",
    "      labels.append(int(label))\n",
    "    \n",
    "    # set the size of the corpus\n",
    "    self.n = len(corpus)\n",
    "    \n",
    "    # vectorize all the tweets\n",
    "    features = vectorizer.transform(corpus)\n",
    "    \n",
    "    # convert features and labels to torch.tensor\n",
    "    self.features = torch.from_numpy(features.toarray()).float()\n",
    "    self.features.to(device)\n",
    "    self.labels = torch.tensor(labels, device=device, requires_grad=False)\n",
    "    \n",
    "  # return input and output of a single example\n",
    "  # Input: Feature vectors, where each vector corresponds to a tweet. \n",
    "  # Output: Labels, where each label is one index for each of our tags in the set {positive, negative, neutral}\n",
    "  def __getitem__(self, index):\n",
    "    return self.features[index], self.labels[index]\n",
    "  \n",
    "  # return the total number of examples\n",
    "  def __len__(self):\n",
    "    return self.n\n",
    "\n",
    "# create the dataloader object\n",
    "train_loader = DataLoader(dataset=TweetSentimentDataset(TRAIN_FILE, vectorizer), batch_size=BATCH_SIZE, shuffle=True, num_workers=2) \n",
    "valid_loader = DataLoader(dataset=TweetSentimentDataset(VALID_FILE, vectorizer), batch_size=BATCH_SIZE, shuffle=False, num_workers=1)\n",
    "test_loader = DataLoader(dataset=TweetSentimentDataset(TEST_FILE, vectorizer), batch_size=BATCH_SIZE, shuffle=False, num_workers=1) \n",
    "\n",
    "# train logic (similar to that of linear regression model)\n",
    "def train(loader):\n",
    "  total_loss = 0.0\n",
    "  # iterate throught the data loader\n",
    "  num_batches = 0\n",
    "  for batch in loader:\n",
    "    # load the current batch\n",
    "    batch_input, batch_output = batch\n",
    "    \n",
    "    # forward propagation\n",
    "    # pass the data through the model\n",
    "    model_outputs = model(batch_input)\n",
    "    # compute the loss\n",
    "    cur_loss = criterion(model_outputs, batch_output)\n",
    "    total_loss += cur_loss.item()\n",
    "    \n",
    "    # backward propagation (compute the gradients and update the model)\n",
    "    # clear the buffer\n",
    "    optimizer.zero_grad()\n",
    "    # compute the gradients\n",
    "    cur_loss.backward()\n",
    "    # update the weights\n",
    "    optimizer.step()\n",
    "    \n",
    "    num_batches += 1\n",
    "  return total_loss/num_batches\n",
    "\n",
    "# evaluation logic based on classification accuracy\n",
    "def evaluate(loader):\n",
    "  accuracy, num_examples = 0.0, 0\n",
    "  with torch.no_grad(): # impacts the autograd engine and deactivate it. reduces memory usage and speeds up computation\n",
    "    for batch in loader:\n",
    "      # load the current batch\n",
    "      batch_input, batch_output = batch\n",
    "      # forward propagation\n",
    "      # pass the data through the model\n",
    "      model_outputs = model(batch_input)\n",
    "      # identify the predicted class for each example in the batch\n",
    "      _, predicted = torch.max(model_outputs.data, 1)\n",
    "      # compare with batch_output (gold labels) to compute accuracy\n",
    "      accuracy += (predicted == batch_output).sum().item()\n",
    "      num_examples += batch_output.size(0)\n",
    "  return accuracy/num_examples\n",
    "\n",
    "\"\"\"\n",
    "create a custom model class inheriting torch.nn.Module\n",
    "\"\"\"\n",
    "class MultiLayerNeuralNetworkModel(nn.Module):\n",
    "  \n",
    "  def __init__(self, num_inputs, hidden_layers, num_outputs):\n",
    "    # In the constructor we define the layers for our model\n",
    "    super(MultiLayerNeuralNetworkModel, self).__init__()\n",
    "    \n",
    "    modules = [] # stores all the layers for the neural network\n",
    "    input_dim = num_inputs\n",
    "    # add input layer followed by hidden layers (excluding the classification module)\n",
    "    for hidden_layer in hidden_layers:\n",
    "      # add one layer followed by non-linearity (nn.Sigmoid)\n",
    "      modules.append(nn.Linear(input_dim, hidden_layer))\n",
    "      modules.append(nn.Sigmoid())\n",
    "      input_dim = hidden_layer\n",
    "    # add the classification module\n",
    "    modules.append(nn.Linear(input_dim, num_outputs))\n",
    "    modules.append(nn.LogSoftmax(dim=1))\n",
    "    \n",
    "    # create the model from all the modules\n",
    "    self.model = nn.Sequential(*modules) # container of layers, for more details: https://pytorch.org/docs/stable/nn.html#torch.nn.Sequential\n",
    "  \n",
    "  def forward(self, x):\n",
    "    # In the forward function we define the forward propagation logic\n",
    "    out = self.model(x)\n",
    "    return out\n",
    "\n",
    "# hyperparameter of neural network\n",
    "hidden_layers = [50, 50]  # [num. of hidden units in first layer, num. of hidden units in second layer]\n",
    "\n",
    "# define the loss function (last node of the graph)\n",
    "model = MultiLayerNeuralNetworkModel(MAX_FEATURES, hidden_layers, NUM_CLASSES)\n",
    "model.to(device)\n",
    "print(model)\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "# create an instance of SGD with required hyperparameters\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/15], Loss: 1.0182, Training Accuracy: 0.5157, Validation Accuracy: 0.4217\n",
      "Epoch [2/15], Loss: 1.0047, Training Accuracy: 0.5157, Validation Accuracy: 0.4217\n",
      "Epoch [3/15], Loss: 1.0016, Training Accuracy: 0.5157, Validation Accuracy: 0.4217\n",
      "Epoch [4/15], Loss: 1.0024, Training Accuracy: 0.5157, Validation Accuracy: 0.4217\n",
      "Epoch [5/15], Loss: 1.0025, Training Accuracy: 0.5157, Validation Accuracy: 0.4217\n",
      "Epoch [6/15], Loss: 1.0009, Training Accuracy: 0.5157, Validation Accuracy: 0.4217\n",
      "Epoch [7/15], Loss: 0.9985, Training Accuracy: 0.5157, Validation Accuracy: 0.4217\n",
      "Epoch [8/15], Loss: 0.9924, Training Accuracy: 0.5157, Validation Accuracy: 0.4217\n",
      "Epoch [9/15], Loss: 0.9610, Training Accuracy: 0.5943, Validation Accuracy: 0.4747\n",
      "Epoch [10/15], Loss: 0.9114, Training Accuracy: 0.6082, Validation Accuracy: 0.4927\n",
      "Epoch [11/15], Loss: 0.8673, Training Accuracy: 0.6205, Validation Accuracy: 0.4902\n",
      "Epoch [12/15], Loss: 0.8248, Training Accuracy: 0.6720, Validation Accuracy: 0.5208\n",
      "Epoch [13/15], Loss: 0.7881, Training Accuracy: 0.6838, Validation Accuracy: 0.5233\n",
      "Epoch [14/15], Loss: 0.7654, Training Accuracy: 0.6958, Validation Accuracy: 0.5168\n",
      "Epoch [15/15], Loss: 0.7391, Training Accuracy: 0.7268, Validation Accuracy: 0.5163\n"
     ]
    }
   ],
   "source": [
    "# start the training\n",
    "for epoch in range(MAX_EPOCHS):\n",
    "  # train the model for one pass over the data\n",
    "  train_loss = train(train_loader)\n",
    "  # compute the training accuracy\n",
    "  train_acc = evaluate(train_loader)\n",
    "  # compute the validation accuracy \n",
    "  val_acc = evaluate(valid_loader)\n",
    "  # print the loss for every epoch\n",
    "  print('Epoch [{}/{}], Loss: {:.4f}, Training Accuracy: {:.4f}, Validation Accuracy: {:.4f}'.format(epoch+1, MAX_EPOCHS, train_loss, train_acc, val_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 In the original tutorial, we considered only *unigrams* as features to represent a tweet. Change the original tutorial code to consider *bigrams, trigrams* (along with unigrams, which is considered by default) as features to represent a tweet.\n",
    "\n",
    "Hints: \n",
    "- Look at the documentation of [``TfidfVectorizer``](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) to see how to incorporate bigrams, trigrams and so on.\n",
    "- Modify the line ``vectorizer = TfidfVectorizer(max_features=MAX_FEATURES)`` and keep the rest of the code intact.\n",
    "\n",
    "### 3.1 **Hand in the**\n",
    "- Accuracy on the validation set after training\n",
    "- Python code (ONLY the changed lines)\n",
    "\n",
    "rubric={accuracy:3, quality:1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Report performance of your model here:** (double-click to edit)\n",
    "\n",
    "**3.1.1 Performance of my neural network model on the validation set after training is X.XXX%** on accuracy (fill in your accuracy)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.1.2 Your code:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here (put only the changed lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2  In the original tutorial, we considered only two hidden layers with 50 units each. Change the original tutorial code to consider *five hidden layers* with *100 dimensions each*.\n",
    "\n",
    "Hints: \n",
    "- Modify the line ``hidden_layers = [50, 50]`` and keep the rest of the code intact.\n",
    "\n",
    "### 3.2 **Hand in the**\n",
    "- Accuracy on the validation set after training\n",
    "- Python code (ONLY the changed lines)\n",
    "\n",
    "rubric={accuracy:3, quality:1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Report performance of your model here:** (double-click to edit)\n",
    "\n",
    "**3.2.1  Performance of my neural network model on the validation set after training is X.XXX%** on accuracy (fill in your accuracy)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.2.2 Your code:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here (put only the changed lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 In the original tutorial, we used [*Sigmoid*](https://pytorch.org/docs/stable/nn.functional.html#torch.nn.functional.sigmoid) as the activation function. Change the original tutorial code to consider other nonlinearities such as [*ReLU*](https://pytorch.org/docs/stable/nn.functional.html#torch.nn.functional.relu) and [*Tanh*](https://pytorch.org/docs/stable/nn.functional.html#torch.nn.functional.tanh) and report the nonlinearity that gives the best validation performance.\n",
    "\n",
    "Hints: \n",
    "- Modify the line ``modules.append(nn.Sigmoid())`` and keep the rest of the code intact.\n",
    "\n",
    "### 3.3 **Hand in the**\n",
    "- Nonlinearity function that gives the best validation performance\n",
    "- Accuracy on the validation set after training with the best nonlinearity\n",
    "- Python code (ONLY the changed lines)\n",
    "\n",
    "rubric={accuracy:4, quality:1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Report your results here:** (double-click to edit)\n",
    "\n",
    "**3.3.1. Report the nonlinearity function that gives the best validation performance: nn.XXX()** (change this)\n",
    "\n",
    "**3.3.2. Report performance of my neural network model on the validation set after training is X.XXX%** on accuracy (fill in your accuracy)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.3.3 Your code:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here (put only the changed lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 In the original tutorial, we used *learning rate of 0.5*. Change the original tutorial code by trying out different learning rates preferably *between 0.0001 and 1*. Report the learning rate that gives the best validation performance.\n",
    "\n",
    "Hints: \n",
    "- Modify the line ``LEARNING_RATE = 0.5`` and keep the rest of the code intact.\n",
    "\n",
    "### 3.4 **Hand in the**\n",
    "- Learning rate that gives the best validation performance\n",
    "- Accuracy on the validation set after training with the best learning rate\n",
    "- Python code (ONLY the changed lines)\n",
    "\n",
    "rubric={accuracy:6, quality:1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Report your resuls here:** (double-click to edit)\n",
    "\n",
    "**3.4.1. Report the learning rate that gives the best validation performance: X.XXX** (fill in your learning rate).\n",
    "\n",
    "**3.4.2. Report the performance of my neural network model on the validation set after training is X.XXX%** on accuracy (fill in your accuracy)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.4.3 Your code:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here (put only the changed lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: Very-Short answer questions\n",
    "\n",
    "(Double-click each question block and place your answer at the end of the question) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Can we build deep neural network without any nonlinearity layers? What's the nature of such deep neural network without any nonlinearity layers?\n",
    "rubric={reasoning:2}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 What is the distinction between single layer neural network and multilayer neural network?\n",
    "rubric={reasoning:2}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 What is the difference between hidden layer and hidden unit (or neuron)?\n",
    "rubric={reasoning:2}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 What is the gradient of sigmoid function over a scalar ($\\nabla \\sigma(a)$)? What is the interesting property of this gradient?\n",
    "rubric={reasoning:2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
