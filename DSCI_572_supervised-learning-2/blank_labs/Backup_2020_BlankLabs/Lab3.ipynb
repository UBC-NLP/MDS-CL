{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework 3 - Supervised Learning II - MDS Computational Linguistics\n",
    "\n",
    "### Assignment Topics\n",
    "- Recurrent Neural Networks\n",
    "- Long Short-Term \n",
    "- Saving and Loading NN models using Pytorch\n",
    "- Very-short answer questions\n",
    "\n",
    "### Software Requirements\n",
    "- Python (>=3.6)\n",
    "- PyTorch (>=1.2.0) \n",
    "- Jupyter (latest)\n",
    "\n",
    "### Submission Info.\n",
    "- Due Date: February 1, 2020, 18:00:00 (Vancouver time)\n",
    "\n",
    "## Getting Started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all the necessary imports\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch\n",
    "import torchtext\n",
    "from torchtext.data import Field, LabelField\n",
    "from torchtext.data import TabularDataset\n",
    "from torchtext.data import Iterator, BucketIterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the seed\n",
    "manual_seed = 123\n",
    "torch.manual_seed(manual_seed)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_gpu = torch.cuda.device_count()\n",
    "if n_gpu > 0:\n",
    "    torch.cuda.manual_seed(manual_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tidy Submission\n",
    "\n",
    "rubric={mechanics:1}\n",
    "\n",
    "To get the marks for tidy submission:\n",
    "- Submit the assignment by filling in this jupyter notebook with your answers embedded\n",
    "- Be sure to follow the [general lab instructions](https://ubc-mds.github.io/resources_pages/general_lab_instructions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Building a Recurrent Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, we provide a corpus from the [CL-Aff shared task](https://sites.google.com/view/affcon2019/cl-aff-shared-task?authuser=0). HappyDB is a dataset of about 100,000 `happy moments` crowd-sourced via Amazonâ€™s Mechanical Turk where each worker was asked to describe in a complete sentence `what made them happy in the past 24 hours`. Each user was asked to describe three such moments. \n",
    "In this exercise, we focus on sociality classification. We only use labelled dataset which include 10,562 labelled samples. \n",
    "\n",
    "We have already preprocessed (tokenization, removing URLs, mentions, hashtags and so on) the tweets and placed it under ``data/happy_db`` folder in three files as ``train.tsv``, ``dev.tsv`` and ``test.tsv``."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Write code to  define a `whitespace_tokenizer` whose input is a tweet text.\n",
    "\n",
    "rubric={accuracy:1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def whitespace_tokenize(text):\n",
    "    # your code goes here "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Wrote code to define `TorchText's Fields` handle how data should be processed.  \n",
    "Hint: You need 2 Fields `TEXT` and `LABEL` for tweet text and label respectively. \n",
    "\n",
    "* The tokenizer is the whitespace_tokenizer in 1.1.\n",
    "* Use the truecase of words. \n",
    "\n",
    "rubric={accuracy:2} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 Write code to use `TabularDataset class`  and `Fields` to process the tsv files (train, dev, and test). \n",
    "\n",
    "Hint 1: `Fields` will call tokenizer, and convert tokens to numerical index. \n",
    "\n",
    "Hint 2: Use `TabularDataset.splits(...)` to load train, dev, and test sets.\n",
    "\n",
    "rubric={accuracy:2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4  Write code to build your vocabulary to map words and labels to integers. The maximum size of vocabulary is 5,000 (not count `<pad>` and `<unk>` in).\n",
    "Hint: Use `build_vocab` to build vocabularies for `TEXT` field and `LABEL` field respectively. \n",
    "\n",
    "rubric={accuracy:2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.5 Write code to print the sizes of your two vocabularies (i.e., `TEXT` and `LABEL`) individually. \n",
    "\n",
    "rubric={accuracy:2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.6 Write code to construct the Iterators to get the train, dev, and test splits. Use `BucketIterator` to initialize the Iterators for the train, dev, and test data.\n",
    "* apply same batch size of 32 on train, dev, and test set.\n",
    "* Samples are sorted by length.\n",
    "* Sort samples within each batch\n",
    "\n",
    "Hint: Use `BucketIterator.splits(...)`\n",
    "\n",
    "rubric={accuracy:2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.7.1 Write code to create a class called `LSTMmodel` to define a classifier of the task. In this model, you should have:\n",
    "\n",
    "1. An embedding layer with input dimention equal to the size of your `TEXT` vocabulary, and that represent each token in a 300-dimentional vector.  The parameters of this embedding layer should be randomly initialized with numbers sampled from a normal distribution (mean 0 and variance 0.05).\n",
    "\n",
    "2. Two uni-directional `LSTM` layers. Each layer has 500 hidden unites.\n",
    "\n",
    "3. Pass the last hidden of last layer into a `Tanh` activation function, then feed the output of `Tanh` to a linear layer whose dimensionality is equal to the number of classes in the dataset (i.e., 2 in our case).\n",
    "\n",
    "4. Then, a `LogSoftmax` layer on top of the outcome of `ReLU`.\n",
    "\n",
    "5. Return the output of `LogSoftmax` layer.\n",
    "\n",
    "Hint: `Tanh` might not be the ideal function to use, but we want you to explore it. (Usually `ReLU` works well).\n",
    "\n",
    "rubric={accuracy:8, quality:4}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.7.2 Write code to Instantiate the model class with aforementioned hyper-parameters and print out the model architecture.\n",
    "rubric={accuracy:3}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint: Your output should generally look like the following (with correct names of the model, functions, etc.):\n",
    "```\n",
    "modelName (\n",
    "  (embedding): Embedding(xxx, xxx)\n",
    "  (lstm_rnn): MODEL(xxx, xxx, num_layers=xxx)\n",
    "  (activation_fn): Tanh()\n",
    "  (linear_layer): Linear(in_features=xxx, out_features=xxx, bias=True)\n",
    "  (softmax_layer): somethingRelatedToSoftmax()\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create an optimizer for training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.1\n",
    "criterion = nn.NLLLoss()\n",
    "# create an instance of SGD with required hyperparameters\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.8.1 How many learnable (or updatable) parameters are present in the model defined in 1.7. Compute the result by writing code.\n",
    "rubric={accuracy:1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.8.2 OPTIONAL QUESTION: How many megabyte memory will this model use? Please show your work. \n",
    "\n",
    "Hint1: Each parameter is a `torch.float32` tensor which is 32-bit floating point. \n",
    "\n",
    "Hint2: All the parameters of this model are learnable (or updatable) parameters.\n",
    "\n",
    "rubric={spark:2}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write your answer here.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To facilitate your work for the next questions (1.9 and 1.10), we provide two functions for training and evaluation.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def train(loader):\n",
    "    total_loss = 0.0\n",
    "    # iterate throught the data loader\n",
    "    num_sample = 0\n",
    "    for batch in loader:\n",
    "        # load the current batch\n",
    "        batch_input = batch.tweet\n",
    "        batch_output = batch.label\n",
    "        \n",
    "        batch_input = batch_input.to(device)\n",
    "        batch_output = batch_output.to(device)\n",
    "        # forward propagation\n",
    "        # pass the data through the model\n",
    "        model_outputs = model(batch_input)\n",
    "        # compute the loss\n",
    "        cur_loss = criterion(model_outputs, batch_output)\n",
    "        total_loss += cur_loss.item()\n",
    "\n",
    "        # backward propagation (compute the gradients and update the model)\n",
    "        # clear the buffer\n",
    "        optimizer.zero_grad()\n",
    "        # compute the gradients\n",
    "        cur_loss.backward()\n",
    "        # update the weights\n",
    "        optimizer.step()\n",
    "\n",
    "        num_sample += batch_output.shape[0]\n",
    "    return total_loss/num_sample\n",
    "\n",
    "# evaluation logic based on classification accuracy\n",
    "def evaluate(loader):\n",
    "    all_pred=[]\n",
    "    all_label = []\n",
    "    with torch.no_grad(): # impacts the autograd engine and deactivate it. reduces memory usage and speeds up computation\n",
    "        for batch in loader:\n",
    "             # load the current batch\n",
    "            batch_input = batch.tweet\n",
    "            batch_output = batch.label\n",
    "\n",
    "            batch_input = batch_input.to(device)\n",
    "            # forward propagation\n",
    "            # pass the data through the model\n",
    "            model_outputs = model(batch_input)\n",
    "            # identify the predicted class for each example in the batch\n",
    "            probabilities, predicted = torch.max(model_outputs.cpu().data, 1)\n",
    "            # put all the true labels and predictions to two lists\n",
    "            all_pred.extend(predicted)\n",
    "            all_label.extend(batch_output)\n",
    "            \n",
    "    accuracy = accuracy_score(all_label, all_pred)\n",
    "    f1score = f1_score(all_label, all_pred, average='macro') \n",
    "    return accuracy,f1score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.9 Please read [Pytorch documentation](https://pytorch.org/tutorials/beginner/saving_loading_models.html) and write code to load the trained model checkpoint from directory: `./ckpt/model_24.pt`.\n",
    "\n",
    "Note this ckpt is a dictionary which includes three keys: epoch, model_state_dict, and optimizer_state_dict.\n",
    "The model parameters are the values of \"model_state_dictm\".\n",
    "\n",
    "\n",
    "\n",
    "rubric={accuracy:2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.10 Write code to evaluate the trained model on test set and report the test accuracy and F-score. \n",
    "Report the performance of the trained model on the test set is XX.XXX% on accuracy (fill in your accuracy).\n",
    "\n",
    "Report the performance of the trained model on the test set is XX.XXX on F1_score (fill in your F1_score).\n",
    "\n",
    "rubric={accuracy:2,quality:1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Short Answer Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Non-Linearity Review\n",
    "rubric={reasoning:3}\n",
    "\n",
    "There are a number of different non-linearities that can be used in our neural networks, for instance: **Sigmoid, tanh, and ReLU**. Different variations and tweaks to these non-linearities get introduced fairly frequently, and it pays to have a sense of why you might pick one non-linearity for your network instead of another. **Explain how these three (Sigmoid, tanh, and ReLU) are different and what might be some pros and cons of using them in a neural network**. (There are some great blog/quora posts talking about this topic, but if you are going to be summarizing from a post please include a link to the resource).\n",
    "\n",
    "\n",
    "Hint: Your answer should be a maximum of 2-3 paragraphs. Short answers are just fine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 ReLU Variations\n",
    "rubric={reasoning:1}\n",
    "\n",
    "PyTorch supports several other non-linearities, find two variations on ReLU that PyTorch implements (see https://pytorch.org/docs/stable/nn.html) and explain how they are different from standard ReLU.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write your answer here.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Wait why are we using Logs?\n",
    "rubric={reasoning:1}\n",
    "\n",
    "What is the purpose of taking logs of probabilities, as in the case of NLLLoss and LogSoftmax?\n",
    "\n",
    "Hint: A short answer is just fine.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Softmax\n",
    "rubric={reasoning:2}\n",
    "\n",
    "For a multiclass problem with a Softmax layer, give an example of a hypothetical Softmax output with 3 classes (hint, think back to Lab1 for what that output might look like). Generally, which of the three classes in your example should your classifier pick? Why might we care about values corresponding to more than one of these classes?\n",
    "\n",
    "Hint: Sometimes knowing what the top n most likely classes is of interest. Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 RNNs\n",
    "rubric={reasoning:1}\n",
    "\n",
    "RNNs are quite powerful for dealing with certain types of data (such as sequences), however, they have some drawbacks. What are some of these drawbacks? (List two drawbacks)\n",
    "\n",
    "Hint: Look at RNN slides, including the last few slides.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 LSTMs vs RNNs\n",
    "\n",
    "rubric={reasoning:2}\n",
    "\n",
    "LSTMs help alleviate one of the issues with RNNs (see 2.5). What do they alleviate? Do you see any problems that the model might still have (just reason generally about the model based on your understanding).\n",
    "\n",
    "Hint: Short answer is good.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write your answer here."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
