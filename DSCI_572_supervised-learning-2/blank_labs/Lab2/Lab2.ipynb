{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework 2 - Supervised Learning II - MDS Computational Linguistics\n",
    "\n",
    "### Assignment Topics\n",
    "- More linearities, non-linearities\n",
    "- Embedding layer\n",
    "- Neural Network for sentiment analysis\n",
    "- Very-short answer questions\n",
    "\n",
    "### Software Requirements\n",
    "- Python (>=3.6)\n",
    "- PyTorch (>=1.2.0) \n",
    "- Jupyter (latest)\n",
    "\n",
    "### Submission Info.\n",
    "- Due Date: 1/23/21 12:59pm Pacific Time\n",
    "\n",
    "## Getting Started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all necessary imports\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# set the seed (allows reproducibility of the results)\n",
    "manual_seed = 572\n",
    "torch.manual_seed(manual_seed) # allows us to reproduce results when using random generation on the cpu\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # creates the device object, either GPU (cuda) or CPU\n",
    "torch.backends.cudnn.deterministic=True\n",
    "torch.set_deterministic(True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tidy Submission\n",
    "\n",
    "rubric={mechanics:1}\n",
    "\n",
    "To get the marks for tidy submission:\n",
    "- Submit the assignment by filling in this jupyter notebook with your answers embedded\n",
    "- Be sure to follow the [general lab instructions](https://ubc-mds.github.io/resources_pages/general_lab_instructions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T2 Additional NN practice\n",
    "rubric={accuracy:2, quality:1}\n",
    "\n",
    "Some Pytorch models expect certain dimensions of input. This can be tricky if they get mixed in with other model that expect something different (e.g. CNNs with RNNs say). Let's look at dealing with this: In the model below complete the forward pass by ensuring that the model passes the data correctly through the following CNN -> ReLU -> RNN -> Linear Layer -> Softmax.\n",
    "\n",
    "Double check the Pytorch documentation to find the input/output dimensions for each of these modules! https://pytorch.org/docs/stable/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assume (Batch,Length,Embeddings)\n",
    "x = torch.rand((7,10,20))\n",
    "\n",
    "class DimensionTestModel(nn.Module):\n",
    "  \n",
    "  def __init__(self, input_size, filters, hidden, output_size):\n",
    "    super(DimensionTestModel, self).__init__()\n",
    "    self.cnn = nn.Conv1d(input_size, filters, kernel_size=3, padding =1) # we'll look more at CNNs a little next lab and COLX 585\n",
    "    self.activation = nn.ReLU()\n",
    "    self.rnn = nn.RNN(filters, hidden)  # RNNs in detail in lecture\n",
    "    self.linear_layer = nn.Linear(hidden, output_size) \n",
    "    self.softmax_layer = nn.LogSoftmax(dim=1)\n",
    "  \n",
    "  def forward(self, x):\n",
    "    # Your Code Here\n",
    "    \n",
    "    return \n",
    "\n",
    "model = DimensionTestModel(20,5,11,9)\n",
    "\n",
    "print(model(x).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1 Initializing Weights\n",
    "\n",
    "Below is a linear layer with the weights printed out.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer_1 weight:\n",
      " tensor([[ 0.2852,  0.0739,  0.3815, -0.2404,  0.4404],\n",
      "        [ 0.0549, -0.4405,  0.2186,  0.2818, -0.3965],\n",
      "        [-0.2733,  0.3812,  0.1627, -0.0800,  0.1522],\n",
      "        [-0.3782, -0.4295,  0.1811,  0.1469, -0.2809]])\n"
     ]
    }
   ],
   "source": [
    "layer_1 = torch.nn.Linear(5, 4)\n",
    "print(\"layer_1 weight:\\n\", layer_1.weight.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 1.1 Initialized distribution\n",
    "rubric={reasoning:1}\n",
    "\n",
    "From what distribution (and range) the default values in ``layer_1.weight.data`` are sampled from? \n",
    "(Hint: You can look at the [documentation](https://pytorch.org/docs/stable/nn.html#torch.nn.Linear) and/or [source code](https://pytorch.org/docs/stable/_modules/torch/nn/modules/linear.html#Linear) of ``nn.Linear``. And the symbols  $\\textit{U}$ and $\\mathcal{N}$ correspond to uniform and normal (Gaussian) distribution.)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer here**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Reproducibility\n",
    "rubric={reasoning:1}\n",
    "\n",
    "What happens when you add torch.manual_seed(manual_seed) before the layer is created in the above code? Run it a few times with and without."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**your answer goes here:**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Parameter Initialization\n",
    "rubric={accuracy:1}\n",
    "\n",
    "Write code to initialize parameters (weights and biases) in ``layer_1`` (previous queston, 1.1) with numbers sampled randomly from standard normal distribution (mean 0 and variance 1).\n",
    "\n",
    "\n",
    "Hint: Look at ``torch.randn`` function\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Embedding layer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get some intuition for how embedding layers work, let's just run through the expected results from passing some input into an embedding model.\n",
    "\n",
    "Below is some code to do this (so the ouput should be your expected answer). For this problem, show how you'll get the expected output by just writing out the math that the model performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings_avg:\n",
      " tensor([[  2.0000,   1.0000,   1.5000],\n",
      "        [  5.5000,  10.5000,  15.5000]])\n"
     ]
    }
   ],
   "source": [
    "# the model\n",
    "embedding_model = nn.Embedding(4, 3) # 4 embeddings with each of three dimensions\n",
    "\n",
    "# set the weights for each of the four embedding\n",
    "embedding_model.weight.data = torch.tensor([[1., 2., 3.], [1., 1., 1.], [3., 0., 0.], [10., 20., 30.]])\n",
    "\n",
    "# data (2 examples each with two inputs)\n",
    "inputs = torch.tensor([[0, 2], [1, 3]])\n",
    "\n",
    "# forward propagation for computing average of input embeddings\n",
    "embeddings_out = embedding_model(inputs)\n",
    "embeddings_avg = embeddings_out.mean(1)\n",
    "print(\"embeddings_avg:\\n\", embeddings_avg.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Computing Embeddings\n",
    "rubric={accuracy:1}\n",
    "\n",
    "Compute the values in **embeddings_avg** by hand. Show your work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**your answer goes here:**  You can write \\\\$ **Math Equations Here** \\\\$ to use math formating\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 EmbeddingBag\n",
    "\n",
    "Reimplement the Embedding model above by using [``nn.EmbeddingBag``](https://pytorch.org/docs/stable/nn.html#torch.nn.EmbeddingBag) instead of ``nn.Embedding`` by setting the appropriate mode?  \n",
    "\n",
    "Hint: You can look at the [documentation](https://pytorch.org/docs/stable/nn.html#torch.nn.EmbeddingBag) and/or [source code](https://pytorch.org/docs/stable/_modules/torch/nn/modules/sparse.html#EmbeddingBag) of ``nn.EmbeddingBag``\n",
    "\n",
    "rubric={accuracy:2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Word to Index Map\n",
    "rubric={accuracy:2}\n",
    "\n",
    "In the tutorial, we constructed word to index mapping for a one sentence corpus. Write code to build word to index mapping for this toy corpus containing three sentences.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Toy corpus worth 3 sentences (each row excluding the header corresponds to a sentence)\n",
    "\n",
    "|  sentence no. | sentence text |\n",
    "| --------------- | ------------------------------ |\n",
    "| 1  | UBC’s Master of Data Science in Computational Linguistics is the credential to set you apart. |\n",
    "| 2  | Offered at the Vancouver campus, this unique degree is tailored to those with a passion for language and data.|\n",
    "| 3  | Over 10 months, the program combines foundational data science courses with advanced computational linguistics courses—equipping graduates with the skills to turn language-related data into knowledge and to build AI that can interpret human language. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_list = [\"UBC’s Master of Data Science in Computational Linguistics is the credential to set you apart.\",\n",
    "            \"Offered at the Vancouver campus, this unique degree is tailored to those with a passion for language and data.\",\n",
    "            \"Over 10 months, the program combines foundational data science courses with advanced computational linguistics courses—equipping graduates with the skills to turn language-related data into knowledge and to build AI that can interpret human language.\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Building Training Data for CBOW\n",
    "rubric={accuracy:2}  \n",
    "In the tutorial, we constructed the train data (input and output for each training example) for a one sentence corpus. Write code that outputs the train data for CBOW model created from this toy corpus and prints the number of training examples. \n",
    "\n",
    "Note:\n",
    "- Assume the **window size to be 3**. \n",
    "- Use **truecase** of the words. \n",
    "- Use [white space tokenizer](https://kite.com/python/docs/nltk.WhitespaceTokenizer) to get the words from each sentence and no further preprocessing. \n",
    "- A training example is generated from a sentence and doesn't span across multiple sentences. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "## Hint: create a custom Dataset class to process the data and then use Dataloader.\n",
    "class CBOWDataset(Dataset):\n",
    "    def __init__(self, window_size=3):\n",
    "        #to complete\n",
    "            \n",
    "    def __getitem__(self, index):\n",
    "        #to complete\n",
    "        \n",
    "    def __len__(self):\n",
    "        #to complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CBOWDataset(window_size = 3)\n",
    "print(\"number of samples in the dataset:\", dataset.n)\n",
    "print(\"feature matrix:\", dataset.features)\n",
    "print(\"label matrix:\", dataset.labels)\n",
    "\n",
    "\n",
    "#to complete\n",
    "#create the batch to load the dataset into a dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Build CBOW\n",
    "rubric={accuracy:3, quality:2}\n",
    "\n",
    "Write code that\n",
    "- defines the CBOW model\n",
    "- train the CBOW model (update its parameters) with all the training examples (use SGD)\n",
    "- prints the word embedding for one of the word involved in the training before and after training\n",
    "- assumes the hyperparameters: EMBEDDING_SIZE to 3, LEARNING_RATE to 0.5, WINDOW_SIZE to 3 and MAX_EPOCHS to 1.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CBOWmodel(nn.Module):\n",
    "  \n",
    "  def __init__(self, embedding_size, vocab_size, output_size):\n",
    "    #to complete\n",
    "  \n",
    "  def forward(self, x):\n",
    "    #to complete\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameter of CBOW model\n",
    "EMBEDDING_SIZE = 3 # size of the word embedding\n",
    "LEARNING_RATE = 0.5 # learning rate of gradient descent\n",
    "WINDOW_SIZE = 3  # number of words to be considerd before (or after) the target word for making the context\n",
    "MAX_EPOCHS = 1 # number of passes over the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the code to instantiate your model, loss function, and optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# instantiate model  (often nice to also print out the model here to see that all the layers are correct)  \n",
    "\n",
    "# define the loss function\n",
    "\n",
    "# create an instance of SGD with required hyperparameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train logic (similar to that of linear regression model)\n",
    "def train(loader):\n",
    "    total_loss = 0.0\n",
    "    # iterate throught the data loader\n",
    "    num_batches = 0\n",
    "    for batch in loader:\n",
    "    #TO COMPLETE: (Use the comments to guide your code)\n",
    "        \n",
    "        # load the current batch\n",
    "\n",
    "        \n",
    "        # forward propagation:\n",
    "        # pass the data through the model\n",
    "        \n",
    "        # compute the loss\n",
    "\n",
    "\n",
    "        # backward propagation (compute the gradients and update the model):\n",
    "        # clear the buffer\n",
    "        \n",
    "        # compute the gradients\n",
    "        \n",
    "        # update the weights\n",
    "\n",
    "        num_batches += 1\n",
    "    return total_loss/num_batches\n",
    "\n",
    "# evaluation logic based on classification accuracy\n",
    "def evaluate(loader):\n",
    "    accuracy, num_examples = 0.0, 0\n",
    "    with torch.no_grad(): # impacts the autograd engine and deactivate it. reduces memory usage and speeds up computation\n",
    "        for batch in loader:\n",
    "        #TO COMPLETE (USE THE COMMENTS TO GUIDE YOUR CODE):\n",
    "            # load the current batch\n",
    "\n",
    "            # forward propagation:\n",
    "            # pass the data through the model\n",
    "\n",
    "            # identify the predicted class for each example in the batch (row-wise)\n",
    "\n",
    "            # compare with batch_output (gold labels) to compute accuracy\n",
    "\n",
    "            num_examples += batch_output.size(0)\n",
    "    return accuracy/num_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start the training\n",
    "for epoch in range(MAX_EPOCHS):\n",
    "  # train the model for one pass over the data\n",
    "  train_loss = train(train_loader)   \n",
    "  # print the loss for every epoch\n",
    "  print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, MAX_EPOCHS, train_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 How many learnable (or updatable) parameters are present in the model defined in 2.5. Compute the result by writing code or by hand.\n",
    "rubric={accuracy:2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here (if you are writing code, use this block. Otherwise, change this block to a Markdown block.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Neural Network for Sentiment Analysis\n",
    "\n",
    "\n",
    "The multilayer neural network code used in our tutorial is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiLayerNeuralNetworkModel(\n",
      "  (model): Sequential(\n",
      "    (0): Linear(in_features=5000, out_features=50, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=50, out_features=3, bias=True)\n",
      "    (5): LogSoftmax()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# all the necessary imports\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "\n",
    "# set the seed\n",
    "torch.manual_seed(manual_seed)\n",
    "\n",
    "\n",
    "# hyperparameters\n",
    "BATCH_SIZE = 5\n",
    "MAX_EPOCHS = 15\n",
    "LEARNING_RATE = 0.5\n",
    "MAX_FEATURES = 5000 # x_j, the number of j\n",
    "NUM_CLASSES = 3\n",
    "\n",
    "# dataset\n",
    "DATA_FOLDER = \"data/sentiment-twitter-2016-task4\"\n",
    "TRAIN_FILE = DATA_FOLDER + \"/train.tsv\"\n",
    "VALID_FILE = DATA_FOLDER + \"/dev.tsv\"\n",
    "TEST_FILE = DATA_FOLDER + \"/test.tsv\"\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# function for reading tsv file\n",
    "def read_corpus(file):\n",
    "    corpus = [] \n",
    "    for line in open(file):\n",
    "        content, label = line.strip().split(\"\\t\") # first column is tweet, second column is golden label.\n",
    "        corpus.append(content)\n",
    "    return corpus\n",
    "\n",
    "# reads the train corpus\n",
    "train_corpus = read_corpus(TRAIN_FILE)  # get a list of tweets\n",
    "\n",
    "# define the vectorizer\n",
    "# builds a vocabulary that only considering the top max_features ordered by term frequency across the corpus.\n",
    "vectorizer = TfidfVectorizer(max_features=MAX_FEATURES)\n",
    "\n",
    "# fit the vectorizer on train set\n",
    "vectorizer.fit(train_corpus)\n",
    "\n",
    "# create a new class inheriting torch.utils.data.Dataset\n",
    "class TweetSentimentDataset(Dataset):\n",
    "  \"\"\" sentiment-twitter-2016-task4 dataset.\"\"\"\n",
    "  def __init__(self, file, vectorizer):\n",
    "    # read the corpus\n",
    "    corpus, labels = [], []\n",
    "    for line in open(file):\n",
    "        content, label = line.strip().split(\"\\t\")\n",
    "        corpus.append(content)\n",
    "        labels.append(int(label))\n",
    "    \n",
    "    # set the size of the corpus\n",
    "    self.n = len(corpus)\n",
    "    \n",
    "    # vectorize all the tweets\n",
    "    features = vectorizer.transform(corpus)\n",
    "    \n",
    "    # convert features and labels to torch.tensor\n",
    "    self.features = torch.from_numpy(features.toarray()).float()\n",
    "    self.features.to(device)\n",
    "    self.labels = torch.tensor(labels, device=device, requires_grad=False)\n",
    "    \n",
    "  # return input and output of a single example\n",
    "  # Input: Feature vectors, where each vector corresponds to a tweet. \n",
    "  # Output: Labels, where each label is one index for each of our tags in the set {positive, negative, neutral}\n",
    "  def __getitem__(self, index):\n",
    "    return self.features[index], self.labels[index]\n",
    "  \n",
    "  # return the total number of examples\n",
    "  def __len__(self):\n",
    "    return self.n\n",
    "\n",
    "# create the dataloader object\n",
    "train_loader = DataLoader(dataset=TweetSentimentDataset(TRAIN_FILE, vectorizer), batch_size=BATCH_SIZE, shuffle=True, num_workers=2) \n",
    "valid_loader = DataLoader(dataset=TweetSentimentDataset(VALID_FILE, vectorizer), batch_size=BATCH_SIZE, shuffle=False, num_workers=1)\n",
    "test_loader = DataLoader(dataset=TweetSentimentDataset(TEST_FILE, vectorizer), batch_size=BATCH_SIZE, shuffle=False, num_workers=1) \n",
    "\n",
    "# train logic (similar to that of linear regression model)\n",
    "def train(loader):\n",
    "    total_loss = 0.0\n",
    "  # iterate throught the data loader\n",
    "    num_batches = 0\n",
    "    for batch in loader:\n",
    "    # load the current batch\n",
    "        batch_input, batch_output = batch\n",
    "    \n",
    "    # forward propagation\n",
    "    # pass the data through the model\n",
    "        model_outputs = model(batch_input)\n",
    "    # compute the loss\n",
    "        cur_loss = criterion(model_outputs, batch_output)\n",
    "        total_loss += cur_loss.item()\n",
    "    \n",
    "    # backward propagation (compute the gradients and update the model)\n",
    "    # clear the buffer\n",
    "        optimizer.zero_grad()\n",
    "    # compute the gradients\n",
    "        cur_loss.backward()\n",
    "    # update the weights\n",
    "        optimizer.step()\n",
    "    \n",
    "        num_batches += 1\n",
    "    return total_loss/num_batches\n",
    "\n",
    "# evaluation logic based on classification accuracy\n",
    "def evaluate(loader):\n",
    "    accuracy, num_examples = 0.0, 0\n",
    "    with torch.no_grad(): # impacts the autograd engine and deactivate it. reduces memory usage and speeds up computation\n",
    "        for batch in loader:\n",
    "        # load the current batch\n",
    "            batch_input, batch_output = batch\n",
    "              # forward propagation\n",
    "              # pass the data through the model\n",
    "            model_outputs = model(batch_input)\n",
    "              # identify the predicted class for each example in the batch\n",
    "            _, predicted = torch.max(model_outputs.data, 1)\n",
    "              # compare with batch_output (gold labels) to compute accuracy\n",
    "            accuracy += (predicted == batch_output).sum().item()\n",
    "            num_examples += batch_output.size(0)\n",
    "    return accuracy/num_examples\n",
    "\n",
    "\"\"\"\n",
    "create a custom model class inheriting torch.nn.Module\n",
    "\"\"\"\n",
    "class MultiLayerNeuralNetworkModel(nn.Module):\n",
    "  \n",
    "  def __init__(self, num_inputs, hidden_layers, num_outputs):\n",
    "    # In the constructor we define the layers for our model\n",
    "    super(MultiLayerNeuralNetworkModel, self).__init__()\n",
    "    \n",
    "    modules = [] # stores all the layers for the neural network\n",
    "    input_dim = num_inputs\n",
    "    # add input layer followed by hidden layers (excluding the classification module)\n",
    "    for hidden_layer in hidden_layers:\n",
    "      # add one layer followed by non-linearity (nn.Sigmoid)\n",
    "        modules.append(nn.Linear(input_dim, hidden_layer))\n",
    "        modules.append(nn.Sigmoid())\n",
    "        input_dim = hidden_layer\n",
    "    # add the classification module\n",
    "    modules.append(nn.Linear(input_dim, num_outputs))\n",
    "    modules.append(nn.LogSoftmax(dim=1))\n",
    "    \n",
    "    # create the model from all the modules\n",
    "    self.model = nn.Sequential(*modules) # container of layers, for more details: https://pytorch.org/docs/stable/nn.html#torch.nn.Sequential\n",
    "  \n",
    "  def forward(self, x):\n",
    "    # In the forward function we define the forward propagation logic\n",
    "    out = self.model(x)\n",
    "    return out\n",
    "\n",
    "# hyperparameter of neural network\n",
    "hidden_layers = [50, 50]  # [num. of hidden units in first layer, num. of hidden units in second layer]\n",
    "\n",
    "# define the loss function (last node of the graph)\n",
    "model = MultiLayerNeuralNetworkModel(MAX_FEATURES, hidden_layers, NUM_CLASSES)\n",
    "model.to(device)\n",
    "print(model)\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "# create an instance of SGD with required hyperparameters\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/15], Loss: 0.9924, Training Accuracy: 0.5157, Validation Accuracy: 0.4217\n",
      "Epoch [2/15], Loss: 0.9652, Training Accuracy: 0.5517, Validation Accuracy: 0.4487\n",
      "Epoch [3/15], Loss: 0.8733, Training Accuracy: 0.6648, Validation Accuracy: 0.5143\n",
      "Epoch [4/15], Loss: 0.8072, Training Accuracy: 0.5840, Validation Accuracy: 0.4507\n",
      "Epoch [5/15], Loss: 0.7601, Training Accuracy: 0.7340, Validation Accuracy: 0.5033\n",
      "Epoch [6/15], Loss: 0.7045, Training Accuracy: 0.7255, Validation Accuracy: 0.4907\n",
      "Epoch [7/15], Loss: 0.6577, Training Accuracy: 0.8053, Validation Accuracy: 0.5048\n",
      "Epoch [8/15], Loss: 0.6108, Training Accuracy: 0.8378, Validation Accuracy: 0.4887\n",
      "Epoch [9/15], Loss: 0.5653, Training Accuracy: 0.7482, Validation Accuracy: 0.4462\n",
      "Epoch [10/15], Loss: 0.5189, Training Accuracy: 0.8258, Validation Accuracy: 0.4867\n",
      "Epoch [11/15], Loss: 0.4812, Training Accuracy: 0.7813, Validation Accuracy: 0.4462\n",
      "Epoch [12/15], Loss: 0.4474, Training Accuracy: 0.8922, Validation Accuracy: 0.4977\n",
      "Epoch [13/15], Loss: 0.4028, Training Accuracy: 0.9175, Validation Accuracy: 0.4707\n",
      "Epoch [14/15], Loss: 0.3721, Training Accuracy: 0.8765, Validation Accuracy: 0.4867\n",
      "Epoch [15/15], Loss: 0.3323, Training Accuracy: 0.7765, Validation Accuracy: 0.4652\n"
     ]
    }
   ],
   "source": [
    "# start the training\n",
    "for epoch in range(MAX_EPOCHS):\n",
    "  # train the model for one pass over the data\n",
    "  train_loss = train(train_loader)\n",
    "  # compute the training accuracy\n",
    "  train_acc = evaluate(train_loader)\n",
    "  # compute the validation accuracy \n",
    "  val_acc = evaluate(valid_loader)\n",
    "  # print the loss for every epoch\n",
    "  print('Epoch [{}/{}], Loss: {:.4f}, Training Accuracy: {:.4f}, Validation Accuracy: {:.4f}'.format(epoch+1, MAX_EPOCHS, train_loss, train_acc, val_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Bigram and Trigrams\n",
    "rubric={accuracy:1}  \n",
    "In the original tutorial, we considered only *unigrams* as features to represent a tweet. Change the original tutorial code to consider *bigrams, trigrams* (along with unigrams, which is considered by default) as features to represent a tweet.\n",
    "Hints: \n",
    "- Look at the documentation of [``TfidfVectorizer``](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) to see how to incorporate bigrams, trigrams and so on.\n",
    "- Modify the line ``vectorizer = TfidfVectorizer(max_features=MAX_FEATURES)`` and keep the rest of the code intact.\n",
    "\n",
    "### **Hand in the**\n",
    "- Accuracy on the validation set after training\n",
    "- Python code (ONLY the changed lines)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Report performance of your model here:** (double-click to edit)\n",
    "\n",
    "**3.1.1 Performance of my neural network model on the validation set after training is** (fill in your accuracy)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.1.2 Your code:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here (put only the changed lines)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2  In the original tutorial, we considered only two hidden layers with 50 units each. Change the original tutorial code to consider *five hidden layers* with *100 dimensions each*.\n",
    "rubric={accuracy:1}  \n",
    "Hints: \n",
    "- Modify the line ``hidden_layers = [50, 50]`` and keep the rest of the code intact.\n",
    "\n",
    "### **Hand in the**\n",
    "- Accuracy on the validation set after training\n",
    "- Python code (ONLY the changed lines)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Report performance of your model here:** (double-click to edit)\n",
    "\n",
    "**3.2.1  Performance of my neural network model on the validation set after training is**  (fill in your accuracy)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.2.2 Your code:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here (put only the changed lines)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 In the original tutorial, we used [*Sigmoid*](https://pytorch.org/docs/stable/nn.functional.html#torch.nn.functional.sigmoid) as the activation function. Change the original tutorial code to consider other nonlinearities such as [*ReLU*](https://pytorch.org/docs/stable/nn.functional.html#torch.nn.functional.relu) and [*Tanh*](https://pytorch.org/docs/stable/nn.functional.html#torch.nn.functional.tanh) and report the nonlinearity that gives the best validation performance.\n",
    "rubric={accuracy:1}  \n",
    "Hints: \n",
    "- Modify the line ``modules.append(nn.Sigmoid())`` and keep the rest of the code intact.\n",
    "\n",
    "### **Hand in the**\n",
    "- Nonlinearity function that gives the best validation performance\n",
    "- Accuracy on the validation set after training with the best nonlinearity\n",
    "- Python code (ONLY the changed lines)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Report your results here:** (double-click to edit)\n",
    "\n",
    "**3.3.1. Report the nonlinearity function that gives the best validation performance:**  \n",
    "\n",
    "**3.3.2. Report performance of my neural network model on the validation set after training is:**  accuracy (fill in your accuracy)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.3.3 Your code:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here (put only the changed lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 In the original tutorial, we used *learning rate of 0.5*. Change the original tutorial code by trying out different learning rates preferably *between 0.0001 and 1*. Report the learning rate that gives the best validation performance.\n",
    "rubric={accuracy:1}  \n",
    "Hints: \n",
    "- Modify the line ``LEARNING_RATE = 0.5`` and keep the rest of the code intact.\n",
    "\n",
    "### **Hand in the**\n",
    "- Learning rate that gives the best validation performance\n",
    "- Accuracy on the validation set after training with the best learning rate\n",
    "- Python code (ONLY the changed lines)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Report your results here:** (double-click to edit)\n",
    "\n",
    "**3.4.1. Report the learning rate that gives the best validation performance:**  \n",
    "**3.4.2. Report the performance of my neural network model on the validation set after training is :** (fill in your accuracy)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.4.3 Your code:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This answer depends on what learning rates you tried. You should be getting ~52%ish"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: Very-Short answer questions\n",
    "\n",
    "(Double-click each question block and place your answer at the end of the question) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Can we build deep neural network without any nonlinearity layers? What's the nature of such deep neural network without any nonlinearity layers?\n",
    "rubric={reasoning:1}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 What is the distinction between single layer neural network and multilayer neural network?\n",
    "rubric={reasoning:1}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 What is the difference between hidden layer and hidden unit (or neuron)?\n",
    "rubric={reasoning:1}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 What is the gradient of sigmoid function over a scalar ($\\nabla \\sigma(a)$)? What is the interesting property of this gradient?\n",
    "rubric={reasoning:1}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
