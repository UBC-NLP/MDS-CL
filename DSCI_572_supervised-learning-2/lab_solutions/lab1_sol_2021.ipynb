{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework 1 - Supervised Learning II - MDS Computational Linguistics\n",
    "\n",
    "### Assignment Topics\n",
    "- Introduction to Neural Nets, (Feed Forward-NNs / Multi-layer Perceptrons)\n",
    "- Neural Net Hyperparameter Tuning\n",
    "- Operations on tensor\n",
    "- Linearities, non-linearities and loss functions \n",
    "- Very-short answer questions\n",
    "\n",
    "### Software Requirements\n",
    "- Python (>=3.6)\n",
    "- PyTorch (>=1.2.0) \n",
    "- Jupyter (latest)\n",
    "- Scikit Learn (>=0.23.2)\n",
    "- Skorch (>=0.9)\n",
    "\n",
    "### Submission Info. \n",
    "- Due Date: 1/16/21 6pm (Pacific Time)\n",
    "\n",
    "## Getting Started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# all necessary imports\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import sklearn\n",
    "\n",
    "# set the seed (allows reproducibility of the results)\n",
    "manual_seed = 572\n",
    "torch.manual_seed(manual_seed) # allows us to reproduce results when using random generation on the cpu\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # checks if GPU is there in this system and automatically uses GPU if its available, otherwise uses CPU.\n",
    "torch.backends.cudnn.deterministic=True\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tidy Submission\n",
    "\n",
    "rubric={mechanics:1}\n",
    "\n",
    "To get the marks for tidy submission:\n",
    "- Submit the assignment by filling in this jupyter notebook with your answers embedded\n",
    "- Make sure code that is randomly initialized is set correctly with the manual_seed=572\n",
    "- Be sure to follow the [general lab instructions](https://ubc-mds.github.io/resources_pages/general_lab_instructions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise T1: Simple Neural Net Optimization\n",
    "\n",
    "For this group assignment we will be applying a simple 2-layer Feed Forward Neural Net classifier for a classification task. we'll use a newsgroup data set, which consists of the words in posts from a set of different forums, with the post data represented in Tfidf format, can our classifiers predict which forum each post came from? In essense this is a quick recap of 571, but applying it to Neural Networks, which end up having much more hyperparameters to deal with than the algorithms you encountered in 571. For your sanity, we will limit how many different trials we expect you to run in each section. \n",
    "\n",
    "Our comparison will be a SVM classifier with a simple linear kernel.\n",
    "\n",
    "### Data Loading "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups_vectorized \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_data = fetch_20newsgroups_vectorized(subset='train', remove=('headers', 'footers', 'quotes'))\n",
    "test_data =fetch_20newsgroups_vectorized(subset='test', remove=('headers', 'footers', 'quotes'))\n",
    "\n",
    "X_train = train_data.data\n",
    "y_train = train_data.target\n",
    "X_test = test_data.data\n",
    "y_test = test_data.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM Baseline\n",
    "\n",
    "Let's run a quick test to see how well an SVM does on this dataset (might take a couple minutes):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5624004248539565\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn import svm\n",
    "\n",
    "clf = svm.SVC(kernel='linear') #Use a linear kernel with default regularization for simplicity\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "pred = clf.predict(X_test)\n",
    "\n",
    "acc = metrics.accuracy_score(y_test, pred)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like SVMs and other ML classifiers, neural nets have flexibility in their configuration, the so called hyperparameters that control how the network is structured and how it learns. And like SVMs it turns out that this tuning is extremely important for appropriate performance.\n",
    "\n",
    "### Example 2 Layer NN\n",
    "\n",
    "We'll use Skorch (see https://github.com/skorch-dev/skorch for conda install instructions) to allow us to use Sklearn alongside Pytorch. Note: in later assignments in this course we'll just focus on Pytorch directly, but it's always a good tool to have in your back pocket (if you want to use say Sklearn's cross validation tools)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m3.0029\u001b[0m       \u001b[32m0.0486\u001b[0m        \u001b[35m3.0022\u001b[0m  5.8500\n",
      "      2        \u001b[36m3.0018\u001b[0m       0.0486        \u001b[35m3.0012\u001b[0m  5.2916\n",
      "      3        \u001b[36m3.0007\u001b[0m       0.0486        \u001b[35m3.0002\u001b[0m  4.5647\n",
      "      4        \u001b[36m2.9998\u001b[0m       0.0486        \u001b[35m2.9993\u001b[0m  4.6539\n",
      "      5        \u001b[36m2.9990\u001b[0m       0.0486        \u001b[35m2.9986\u001b[0m  4.7641\n",
      "      6        \u001b[36m2.9982\u001b[0m       0.0486        \u001b[35m2.9978\u001b[0m  4.5727\n",
      "      7        \u001b[36m2.9975\u001b[0m       0.0486        \u001b[35m2.9972\u001b[0m  4.6661\n",
      "      8        \u001b[36m2.9969\u001b[0m       0.0486        \u001b[35m2.9966\u001b[0m  4.6782\n",
      "      9        \u001b[36m2.9963\u001b[0m       0.0486        \u001b[35m2.9960\u001b[0m  4.6634\n",
      "     10        \u001b[36m2.9957\u001b[0m       \u001b[32m0.0499\u001b[0m        \u001b[35m2.9955\u001b[0m  4.7008\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<class 'skorch.classifier.NeuralNetClassifier'>[initialized](\n",
       "  module_=TwoLayerNN(\n",
       "    (dense0): Linear(in_features=101631, out_features=20, bias=True)\n",
       "    (nonlin): ReLU()\n",
       "    (dropout): Dropout(p=0.1)\n",
       "    (dense1): Linear(in_features=20, out_features=20, bias=True)\n",
       "    (output): Linear(in_features=20, out_features=20, bias=True)\n",
       "    (softmax): Softmax()\n",
       "  ),\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from skorch import NeuralNetClassifier\n",
    "\n",
    "\n",
    "##Define our NN\n",
    "class TwoLayerNN(nn.Module): #feel free to change the inputs it takes\n",
    "    def __init__(self, input_dim, hidden_units=20, output_classes=20, nonlin=nn.ReLU()):  \n",
    "        super(TwoLayerNN, self).__init__()\n",
    "        self.dense0 = nn.Linear(input_dim, hidden_units)\n",
    "        self.nonlin = nonlin\n",
    "        self.dropout = nn.Dropout(p=.1)\n",
    "        self.dense1 = nn.Linear(hidden_units, hidden_units)\n",
    "        self.output = nn.Linear(hidden_units, output_classes)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, X, **kwargs):\n",
    "        X = self.nonlin(self.dense0(X))\n",
    "        X = self.dropout(X)\n",
    "        X = self.nonlin(self.dense1(X))\n",
    "        X = self.softmax(self.output(X))\n",
    "        return X\n",
    "\n",
    "\n",
    "#Make sure you run these each time before you (re)initialize the network!!! \n",
    "torch.manual_seed(manual_seed)\n",
    "np.random.seed(manual_seed)\n",
    "torch.cuda.manual_seed(manual_seed)\n",
    "\n",
    "net = NeuralNetClassifier(\n",
    "    TwoLayerNN(input_dim=X_train.shape[1]),\n",
    "    max_epochs=10,\n",
    "    lr=0.01,\n",
    "    optimizer=torch.optim.SGD,   \n",
    "    optimizer__weight_decay=0.001,  #roughly equivalent to L2 regularization\n",
    "    iterator_train__shuffle=True,\n",
    "    device=device  #'cpu' or 'cuda'\n",
    ")\n",
    "\n",
    "net.fit(torch.from_numpy(X_train.todense()).float(), torch.tensor(y_train,dtype=torch.long))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05018587360594796\n"
     ]
    }
   ],
   "source": [
    "#when you're ready to run on TEST:\n",
    "pred = net.predict(torch.from_numpy(X_test.todense()).float())\n",
    "acc = metrics.accuracy_score(y_test, pred)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not great! Let's look at improving our network.\n",
    "\n",
    "### Hyperparameter Optimization\n",
    "\n",
    "It turns out there are quite a few choices that we can make to optimize our NN: the architecture of our net (number of hidden layers, number of units in each layer, choice of nonlinearity (sigmoid, ReLU etc.)) being a major factor. We also care about our choice of training procedure such as which optimizer (SGD, Adam etc.) learning rate and number of epochs. Last but not least regularization is also important (L2 regularization can be thought of as weight decay in Pytorch).   \n",
    "\n",
    "*NOTE 1*: One thing we won't touch yet, but is extremely important for good optimization is how model weights are initialized, we'll talk about this later.  \n",
    "\n",
    "*NOTE 2*: Keep it to 2 layers, you'll be doing a multilayer model in question 2 (there are tricks to make it easier to implement).\n",
    "\n",
    "#### Brief recap of these concepts\n",
    "  \n",
    "*Learning Rate*: Multiplication factor to set how far your model parameters move per each update.    \n",
    "*Optimizer*: Which algorithm is used to update the model parameters (Stochastic Gradient Descent, Adam, Adadelta etc. we'll talk about these later).  \n",
    "*Epoch*: A number/count to represent times that the model has cycled through the entire training set during training.  \n",
    "*Nonlinearity*: A non-linear function that allows for neural nets to 'learn' to solve non-linearly seperable problems. Common ones include sigmoid, tanh, ReLU.  \n",
    "*Regularization*: Penalty based on magnitude of the weighs to the model, encouraging your model to not overfit. Pytorch uses weight decay which is roughly equal to L2 regularization (but subtly differs). Other regularization types (L0, L1, L_inf etc.) might need to be manually calculated and added to loss directly.  \n",
    "*Dropout*: A regularization technique used with Neural networks, randomly zeros values (based on a specified percent) passed through the dropout layer to encourage the model to generalize learning between nodes (don't rely on just some single node for something). \n",
    "\n",
    "\n",
    "#### For these sections it is advised to split up the work with teammates.\n",
    "\n",
    "First let's do a quick manual search of the hyper parameters (you'll be graded here on just following a logical process and explaining it), second try a grid search, and finally a random search.\n",
    "\n",
    "### T1 Manual Search\n",
    "rubric={reasoning:1}\n",
    "\n",
    "As a reminder, manual search is basically just using trial-and-error to find the best combination, starting with an educated guess as to the best parameters, and then making adjustments as you go. For this part, you are only allowed to try *5* different combinations of hyperparameters.\n",
    "\n",
    "Document your starting point, and how you adjusted the hyperparameters along the way, reporting the accuracy for each round. Explain your reasoning for some of the choices you made.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Documentation Goes Here\n",
    "\n",
    "ex:  \n",
    "Round 1  \n",
    "\n",
    "Regularization(L2=.001)  \n",
    "Optimizer(SGD)  \n",
    "Max_epochs(20)  \n",
    "Learning_rate(0.01)  \n",
    "NN_layers(2)  \n",
    "NN_hidden_units(20 for all layers)  \n",
    "Dropout(.1 on output of dense0)  \n",
    "Nonlinearity(ReLU)  \n",
    "~Anything Else~    \n",
    "ACC = 0.05  Awful!  No better than chance!\n",
    "\n",
    "*YOUR WORK*\n",
    "\n",
    "*YOUR EXPLANATION*\n",
    "\n",
    "\n",
    "Grading overview: As long as you provide a rational for what hyperparameters you change, and logically follow some process you will get full credit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T1 Grid Search\n",
    "rubric={accuracy:1, quality:1}\n",
    "\n",
    "Another approach is to try a number of different values by setting intervals to check and then covering all possibilities. Use Scikit-learn's grid search functionality to check a total of around 20 different possible configurations of hyperparameters. **With very large epoch/parameter values it might take 30 minutes to run a trial (depending on your CPU/GPU) so plan accordingly and potentially split up the grid between teammates** (Teammates who didn't run the code can copy outputs into a rawNB convert box)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 24 candidates, totalling 48 fits\n",
      "[CV] lr=0.01, max_epochs=20, module__dropout=0.1, module__hidden_units=50, module__input_dim=101631, module__nonlin=ReLU(), module__output_classes=20 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  lr=0.01, max_epochs=20, module__dropout=0.1, module__hidden_units=50, module__input_dim=101631, module__nonlin=ReLU(), module__output_classes=20, total=  52.6s\n",
      "[CV] lr=0.01, max_epochs=20, module__dropout=0.1, module__hidden_units=50, module__input_dim=101631, module__nonlin=ReLU(), module__output_classes=20 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   53.4s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  lr=0.01, max_epochs=20, module__dropout=0.1, module__hidden_units=50, module__input_dim=101631, module__nonlin=ReLU(), module__output_classes=20, total=  57.8s\n",
      "[CV] lr=0.01, max_epochs=20, module__dropout=0.1, module__hidden_units=50, module__input_dim=101631, module__nonlin=Tanh(), module__output_classes=20 \n",
      "[CV]  lr=0.01, max_epochs=20, module__dropout=0.1, module__hidden_units=50, module__input_dim=101631, module__nonlin=Tanh(), module__output_classes=20, total= 1.1min\n",
      "[CV] lr=0.01, max_epochs=20, module__dropout=0.1, module__hidden_units=50, module__input_dim=101631, module__nonlin=Tanh(), module__output_classes=20 \n",
      "[CV]  lr=0.01, max_epochs=20, module__dropout=0.1, module__hidden_units=50, module__input_dim=101631, module__nonlin=Tanh(), module__output_classes=20, total=  58.5s\n",
      "[CV] lr=0.01, max_epochs=20, module__dropout=0.1, module__hidden_units=100, module__input_dim=101631, module__nonlin=ReLU(), module__output_classes=20 \n",
      "[CV]  lr=0.01, max_epochs=20, module__dropout=0.1, module__hidden_units=100, module__input_dim=101631, module__nonlin=ReLU(), module__output_classes=20, total= 1.1min\n",
      "[CV] lr=0.01, max_epochs=20, module__dropout=0.1, module__hidden_units=100, module__input_dim=101631, module__nonlin=ReLU(), module__output_classes=20 \n",
      "[CV]  lr=0.01, max_epochs=20, module__dropout=0.1, module__hidden_units=100, module__input_dim=101631, module__nonlin=ReLU(), module__output_classes=20, total= 1.1min\n",
      "[CV] lr=0.01, max_epochs=20, module__dropout=0.1, module__hidden_units=100, module__input_dim=101631, module__nonlin=Tanh(), module__output_classes=20 \n",
      "[CV]  lr=0.01, max_epochs=20, module__dropout=0.1, module__hidden_units=100, module__input_dim=101631, module__nonlin=Tanh(), module__output_classes=20, total= 1.1min\n",
      "[CV] lr=0.01, max_epochs=20, module__dropout=0.1, module__hidden_units=100, module__input_dim=101631, module__nonlin=Tanh(), module__output_classes=20 \n",
      "[CV]  lr=0.01, max_epochs=20, module__dropout=0.1, module__hidden_units=100, module__input_dim=101631, module__nonlin=Tanh(), module__output_classes=20, total= 1.1min\n",
      "[CV] lr=0.01, max_epochs=20, module__dropout=0.1, module__hidden_units=150, module__input_dim=101631, module__nonlin=ReLU(), module__output_classes=20 \n",
      "[CV]  lr=0.01, max_epochs=20, module__dropout=0.1, module__hidden_units=150, module__input_dim=101631, module__nonlin=ReLU(), module__output_classes=20, total= 1.2min\n",
      "[CV] lr=0.01, max_epochs=20, module__dropout=0.1, module__hidden_units=150, module__input_dim=101631, module__nonlin=ReLU(), module__output_classes=20 \n",
      "[CV]  lr=0.01, max_epochs=20, module__dropout=0.1, module__hidden_units=150, module__input_dim=101631, module__nonlin=ReLU(), module__output_classes=20, total= 1.2min\n",
      "[CV] lr=0.01, max_epochs=20, module__dropout=0.1, module__hidden_units=150, module__input_dim=101631, module__nonlin=Tanh(), module__output_classes=20 \n",
      "[CV]  lr=0.01, max_epochs=20, module__dropout=0.1, module__hidden_units=150, module__input_dim=101631, module__nonlin=Tanh(), module__output_classes=20, total= 1.2min\n",
      "[CV] lr=0.01, max_epochs=20, module__dropout=0.1, module__hidden_units=150, module__input_dim=101631, module__nonlin=Tanh(), module__output_classes=20 \n",
      "[CV]  lr=0.01, max_epochs=20, module__dropout=0.1, module__hidden_units=150, module__input_dim=101631, module__nonlin=Tanh(), module__output_classes=20, total= 1.2min\n",
      "[CV] lr=0.01, max_epochs=20, module__dropout=0.1, module__hidden_units=200, module__input_dim=101631, module__nonlin=ReLU(), module__output_classes=20 \n",
      "[CV]  lr=0.01, max_epochs=20, module__dropout=0.1, module__hidden_units=200, module__input_dim=101631, module__nonlin=ReLU(), module__output_classes=20, total= 1.3min\n",
      "[CV] lr=0.01, max_epochs=20, module__dropout=0.1, module__hidden_units=200, module__input_dim=101631, module__nonlin=ReLU(), module__output_classes=20 \n",
      "[CV]  lr=0.01, max_epochs=20, module__dropout=0.1, module__hidden_units=200, module__input_dim=101631, module__nonlin=ReLU(), module__output_classes=20, total= 1.3min\n",
      "[CV] lr=0.01, max_epochs=20, module__dropout=0.1, module__hidden_units=200, module__input_dim=101631, module__nonlin=Tanh(), module__output_classes=20 \n",
      "[CV]  lr=0.01, max_epochs=20, module__dropout=0.1, module__hidden_units=200, module__input_dim=101631, module__nonlin=Tanh(), module__output_classes=20, total= 1.3min\n",
      "[CV] lr=0.01, max_epochs=20, module__dropout=0.1, module__hidden_units=200, module__input_dim=101631, module__nonlin=Tanh(), module__output_classes=20 \n",
      "[CV]  lr=0.01, max_epochs=20, module__dropout=0.1, module__hidden_units=200, module__input_dim=101631, module__nonlin=Tanh(), module__output_classes=20, total= 1.3min\n",
      "[CV] lr=0.01, max_epochs=20, module__dropout=0.5, module__hidden_units=50, module__input_dim=101631, module__nonlin=ReLU(), module__output_classes=20 \n",
      "[CV]  lr=0.01, max_epochs=20, module__dropout=0.5, module__hidden_units=50, module__input_dim=101631, module__nonlin=ReLU(), module__output_classes=20, total=  58.0s\n",
      "[CV] lr=0.01, max_epochs=20, module__dropout=0.5, module__hidden_units=50, module__input_dim=101631, module__nonlin=ReLU(), module__output_classes=20 \n",
      "[CV]  lr=0.01, max_epochs=20, module__dropout=0.5, module__hidden_units=50, module__input_dim=101631, module__nonlin=ReLU(), module__output_classes=20, total=  58.3s\n",
      "[CV] lr=0.01, max_epochs=20, module__dropout=0.5, module__hidden_units=50, module__input_dim=101631, module__nonlin=Tanh(), module__output_classes=20 \n",
      "[CV]  lr=0.01, max_epochs=20, module__dropout=0.5, module__hidden_units=50, module__input_dim=101631, module__nonlin=Tanh(), module__output_classes=20, total=  58.6s\n",
      "[CV] lr=0.01, max_epochs=20, module__dropout=0.5, module__hidden_units=50, module__input_dim=101631, module__nonlin=Tanh(), module__output_classes=20 \n",
      "[CV]  lr=0.01, max_epochs=20, module__dropout=0.5, module__hidden_units=50, module__input_dim=101631, module__nonlin=Tanh(), module__output_classes=20, total=  57.6s\n",
      "[CV] lr=0.01, max_epochs=20, module__dropout=0.5, module__hidden_units=100, module__input_dim=101631, module__nonlin=ReLU(), module__output_classes=20 \n",
      "[CV]  lr=0.01, max_epochs=20, module__dropout=0.5, module__hidden_units=100, module__input_dim=101631, module__nonlin=ReLU(), module__output_classes=20, total= 1.1min\n",
      "[CV] lr=0.01, max_epochs=20, module__dropout=0.5, module__hidden_units=100, module__input_dim=101631, module__nonlin=ReLU(), module__output_classes=20 \n",
      "[CV]  lr=0.01, max_epochs=20, module__dropout=0.5, module__hidden_units=100, module__input_dim=101631, module__nonlin=ReLU(), module__output_classes=20, total= 1.1min\n",
      "[CV] lr=0.01, max_epochs=20, module__dropout=0.5, module__hidden_units=100, module__input_dim=101631, module__nonlin=Tanh(), module__output_classes=20 \n",
      "[CV]  lr=0.01, max_epochs=20, module__dropout=0.5, module__hidden_units=100, module__input_dim=101631, module__nonlin=Tanh(), module__output_classes=20, total= 1.1min\n",
      "[CV] lr=0.01, max_epochs=20, module__dropout=0.5, module__hidden_units=100, module__input_dim=101631, module__nonlin=Tanh(), module__output_classes=20 \n",
      "[CV]  lr=0.01, max_epochs=20, module__dropout=0.5, module__hidden_units=100, module__input_dim=101631, module__nonlin=Tanh(), module__output_classes=20, total= 1.0min\n",
      "[CV] lr=0.01, max_epochs=20, module__dropout=0.5, module__hidden_units=150, module__input_dim=101631, module__nonlin=ReLU(), module__output_classes=20 \n",
      "[CV]  lr=0.01, max_epochs=20, module__dropout=0.5, module__hidden_units=150, module__input_dim=101631, module__nonlin=ReLU(), module__output_classes=20, total= 1.0min\n",
      "[CV] lr=0.01, max_epochs=20, module__dropout=0.5, module__hidden_units=150, module__input_dim=101631, module__nonlin=ReLU(), module__output_classes=20 \n",
      "[CV]  lr=0.01, max_epochs=20, module__dropout=0.5, module__hidden_units=150, module__input_dim=101631, module__nonlin=ReLU(), module__output_classes=20, total= 1.0min\n",
      "[CV] lr=0.01, max_epochs=20, module__dropout=0.5, module__hidden_units=150, module__input_dim=101631, module__nonlin=Tanh(), module__output_classes=20 \n",
      "[CV]  lr=0.01, max_epochs=20, module__dropout=0.5, module__hidden_units=150, module__input_dim=101631, module__nonlin=Tanh(), module__output_classes=20, total= 1.0min\n",
      "[CV] lr=0.01, max_epochs=20, module__dropout=0.5, module__hidden_units=150, module__input_dim=101631, module__nonlin=Tanh(), module__output_classes=20 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  lr=0.01, max_epochs=20, module__dropout=0.5, module__hidden_units=150, module__input_dim=101631, module__nonlin=Tanh(), module__output_classes=20, total= 1.0min\n",
      "[CV] lr=0.01, max_epochs=20, module__dropout=0.5, module__hidden_units=200, module__input_dim=101631, module__nonlin=ReLU(), module__output_classes=20 \n",
      "[CV]  lr=0.01, max_epochs=20, module__dropout=0.5, module__hidden_units=200, module__input_dim=101631, module__nonlin=ReLU(), module__output_classes=20, total= 1.1min\n",
      "[CV] lr=0.01, max_epochs=20, module__dropout=0.5, module__hidden_units=200, module__input_dim=101631, module__nonlin=ReLU(), module__output_classes=20 \n",
      "[CV]  lr=0.01, max_epochs=20, module__dropout=0.5, module__hidden_units=200, module__input_dim=101631, module__nonlin=ReLU(), module__output_classes=20, total= 1.2min\n",
      "[CV] lr=0.01, max_epochs=20, module__dropout=0.5, module__hidden_units=200, module__input_dim=101631, module__nonlin=Tanh(), module__output_classes=20 \n",
      "[CV]  lr=0.01, max_epochs=20, module__dropout=0.5, module__hidden_units=200, module__input_dim=101631, module__nonlin=Tanh(), module__output_classes=20, total= 1.2min\n",
      "[CV] lr=0.01, max_epochs=20, module__dropout=0.5, module__hidden_units=200, module__input_dim=101631, module__nonlin=Tanh(), module__output_classes=20 \n",
      "[CV]  lr=0.01, max_epochs=20, module__dropout=0.5, module__hidden_units=200, module__input_dim=101631, module__nonlin=Tanh(), module__output_classes=20, total= 1.1min\n",
      "[CV] lr=0.01, max_epochs=20, module__dropout=0.9, module__hidden_units=50, module__input_dim=101631, module__nonlin=ReLU(), module__output_classes=20 \n",
      "[CV]  lr=0.01, max_epochs=20, module__dropout=0.9, module__hidden_units=50, module__input_dim=101631, module__nonlin=ReLU(), module__output_classes=20, total=  51.5s\n",
      "[CV] lr=0.01, max_epochs=20, module__dropout=0.9, module__hidden_units=50, module__input_dim=101631, module__nonlin=ReLU(), module__output_classes=20 \n",
      "[CV]  lr=0.01, max_epochs=20, module__dropout=0.9, module__hidden_units=50, module__input_dim=101631, module__nonlin=ReLU(), module__output_classes=20, total=  47.8s\n",
      "[CV] lr=0.01, max_epochs=20, module__dropout=0.9, module__hidden_units=50, module__input_dim=101631, module__nonlin=Tanh(), module__output_classes=20 \n",
      "[CV]  lr=0.01, max_epochs=20, module__dropout=0.9, module__hidden_units=50, module__input_dim=101631, module__nonlin=Tanh(), module__output_classes=20, total=  46.4s\n",
      "[CV] lr=0.01, max_epochs=20, module__dropout=0.9, module__hidden_units=50, module__input_dim=101631, module__nonlin=Tanh(), module__output_classes=20 \n",
      "[CV]  lr=0.01, max_epochs=20, module__dropout=0.9, module__hidden_units=50, module__input_dim=101631, module__nonlin=Tanh(), module__output_classes=20, total=  47.6s\n",
      "[CV] lr=0.01, max_epochs=20, module__dropout=0.9, module__hidden_units=100, module__input_dim=101631, module__nonlin=ReLU(), module__output_classes=20 \n",
      "[CV]  lr=0.01, max_epochs=20, module__dropout=0.9, module__hidden_units=100, module__input_dim=101631, module__nonlin=ReLU(), module__output_classes=20, total=  55.8s\n",
      "[CV] lr=0.01, max_epochs=20, module__dropout=0.9, module__hidden_units=100, module__input_dim=101631, module__nonlin=ReLU(), module__output_classes=20 \n",
      "[CV]  lr=0.01, max_epochs=20, module__dropout=0.9, module__hidden_units=100, module__input_dim=101631, module__nonlin=ReLU(), module__output_classes=20, total=  55.8s\n",
      "[CV] lr=0.01, max_epochs=20, module__dropout=0.9, module__hidden_units=100, module__input_dim=101631, module__nonlin=Tanh(), module__output_classes=20 \n",
      "[CV]  lr=0.01, max_epochs=20, module__dropout=0.9, module__hidden_units=100, module__input_dim=101631, module__nonlin=Tanh(), module__output_classes=20, total=  54.3s\n",
      "[CV] lr=0.01, max_epochs=20, module__dropout=0.9, module__hidden_units=100, module__input_dim=101631, module__nonlin=Tanh(), module__output_classes=20 \n",
      "[CV]  lr=0.01, max_epochs=20, module__dropout=0.9, module__hidden_units=100, module__input_dim=101631, module__nonlin=Tanh(), module__output_classes=20, total=  52.5s\n",
      "[CV] lr=0.01, max_epochs=20, module__dropout=0.9, module__hidden_units=150, module__input_dim=101631, module__nonlin=ReLU(), module__output_classes=20 \n",
      "[CV]  lr=0.01, max_epochs=20, module__dropout=0.9, module__hidden_units=150, module__input_dim=101631, module__nonlin=ReLU(), module__output_classes=20, total= 1.0min\n",
      "[CV] lr=0.01, max_epochs=20, module__dropout=0.9, module__hidden_units=150, module__input_dim=101631, module__nonlin=ReLU(), module__output_classes=20 \n",
      "[CV]  lr=0.01, max_epochs=20, module__dropout=0.9, module__hidden_units=150, module__input_dim=101631, module__nonlin=ReLU(), module__output_classes=20, total= 1.1min\n",
      "[CV] lr=0.01, max_epochs=20, module__dropout=0.9, module__hidden_units=150, module__input_dim=101631, module__nonlin=Tanh(), module__output_classes=20 \n",
      "[CV]  lr=0.01, max_epochs=20, module__dropout=0.9, module__hidden_units=150, module__input_dim=101631, module__nonlin=Tanh(), module__output_classes=20, total= 1.1min\n",
      "[CV] lr=0.01, max_epochs=20, module__dropout=0.9, module__hidden_units=150, module__input_dim=101631, module__nonlin=Tanh(), module__output_classes=20 \n",
      "[CV]  lr=0.01, max_epochs=20, module__dropout=0.9, module__hidden_units=150, module__input_dim=101631, module__nonlin=Tanh(), module__output_classes=20, total= 1.0min\n",
      "[CV] lr=0.01, max_epochs=20, module__dropout=0.9, module__hidden_units=200, module__input_dim=101631, module__nonlin=ReLU(), module__output_classes=20 \n",
      "[CV]  lr=0.01, max_epochs=20, module__dropout=0.9, module__hidden_units=200, module__input_dim=101631, module__nonlin=ReLU(), module__output_classes=20, total= 1.2min\n",
      "[CV] lr=0.01, max_epochs=20, module__dropout=0.9, module__hidden_units=200, module__input_dim=101631, module__nonlin=ReLU(), module__output_classes=20 \n",
      "[CV]  lr=0.01, max_epochs=20, module__dropout=0.9, module__hidden_units=200, module__input_dim=101631, module__nonlin=ReLU(), module__output_classes=20, total= 1.2min\n",
      "[CV] lr=0.01, max_epochs=20, module__dropout=0.9, module__hidden_units=200, module__input_dim=101631, module__nonlin=Tanh(), module__output_classes=20 \n",
      "[CV]  lr=0.01, max_epochs=20, module__dropout=0.9, module__hidden_units=200, module__input_dim=101631, module__nonlin=Tanh(), module__output_classes=20, total= 1.1min\n",
      "[CV] lr=0.01, max_epochs=20, module__dropout=0.9, module__hidden_units=200, module__input_dim=101631, module__nonlin=Tanh(), module__output_classes=20 \n",
      "[CV]  lr=0.01, max_epochs=20, module__dropout=0.9, module__hidden_units=200, module__input_dim=101631, module__nonlin=Tanh(), module__output_classes=20, total= 1.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  48 out of  48 | elapsed: 51.0min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=2,\n",
       "             estimator=<class 'skorch.classifier.NeuralNetClassifier'>[uninitialized](\n",
       "  module=TwoLayerNN(\n",
       "    (dense0): Linear(in_features=101631, out_features=20, bias=True)\n",
       "    (nonlin): ReLU()\n",
       "    (dropout): Dropout(p=0.5)\n",
       "    (dense1): Linear(in_features=20, out_features=20, bias=True)\n",
       "    (output): Linear(in_features=20, out_features=20, bias=True)\n",
       "    (softmax): Softmax()\n",
       "  ),\n",
       "),\n",
       "             param_grid={'lr': [0.01], 'max_epochs': [20],\n",
       "                         'module__dropout': [0.1, 0.5, 0.9],\n",
       "                         'module__hidden_units': [50, 100, 150, 200],\n",
       "                         'module__input_dim': [101631],\n",
       "                         'module__nonlin': [ReLU(), Tanh()],\n",
       "                         'module__output_classes': [20]},\n",
       "             scoring='accuracy', verbose=2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Your code to run the grid search here\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from skorch import NeuralNetClassifier\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class TwoLayerNN(nn.Module): #I changed the arguments to allow dropout as an input\n",
    "    def __init__(self, input_dim, hidden_units=20, output_classes=20, nonlin=nn.ReLU(), dropout=.5):  \n",
    "        super(TwoLayerNN, self).__init__()\n",
    "        self.dense0 = nn.Linear(input_dim, hidden_units)\n",
    "        self.nonlin = nonlin\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.dense1 = nn.Linear(hidden_units, hidden_units)\n",
    "        self.output = nn.Linear(hidden_units, output_classes)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, X, **kwargs):\n",
    "        X = self.nonlin(self.dense0(X))\n",
    "        X = self.dropout(X)\n",
    "        X = self.nonlin(self.dense1(X))\n",
    "        X = self.softmax(self.output(X))\n",
    "        return X\n",
    "\n",
    "torch.manual_seed(manual_seed)\n",
    "np.random.seed(manual_seed)\n",
    "torch.cuda.manual_seed(manual_seed)\n",
    "\n",
    "net = NeuralNetClassifier(\n",
    "    TwoLayerNN(input_dim=X_train.shape[1]),\n",
    "    max_epochs=10,\n",
    "    lr=0.01,\n",
    "    optimizer=torch.optim.Adam,   #Adam can be a little faster than SGD, we'll discuss the difference in lecture\n",
    "    optimizer__weight_decay=0.001,  #roughly equivalent to L2 regularization\n",
    "    iterator_train__shuffle=True,\n",
    "    device=device  #'cpu' or 'cuda'\n",
    ")\n",
    "\n",
    "net.set_params(train_split=False, verbose=0) #train_split=False means we'll use our own test set, verbose=0 means it won't print too much out\n",
    "\n",
    "params = {\n",
    "    #task specific\n",
    "    'module__input_dim': [X_train.shape[1]],\n",
    "    'module__output_classes': [20],\n",
    "    #training hyperparameters\n",
    "    'lr': [0.01],\n",
    "    'max_epochs': [20],\n",
    "    #model architecture hyperparameters \n",
    "    'module__hidden_units': [50,100,150,200],\n",
    "    'module__nonlin': [nn.ReLU(),nn.Tanh()],\n",
    "    'module__dropout': [.1,.5,.9],\n",
    "}\n",
    "gs = GridSearchCV(net, params, refit=True,cv=2, scoring='accuracy', verbose=2)  #could do more CV folds in practice \n",
    "\n",
    "gs.fit(torch.from_numpy(X_train.todense()).float(), torch.tensor(y_train,dtype=torch.long))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.61331094219551\n",
      "{'lr': 0.01, 'max_epochs': 20, 'module__dropout': 0.5, 'module__hidden_units': 50, 'module__input_dim': 101631, 'module__nonlin': Tanh(), 'module__output_classes': 20}\n"
     ]
    }
   ],
   "source": [
    "print(gs.best_score_)\n",
    "print(gs.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5821826872012745\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "pred = gs.best_estimator_.predict(torch.from_numpy(X_test.todense()).float())\n",
    "acc = metrics.accuracy_score(y_test, pred)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "###Copy of outputs from teammates if you didn't run this section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T1 Random Search\n",
    "rubric={accuracy=1, quality=1}\n",
    "\n",
    "Finally, use scikit-learn's random search functionality to check a total of around 20 different possible configurations of hyperparameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 20 candidates, totalling 40 fits\n",
      "[CV] lr=0.01, max_epochs=20, module__dropout=0.5116352079319715, module__hidden_units=482, module__input_dim=101631, module__nonlin=Tanh(), module__output_classes=20, optimizer__weight_decay=9.811577102745637e-06 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  lr=0.01, max_epochs=20, module__dropout=0.5116352079319715, module__hidden_units=482, module__input_dim=101631, module__nonlin=Tanh(), module__output_classes=20, optimizer__weight_decay=9.811577102745637e-06, total= 1.8min\n",
      "[CV] lr=0.01, max_epochs=20, module__dropout=0.5116352079319715, module__hidden_units=482, module__input_dim=101631, module__nonlin=Tanh(), module__output_classes=20, optimizer__weight_decay=9.811577102745637e-06 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  1.8min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  lr=0.01, max_epochs=20, module__dropout=0.5116352079319715, module__hidden_units=482, module__input_dim=101631, module__nonlin=Tanh(), module__output_classes=20, optimizer__weight_decay=9.811577102745637e-06, total= 1.8min\n",
      "[CV] lr=0.01, max_epochs=20, module__dropout=0.059080698832860934, module__hidden_units=425, module__input_dim=101631, module__nonlin=ReLU(), module__output_classes=20, optimizer__weight_decay=0.00037907222167547734 \n",
      "[CV]  lr=0.01, max_epochs=20, module__dropout=0.059080698832860934, module__hidden_units=425, module__input_dim=101631, module__nonlin=ReLU(), module__output_classes=20, optimizer__weight_decay=0.00037907222167547734, total= 1.7min\n",
      "[CV] lr=0.01, max_epochs=20, module__dropout=0.059080698832860934, module__hidden_units=425, module__input_dim=101631, module__nonlin=ReLU(), module__output_classes=20, optimizer__weight_decay=0.00037907222167547734 \n",
      "[CV]  lr=0.01, max_epochs=20, module__dropout=0.059080698832860934, module__hidden_units=425, module__input_dim=101631, module__nonlin=ReLU(), module__output_classes=20, optimizer__weight_decay=0.00037907222167547734, total= 1.7min\n",
      "[CV] lr=0.01, max_epochs=20, module__dropout=0.3845214956654728, module__hidden_units=302, module__input_dim=101631, module__nonlin=ReLU(), module__output_classes=20, optimizer__weight_decay=9.497743174611658e-06 \n",
      "[CV]  lr=0.01, max_epochs=20, module__dropout=0.3845214956654728, module__hidden_units=302, module__input_dim=101631, module__nonlin=ReLU(), module__output_classes=20, optimizer__weight_decay=9.497743174611658e-06, total= 1.4min\n",
      "[CV] lr=0.01, max_epochs=20, module__dropout=0.3845214956654728, module__hidden_units=302, module__input_dim=101631, module__nonlin=ReLU(), module__output_classes=20, optimizer__weight_decay=9.497743174611658e-06 \n",
      "[CV]  lr=0.01, max_epochs=20, module__dropout=0.3845214956654728, module__hidden_units=302, module__input_dim=101631, module__nonlin=ReLU(), module__output_classes=20, optimizer__weight_decay=9.497743174611658e-06, total= 1.4min\n",
      "[CV] lr=0.01, max_epochs=20, module__dropout=0.8418717674059527, module__hidden_units=388, module__input_dim=101631, module__nonlin=ReLU(), module__output_classes=20, optimizer__weight_decay=5.946138722551874e-06 \n",
      "[CV]  lr=0.01, max_epochs=20, module__dropout=0.8418717674059527, module__hidden_units=388, module__input_dim=101631, module__nonlin=ReLU(), module__output_classes=20, optimizer__weight_decay=5.946138722551874e-06, total= 1.6min\n",
      "[CV] lr=0.01, max_epochs=20, module__dropout=0.8418717674059527, module__hidden_units=388, module__input_dim=101631, module__nonlin=ReLU(), module__output_classes=20, optimizer__weight_decay=5.946138722551874e-06 \n",
      "[CV]  lr=0.01, max_epochs=20, module__dropout=0.8418717674059527, module__hidden_units=388, module__input_dim=101631, module__nonlin=ReLU(), module__output_classes=20, optimizer__weight_decay=5.946138722551874e-06, total= 1.6min\n",
      "[CV] lr=0.01, max_epochs=20, module__dropout=0.2627837570285292, module__hidden_units=220, module__input_dim=101631, module__nonlin=Tanh(), module__output_classes=20, optimizer__weight_decay=3.842124987494155e-06 \n",
      "[CV]  lr=0.01, max_epochs=20, module__dropout=0.2627837570285292, module__hidden_units=220, module__input_dim=101631, module__nonlin=Tanh(), module__output_classes=20, optimizer__weight_decay=3.842124987494155e-06, total= 1.1min\n",
      "[CV] lr=0.01, max_epochs=20, module__dropout=0.2627837570285292, module__hidden_units=220, module__input_dim=101631, module__nonlin=Tanh(), module__output_classes=20, optimizer__weight_decay=3.842124987494155e-06 \n",
      "[CV]  lr=0.01, max_epochs=20, module__dropout=0.2627837570285292, module__hidden_units=220, module__input_dim=101631, module__nonlin=Tanh(), module__output_classes=20, optimizer__weight_decay=3.842124987494155e-06, total= 1.1min\n",
      "[CV] lr=0.01, max_epochs=20, module__dropout=0.34171903782250124, module__hidden_units=275, module__input_dim=101631, module__nonlin=Tanh(), module__output_classes=20, optimizer__weight_decay=1.0356310586719617e-06 \n",
      "[CV]  lr=0.01, max_epochs=20, module__dropout=0.34171903782250124, module__hidden_units=275, module__input_dim=101631, module__nonlin=Tanh(), module__output_classes=20, optimizer__weight_decay=1.0356310586719617e-06, total= 1.3min\n",
      "[CV] lr=0.01, max_epochs=20, module__dropout=0.34171903782250124, module__hidden_units=275, module__input_dim=101631, module__nonlin=Tanh(), module__output_classes=20, optimizer__weight_decay=1.0356310586719617e-06 \n",
      "[CV]  lr=0.01, max_epochs=20, module__dropout=0.34171903782250124, module__hidden_units=275, module__input_dim=101631, module__nonlin=Tanh(), module__output_classes=20, optimizer__weight_decay=1.0356310586719617e-06, total= 1.3min\n",
      "[CV] lr=0.01, max_epochs=20, module__dropout=0.3210610368679443, module__hidden_units=74, module__input_dim=101631, module__nonlin=Tanh(), module__output_classes=20, optimizer__weight_decay=0.008296443590675174 \n",
      "[CV]  lr=0.01, max_epochs=20, module__dropout=0.3210610368679443, module__hidden_units=74, module__input_dim=101631, module__nonlin=Tanh(), module__output_classes=20, optimizer__weight_decay=0.008296443590675174, total=  46.8s\n",
      "[CV] lr=0.01, max_epochs=20, module__dropout=0.3210610368679443, module__hidden_units=74, module__input_dim=101631, module__nonlin=Tanh(), module__output_classes=20, optimizer__weight_decay=0.008296443590675174 \n",
      "[CV]  lr=0.01, max_epochs=20, module__dropout=0.3210610368679443, module__hidden_units=74, module__input_dim=101631, module__nonlin=Tanh(), module__output_classes=20, optimizer__weight_decay=0.008296443590675174, total=  47.0s\n",
      "[CV] lr=0.01, max_epochs=20, module__dropout=0.7032006397547076, module__hidden_units=388, module__input_dim=101631, module__nonlin=ReLU(), module__output_classes=20, optimizer__weight_decay=9.790141318570272e-06 \n",
      "[CV]  lr=0.01, max_epochs=20, module__dropout=0.7032006397547076, module__hidden_units=388, module__input_dim=101631, module__nonlin=ReLU(), module__output_classes=20, optimizer__weight_decay=9.790141318570272e-06, total= 1.5min\n",
      "[CV] lr=0.01, max_epochs=20, module__dropout=0.7032006397547076, module__hidden_units=388, module__input_dim=101631, module__nonlin=ReLU(), module__output_classes=20, optimizer__weight_decay=9.790141318570272e-06 \n",
      "[CV]  lr=0.01, max_epochs=20, module__dropout=0.7032006397547076, module__hidden_units=388, module__input_dim=101631, module__nonlin=ReLU(), module__output_classes=20, optimizer__weight_decay=9.790141318570272e-06, total= 1.6min\n",
      "[CV] lr=0.01, max_epochs=20, module__dropout=0.5535718450103309, module__hidden_units=263, module__input_dim=101631, module__nonlin=Tanh(), module__output_classes=20, optimizer__weight_decay=0.0015827071158293006 \n",
      "[CV]  lr=0.01, max_epochs=20, module__dropout=0.5535718450103309, module__hidden_units=263, module__input_dim=101631, module__nonlin=Tanh(), module__output_classes=20, optimizer__weight_decay=0.0015827071158293006, total= 1.3min\n",
      "[CV] lr=0.01, max_epochs=20, module__dropout=0.5535718450103309, module__hidden_units=263, module__input_dim=101631, module__nonlin=Tanh(), module__output_classes=20, optimizer__weight_decay=0.0015827071158293006 \n",
      "[CV]  lr=0.01, max_epochs=20, module__dropout=0.5535718450103309, module__hidden_units=263, module__input_dim=101631, module__nonlin=Tanh(), module__output_classes=20, optimizer__weight_decay=0.0015827071158293006, total= 1.3min\n",
      "[CV] lr=0.01, max_epochs=20, module__dropout=0.9579125093397054, module__hidden_units=275, module__input_dim=101631, module__nonlin=ReLU(), module__output_classes=20, optimizer__weight_decay=3.396787474704536e-05 \n",
      "[CV]  lr=0.01, max_epochs=20, module__dropout=0.9579125093397054, module__hidden_units=275, module__input_dim=101631, module__nonlin=ReLU(), module__output_classes=20, optimizer__weight_decay=3.396787474704536e-05, total= 1.3min\n",
      "[CV] lr=0.01, max_epochs=20, module__dropout=0.9579125093397054, module__hidden_units=275, module__input_dim=101631, module__nonlin=ReLU(), module__output_classes=20, optimizer__weight_decay=3.396787474704536e-05 \n",
      "[CV]  lr=0.01, max_epochs=20, module__dropout=0.9579125093397054, module__hidden_units=275, module__input_dim=101631, module__nonlin=ReLU(), module__output_classes=20, optimizer__weight_decay=3.396787474704536e-05, total= 1.3min\n",
      "[CV] lr=0.01, max_epochs=20, module__dropout=0.07073324041453222, module__hidden_units=273, module__input_dim=101631, module__nonlin=Tanh(), module__output_classes=20, optimizer__weight_decay=0.0049779537405538685 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  lr=0.01, max_epochs=20, module__dropout=0.07073324041453222, module__hidden_units=273, module__input_dim=101631, module__nonlin=Tanh(), module__output_classes=20, optimizer__weight_decay=0.0049779537405538685, total= 1.3min\n",
      "[CV] lr=0.01, max_epochs=20, module__dropout=0.07073324041453222, module__hidden_units=273, module__input_dim=101631, module__nonlin=Tanh(), module__output_classes=20, optimizer__weight_decay=0.0049779537405538685 \n",
      "[CV]  lr=0.01, max_epochs=20, module__dropout=0.07073324041453222, module__hidden_units=273, module__input_dim=101631, module__nonlin=Tanh(), module__output_classes=20, optimizer__weight_decay=0.0049779537405538685, total= 1.3min\n",
      "[CV] lr=0.01, max_epochs=20, module__dropout=0.5188081489541363, module__hidden_units=461, module__input_dim=101631, module__nonlin=ReLU(), module__output_classes=20, optimizer__weight_decay=0.0017813733700752365 \n",
      "[CV]  lr=0.01, max_epochs=20, module__dropout=0.5188081489541363, module__hidden_units=461, module__input_dim=101631, module__nonlin=ReLU(), module__output_classes=20, optimizer__weight_decay=0.0017813733700752365, total= 1.7min\n",
      "[CV] lr=0.01, max_epochs=20, module__dropout=0.5188081489541363, module__hidden_units=461, module__input_dim=101631, module__nonlin=ReLU(), module__output_classes=20, optimizer__weight_decay=0.0017813733700752365 \n",
      "[CV]  lr=0.01, max_epochs=20, module__dropout=0.5188081489541363, module__hidden_units=461, module__input_dim=101631, module__nonlin=ReLU(), module__output_classes=20, optimizer__weight_decay=0.0017813733700752365, total= 1.7min\n",
      "[CV] lr=0.01, max_epochs=20, module__dropout=0.8442937150761967, module__hidden_units=491, module__input_dim=101631, module__nonlin=Tanh(), module__output_classes=20, optimizer__weight_decay=0.0035240063036953906 \n",
      "[CV]  lr=0.01, max_epochs=20, module__dropout=0.8442937150761967, module__hidden_units=491, module__input_dim=101631, module__nonlin=Tanh(), module__output_classes=20, optimizer__weight_decay=0.0035240063036953906, total= 1.8min\n",
      "[CV] lr=0.01, max_epochs=20, module__dropout=0.8442937150761967, module__hidden_units=491, module__input_dim=101631, module__nonlin=Tanh(), module__output_classes=20, optimizer__weight_decay=0.0035240063036953906 \n",
      "[CV]  lr=0.01, max_epochs=20, module__dropout=0.8442937150761967, module__hidden_units=491, module__input_dim=101631, module__nonlin=Tanh(), module__output_classes=20, optimizer__weight_decay=0.0035240063036953906, total= 1.8min\n",
      "[CV] lr=0.01, max_epochs=20, module__dropout=0.8732378254423817, module__hidden_units=94, module__input_dim=101631, module__nonlin=ReLU(), module__output_classes=20, optimizer__weight_decay=0.0001885877938821874 \n",
      "[CV]  lr=0.01, max_epochs=20, module__dropout=0.8732378254423817, module__hidden_units=94, module__input_dim=101631, module__nonlin=ReLU(), module__output_classes=20, optimizer__weight_decay=0.0001885877938821874, total=  49.5s\n",
      "[CV] lr=0.01, max_epochs=20, module__dropout=0.8732378254423817, module__hidden_units=94, module__input_dim=101631, module__nonlin=ReLU(), module__output_classes=20, optimizer__weight_decay=0.0001885877938821874 \n",
      "[CV]  lr=0.01, max_epochs=20, module__dropout=0.8732378254423817, module__hidden_units=94, module__input_dim=101631, module__nonlin=ReLU(), module__output_classes=20, optimizer__weight_decay=0.0001885877938821874, total=  49.2s\n",
      "[CV] lr=0.01, max_epochs=20, module__dropout=0.6409093693092771, module__hidden_units=384, module__input_dim=101631, module__nonlin=Tanh(), module__output_classes=20, optimizer__weight_decay=0.0005588618299140654 \n",
      "[CV]  lr=0.01, max_epochs=20, module__dropout=0.6409093693092771, module__hidden_units=384, module__input_dim=101631, module__nonlin=Tanh(), module__output_classes=20, optimizer__weight_decay=0.0005588618299140654, total= 1.5min\n",
      "[CV] lr=0.01, max_epochs=20, module__dropout=0.6409093693092771, module__hidden_units=384, module__input_dim=101631, module__nonlin=Tanh(), module__output_classes=20, optimizer__weight_decay=0.0005588618299140654 \n",
      "[CV]  lr=0.01, max_epochs=20, module__dropout=0.6409093693092771, module__hidden_units=384, module__input_dim=101631, module__nonlin=Tanh(), module__output_classes=20, optimizer__weight_decay=0.0005588618299140654, total= 1.5min\n",
      "[CV] lr=0.01, max_epochs=20, module__dropout=0.24191297759916075, module__hidden_units=244, module__input_dim=101631, module__nonlin=Tanh(), module__output_classes=20, optimizer__weight_decay=2.2534799050091398e-05 \n",
      "[CV]  lr=0.01, max_epochs=20, module__dropout=0.24191297759916075, module__hidden_units=244, module__input_dim=101631, module__nonlin=Tanh(), module__output_classes=20, optimizer__weight_decay=2.2534799050091398e-05, total= 1.2min\n",
      "[CV] lr=0.01, max_epochs=20, module__dropout=0.24191297759916075, module__hidden_units=244, module__input_dim=101631, module__nonlin=Tanh(), module__output_classes=20, optimizer__weight_decay=2.2534799050091398e-05 \n",
      "[CV]  lr=0.01, max_epochs=20, module__dropout=0.24191297759916075, module__hidden_units=244, module__input_dim=101631, module__nonlin=Tanh(), module__output_classes=20, optimizer__weight_decay=2.2534799050091398e-05, total= 1.2min\n",
      "[CV] lr=0.01, max_epochs=20, module__dropout=0.5477780506478279, module__hidden_units=53, module__input_dim=101631, module__nonlin=Tanh(), module__output_classes=20, optimizer__weight_decay=0.0021921014354713006 \n",
      "[CV]  lr=0.01, max_epochs=20, module__dropout=0.5477780506478279, module__hidden_units=53, module__input_dim=101631, module__nonlin=Tanh(), module__output_classes=20, optimizer__weight_decay=0.0021921014354713006, total=  46.5s\n",
      "[CV] lr=0.01, max_epochs=20, module__dropout=0.5477780506478279, module__hidden_units=53, module__input_dim=101631, module__nonlin=Tanh(), module__output_classes=20, optimizer__weight_decay=0.0021921014354713006 \n",
      "[CV]  lr=0.01, max_epochs=20, module__dropout=0.5477780506478279, module__hidden_units=53, module__input_dim=101631, module__nonlin=Tanh(), module__output_classes=20, optimizer__weight_decay=0.0021921014354713006, total=  44.9s\n",
      "[CV] lr=0.01, max_epochs=20, module__dropout=0.7702907055478478, module__hidden_units=88, module__input_dim=101631, module__nonlin=ReLU(), module__output_classes=20, optimizer__weight_decay=0.0012262989547566626 \n",
      "[CV]  lr=0.01, max_epochs=20, module__dropout=0.7702907055478478, module__hidden_units=88, module__input_dim=101631, module__nonlin=ReLU(), module__output_classes=20, optimizer__weight_decay=0.0012262989547566626, total=  50.7s\n",
      "[CV] lr=0.01, max_epochs=20, module__dropout=0.7702907055478478, module__hidden_units=88, module__input_dim=101631, module__nonlin=ReLU(), module__output_classes=20, optimizer__weight_decay=0.0012262989547566626 \n",
      "[CV]  lr=0.01, max_epochs=20, module__dropout=0.7702907055478478, module__hidden_units=88, module__input_dim=101631, module__nonlin=ReLU(), module__output_classes=20, optimizer__weight_decay=0.0012262989547566626, total=  51.3s\n",
      "[CV] lr=0.01, max_epochs=20, module__dropout=0.9375775374269122, module__hidden_units=361, module__input_dim=101631, module__nonlin=ReLU(), module__output_classes=20, optimizer__weight_decay=0.00012507876646795208 \n",
      "[CV]  lr=0.01, max_epochs=20, module__dropout=0.9375775374269122, module__hidden_units=361, module__input_dim=101631, module__nonlin=ReLU(), module__output_classes=20, optimizer__weight_decay=0.00012507876646795208, total= 1.6min\n",
      "[CV] lr=0.01, max_epochs=20, module__dropout=0.9375775374269122, module__hidden_units=361, module__input_dim=101631, module__nonlin=ReLU(), module__output_classes=20, optimizer__weight_decay=0.00012507876646795208 \n",
      "[CV]  lr=0.01, max_epochs=20, module__dropout=0.9375775374269122, module__hidden_units=361, module__input_dim=101631, module__nonlin=ReLU(), module__output_classes=20, optimizer__weight_decay=0.00012507876646795208, total= 1.6min\n",
      "[CV] lr=0.01, max_epochs=20, module__dropout=0.5652982129529925, module__hidden_units=308, module__input_dim=101631, module__nonlin=Tanh(), module__output_classes=20, optimizer__weight_decay=2.6904903987431842e-05 \n",
      "[CV]  lr=0.01, max_epochs=20, module__dropout=0.5652982129529925, module__hidden_units=308, module__input_dim=101631, module__nonlin=Tanh(), module__output_classes=20, optimizer__weight_decay=2.6904903987431842e-05, total= 1.4min\n",
      "[CV] lr=0.01, max_epochs=20, module__dropout=0.5652982129529925, module__hidden_units=308, module__input_dim=101631, module__nonlin=Tanh(), module__output_classes=20, optimizer__weight_decay=2.6904903987431842e-05 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  lr=0.01, max_epochs=20, module__dropout=0.5652982129529925, module__hidden_units=308, module__input_dim=101631, module__nonlin=Tanh(), module__output_classes=20, optimizer__weight_decay=2.6904903987431842e-05, total= 1.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  40 out of  40 | elapsed: 53.7min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=2,\n",
       "                   estimator=<class 'skorch.classifier.NeuralNetClassifier'>[uninitialized](\n",
       "  module=TwoLayerNN(\n",
       "    (dense0): Linear(in_features=101631, out_features=20, bias=True)\n",
       "    (nonlin): ReLU()\n",
       "    (dropout): Dropout(p=0.5)\n",
       "    (dense1): Linear(in_features=20, out_features=20, bias=True)\n",
       "    (output): Linear(in_features=20, out_features=20, bias=True)\n",
       "    (softmax): Softmax()\n",
       "  ),\n",
       ")...\n",
       "                                        'module__dropout': <scipy.stats._distn_infrastructure.rv_frozen object at 0x00000227ACBE2390>,\n",
       "                                        'module__hidden_units': <scipy.stats._distn_infrastructure.rv_frozen object at 0x00000227ACBE26D8>,\n",
       "                                        'module__input_dim': [101631],\n",
       "                                        'module__nonlin': [ReLU(), Tanh()],\n",
       "                                        'module__output_classes': [20],\n",
       "                                        'optimizer__weight_decay': <scipy.stats._distn_infrastructure.rv_frozen object at 0x00000227ACBE2940>},\n",
       "                   random_state=572, scoring='accuracy', verbose=2)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Your code to run the grid search here\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from skorch import NeuralNetClassifier\n",
    "import scipy.stats\n",
    "\n",
    "\n",
    "class TwoLayerNN(nn.Module): #feel free to change the inputs it takes\n",
    "    def __init__(self, input_dim, hidden_units=20, output_classes=20, nonlin=nn.ReLU(), dropout=.5):  \n",
    "        super(TwoLayerNN, self).__init__()\n",
    "        self.dense0 = nn.Linear(input_dim, hidden_units)\n",
    "        self.nonlin = nonlin\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.dense1 = nn.Linear(hidden_units, hidden_units)\n",
    "        self.output = nn.Linear(hidden_units, output_classes)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, X, **kwargs):\n",
    "        X = self.nonlin(self.dense0(X))\n",
    "        X = self.dropout(X)\n",
    "        X = self.nonlin(self.dense1(X))\n",
    "        X = self.softmax(self.output(X))\n",
    "        return X\n",
    "\n",
    "torch.manual_seed(manual_seed)\n",
    "np.random.seed(manual_seed)\n",
    "torch.cuda.manual_seed(manual_seed)\n",
    "\n",
    "net = NeuralNetClassifier(\n",
    "    TwoLayerNN(input_dim=X_train.shape[1]),\n",
    "    optimizer=torch.optim.Adam,   #Adam can be a little faster than SGD, we'll discuss the difference in lecture\n",
    "    iterator_train__shuffle=True,\n",
    "    device=device  #'cpu' or 'cuda'\n",
    ")\n",
    "\n",
    "net.set_params(train_split=False, verbose=0)  \n",
    "\n",
    "params = {\n",
    "    #task specific\n",
    "    'module__input_dim': [X_train.shape[1]],\n",
    "    'module__output_classes': [20],\n",
    "    #training hyperparameters\n",
    "    'lr': [.01],\n",
    "    'max_epochs': [20],\n",
    "    'optimizer__weight_decay': scipy.stats.loguniform(.000001,.01) ,\n",
    "    #model architecture hyperparameters \n",
    "    'module__hidden_units': scipy.stats.randint(low=50,high=500) ,\n",
    "    'module__nonlin': [nn.ReLU(),nn.Tanh()],\n",
    "    'module__dropout': scipy.stats.uniform(),\n",
    "}\n",
    "rs = RandomizedSearchCV(net, params, refit=True, cv=2, n_iter=20, random_state=manual_seed, scoring='accuracy', verbose=2)  \n",
    "\n",
    "rs.fit(torch.from_numpy(X_train.todense()).float(), torch.tensor(y_train,dtype=torch.long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lr': 0.01, 'max_epochs': 20, 'module__dropout': 0.8418717674059527, 'module__hidden_units': 388, 'module__input_dim': 101631, 'module__nonlin': ReLU(), 'module__output_classes': 20, 'optimizer__weight_decay': 5.946138722551874e-06}\n",
      "0.6776560014141771\n"
     ]
    }
   ],
   "source": [
    "print(rs.best_params_)\n",
    "print(rs.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6500265533722783\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "pred = rs.best_estimator_.predict(torch.from_numpy(X_test.todense()).float())\n",
    "acc = metrics.accuracy_score(y_test, pred)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "###Copy of outputs from teammates if you didn't run this section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T1 Hyperparameter Optimization Reflection\n",
    "rubric={reasoning:2}  \n",
    "Reflect on the process to get this basic Neural Network to work. Were you able to get it to perform better than our baseline SVM? How \"easy\"/\"hard\" was it to get to work well? What might you also consider (but perhaps didn't have time) to try to improve it?\n",
    "\n",
    "**Example of something you could have discussed:   We found that randomized search provides an easier means of fine tuning the hyperparameters of neural nets compared to grid search. Because there are so many potential combinations of hyper parameters, using a random search can cover greater \"area\" in the search than grid search, specifically the ability to search over a distribution rather than sets of values can discover optima outside of the standard \"grid\". Hand tuning the network was difficult because having an intuition for a good size for the network only really came after running a few times, similarly some choices we made didn't seem to have a clear impact, which makes it a little frustrating to feel like you're backtracking.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Tensor Operations (Pytorch Intro)\n",
    "\n",
    "We'll primarily be using Pytorch for coding neural nets in this class (as well as COLX 531 and some others), this exercise is here to give an introduction to the basic operations that are performed on the tensors that pytorch uses. Pytorch tensors are basically numpy arrays, with the advantage that they can be loaded onto GPU to perform extremely fast parallel calculations. Remember in DSCI 512 the speedup from parallelizing code? Deep learning utilizes this in spades!\n",
    "\n",
    "### 1.1 Write code that creates a tensor, **X** of size $5 \\times 5$ containing longs with values initialized to ones. \n",
    "rubric={accuracy:1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here\n",
    "x = torch.tensor((),dtype=torch.long).new_ones((5,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Write code that takes the tensor, **X** (from the previous question 1.1) and sets the values along the diagonal to two.\n",
    "rubric={accuracy:1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2, 1, 1, 1, 1],\n",
       "        [1, 2, 1, 1, 1],\n",
       "        [1, 1, 2, 1, 1],\n",
       "        [1, 1, 1, 2, 1],\n",
       "        [1, 1, 1, 1, 2]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code goes here\n",
    "\n",
    "mask = torch.eye(5,5).bool()  #create a mask of the diagonal\n",
    "x.masked_fill_(mask,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Write code that takes the tensor, **X** (from the previous question 1.2), squares all the values in **X**, sums all the squared values in **X** and prints the square root of this sum? (L2-norm) \n",
    "rubric={accuracy:1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(6.3246, dtype=torch.float64)\n",
      "tensor(6.3246, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# your code goes here\n",
    "#need to convert to a floating point type\n",
    "x = x.double()\n",
    "print(torch.sqrt(torch.sum(torch.mul(x,x))))\n",
    "\n",
    "\n",
    "#to check:\n",
    "print(torch.norm(x)) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Given the following two tensors, **X** $\\in \\mathcal{R}^{4\\times4}$ and $\\textbf{Y} \\in \\mathcal{R}^{4\\times4}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9786, 0.3998, 0.7621, 0.0330],\n",
      "        [0.4713, 0.0497, 0.1247, 0.4957],\n",
      "        [0.5379, 0.8330, 0.0382, 0.4521],\n",
      "        [0.4375, 0.9377, 0.5235, 0.5487]])\n",
      "tensor([[0.5677, 0.9688, 0.5192, 0.4743],\n",
      "        [0.6601, 0.4801, 0.5510, 0.7869],\n",
      "        [0.2726, 0.8603, 0.2272, 0.2190],\n",
      "        [0.4126, 0.3936, 0.4801, 0.9458]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(manual_seed)\n",
    "\n",
    "X = torch.rand(4,4)\n",
    "print(X)\n",
    "Y = torch.rand(4,4)\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.1 Write code that performs standard matrix multiplication, multiply **X** and **Y** without changing their values and prints the result.\n",
    "rubric={accuracy:1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0409, 1.8087, 0.9173, 0.9769],\n",
      "        [0.5389, 0.7828, 0.5384, 0.7588],\n",
      "        [1.0522, 1.1318, 0.9640, 1.3466],\n",
      "        [1.2365, 1.5404, 1.1262, 1.5790]])\n",
      "tensor([[1.0409, 1.8087, 0.9173, 0.9769],\n",
      "        [0.5389, 0.7828, 0.5384, 0.7588],\n",
      "        [1.0522, 1.1318, 0.9640, 1.3466],\n",
      "        [1.2365, 1.5404, 1.1262, 1.5790]])\n"
     ]
    }
   ],
   "source": [
    "# your code goes here\n",
    "\n",
    "print(torch.matmul(X,Y))\n",
    "#or...\n",
    "print(X@Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here\n",
    "\n",
    "print(torch.matmul(X,Y))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.2 Write code that performs standard addition of two matrices, add **X** and **Y** without changing their values and prints the result.\n",
    "rubric={accuracy:1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.5463, 1.3686, 1.2813, 0.5073],\n",
      "        [1.1314, 0.5298, 0.6757, 1.2826],\n",
      "        [0.8105, 1.6933, 0.2654, 0.6711],\n",
      "        [0.8501, 1.3313, 1.0036, 1.4945]])\n"
     ]
    }
   ],
   "source": [
    "# your code goes here\n",
    "print(X + Y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.3 Write code that subtracts matrix **Y** from **X** without changing their values and prints the result.\n",
    "rubric={accuracy:1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.4109, -0.5690,  0.2429, -0.4413],\n",
      "        [-0.1889, -0.4304, -0.4264, -0.2912],\n",
      "        [ 0.2652, -0.0274, -0.1889,  0.2331],\n",
      "        [ 0.0249,  0.5441,  0.0434, -0.3971]])\n"
     ]
    }
   ],
   "source": [
    "# your code goes here\n",
    "\n",
    "print(X - Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.4 Write code that performs standard matrix multiplication, multiply **X** and **Y** and placing the results directly in **X** (modifying **X**) and prints the result.\n",
    "rubric={accuracy:1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9786, 0.3998, 0.7621, 0.0330],\n",
      "        [0.4713, 0.0497, 0.1247, 0.4957],\n",
      "        [0.5379, 0.8330, 0.0382, 0.4521],\n",
      "        [0.4375, 0.9377, 0.5235, 0.5487]])\n",
      "tensor([[1.0409, 1.8087, 0.9173, 0.9769],\n",
      "        [0.5389, 0.7828, 0.5384, 0.7588],\n",
      "        [1.0522, 1.1318, 0.9640, 1.3466],\n",
      "        [1.2365, 1.5404, 1.1262, 1.5790]])\n"
     ]
    }
   ],
   "source": [
    "# your code goes here\n",
    "\n",
    "#note matmul is usually not an in-place operation, think about why that is.\n",
    "print(X)\n",
    "X = torch.matmul(X,Y)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Given the following tensor, **X** $\\in \\mathcal{R}^{5\\times3}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9786, 0.3998, 0.7621],\n",
      "        [0.0330, 0.4713, 0.0497],\n",
      "        [0.1247, 0.4957, 0.5379],\n",
      "        [0.8330, 0.0382, 0.4521],\n",
      "        [0.4375, 0.9377, 0.5235]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(manual_seed)\n",
    "X = torch.rand(5,3)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5.1 Write code to print all the elements in the last row of **X**.\n",
    "rubric={accuracy:1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4375, 0.9377, 0.5235])\n"
     ]
    }
   ],
   "source": [
    "# your code goes here\n",
    "\n",
    "print(X[4,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5.2 Write code to print all the elements in the middle column of **X**.\n",
    "rubric={accuracy:1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.7058, 0.0772, 0.5331, 0.4545, 0.5159])\n"
     ]
    }
   ],
   "source": [
    "# your code goes here\n",
    "\n",
    "print(X[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5.3 Write code to create a 3D tensor of size $1 \\times 5 \\times 3$ using the $5 \\times 3$ values from **X** (unsqueeze operation)\n",
    "rubric={accuracy:1}\n",
    "\n",
    "(You'll often need to \"squeeze\" or \"unsqueeze\" tensors to make sure that the dimensions are correct for certain parts of your model.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here\n",
    "\n",
    "y = X.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5.4 Write code that converts the 3D tensor (created in the previous question (c)) back into 2D tensor (of size $5 \\times 3$). (squeeze operation)\n",
    "rubric={accuracy:1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here\n",
    "\n",
    "y = y.squeeze(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Putting the \"Multi\" in Multilayer Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In T1 you did hyper parameters optimization on a 2 layer neural network. For this assignment, you'll be using the same dataset (20 Newsgroups) but you'll need to build a model that can support an arbitrary number of layers (rather than just two)! There are two tricks to help out with this, the first is *nn.sequential* which allows you to stack pytorch modules together in a *cascade* fashion (*cascade* here meaning one thing passed to another like a pipeline). The other trick is *nn.modulelist* which will allow us to keep track of lists of modules, which we can then iterate through to build out network and perform *forward* passes through the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### nn.Sequential example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2510, 0.1498, 0.1444, 0.1669, 0.2878],\n",
      "        [0.2776, 0.1573, 0.1573, 0.1692, 0.2386],\n",
      "        [0.2137, 0.2039, 0.1484, 0.1878, 0.2462],\n",
      "        [0.2488, 0.1647, 0.1614, 0.1614, 0.2638],\n",
      "        [0.1998, 0.1814, 0.1532, 0.1532, 0.3123]], grad_fn=<SoftmaxBackward>)\n"
     ]
    }
   ],
   "source": [
    "#Make sure you run these each time before you (re)initialize the network or data!!! \n",
    "torch.manual_seed(manual_seed)\n",
    "np.random.seed(manual_seed)\n",
    "torch.cuda.manual_seed(manual_seed)\n",
    "\n",
    "\n",
    "## Fake data for testing\n",
    "x = torch.rand((5,10))\n",
    "\n",
    "\n",
    "## Building the model.  \n",
    "## Note: to use inside a larger model, set it as:  self.name_of_layer  = nn.Sequential(...)\n",
    "## in the initialization function\n",
    "\n",
    "example_model = nn.Sequential(\n",
    "          nn.Linear(10,5),\n",
    "          nn.ReLU(),\n",
    "          nn.Softmax(dim=-1)\n",
    "        )\n",
    "\n",
    "## Forward pass:\n",
    "\n",
    "output = example_model(x)\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 A 5 layer MLP using nn.Sequential\n",
    "rubric={accuracy:2, quality:2}\n",
    "\n",
    "Using nn.Sequential build a network with the following parameters: 5 Hidden Layers, each with 40 hidden units, using a Tanh activation function between layers, 20% dropout on each layer, and finally a softmax output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "### My MLP Here!\n",
    "\n",
    "class MLP_Seq(nn.Module): #feel free to change the inputs it takes\n",
    "    def __init__(self, input_dim, hidden_units=20, output_classes=20, nonlin=nn.ReLU(), dropout=.5):  \n",
    "        super(MLP_Seq, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_dim,hidden_units),\n",
    "            nonlin,\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(hidden_units,hidden_units),\n",
    "            nonlin,\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(hidden_units,hidden_units),\n",
    "            nonlin,\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(hidden_units,hidden_units),\n",
    "            nonlin,\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(hidden_units,hidden_units),\n",
    "            nonlin,\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(hidden_units,output_classes),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "\n",
    "    def forward(self, X, **kwargs):\n",
    "        return self.layers(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Training and testing our 5 layer MLP\n",
    "rubric={accuracy:1, quality:1}\n",
    "\n",
    "Now train the network on the 20 Newgroups training data, and report the Test set accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m3.3822\u001b[0m       \u001b[32m0.0521\u001b[0m        \u001b[35m3.2230\u001b[0m  4.9509\n",
      "      2        \u001b[36m3.2460\u001b[0m       0.0517        \u001b[35m3.1112\u001b[0m  4.0364\n",
      "      3        3.2678       \u001b[32m0.0526\u001b[0m        3.1324  4.0514\n",
      "      4        \u001b[36m3.2426\u001b[0m       0.0499        3.2169  3.9956\n",
      "      5        \u001b[36m3.2405\u001b[0m       \u001b[32m0.0614\u001b[0m        3.2076  3.9814\n",
      "      6        \u001b[36m3.2279\u001b[0m       0.0411        3.3399  4.0851\n",
      "      7        3.2493       0.0521        3.2356  3.9920\n",
      "      8        3.2767       0.0526        3.2047  4.0560\n",
      "      9        3.3164       0.0526        3.2044  3.9200\n",
      "     10        3.2562       0.0521        3.1390  3.9988\n",
      "0.052177376526818905\n"
     ]
    }
   ],
   "source": [
    "### My MLP training / testing code!\n",
    "from sklearn import metrics\n",
    "from skorch import NeuralNetClassifier\n",
    "\n",
    "torch.manual_seed(manual_seed)\n",
    "np.random.seed(manual_seed)\n",
    "torch.cuda.manual_seed(manual_seed)\n",
    "\n",
    "net = NeuralNetClassifier(\n",
    "    MLP_Seq(input_dim=X_train.shape[1], hidden_units=40,nonlin=nn.Tanh(),dropout=.2),\n",
    "    max_epochs=10,\n",
    "    lr=0.1,\n",
    "    optimizer=torch.optim.Adam,   \n",
    "    optimizer__weight_decay=0.001,  #roughly equivalent to L2 regularization\n",
    "    iterator_train__shuffle=True,\n",
    "    device=device  #'cpu' or 'cuda'\n",
    ")\n",
    "\n",
    "net.fit(torch.from_numpy(X_train.todense()).float(), torch.tensor(y_train,dtype=torch.long))\n",
    "\n",
    "pred = net.predict(torch.from_numpy(X_test.todense()).float())\n",
    "acc = metrics.accuracy_score(y_test, pred)\n",
    "print(acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you look at this and think it's garbage, think back to T1, we had to tune the hyperparameters of the network to get any good performance and we haven't done that at all with this model! We are also introducing 9 new hyperparameter choices: 3 for the choices of hidden units on the 3 new layers, 3 for the choices of activation functions of the 3 new layers, 3 for the choices of dropout on the 3 new layers. If we only looked at 2 choices for each of these new ones, it would take us 2^9 times longer to perform a grid search of these parameters! You could, however, still do a randomized search and expect to find a fairly reasonable performance configuration.  \n",
    "\n",
    "In practice we often see major companies Google/Facebook etc. to publish results that have known good hyperparameter configurations for common models (e.g. Transformer), these configurations often just get used \"as is\" by other researchers to save on the enormous amount of time in doing neural architecture search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### nn.ModuleList example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1650, 0.2360, 0.2021, 0.2144, 0.1825],\n",
      "        [0.1614, 0.2381, 0.2030, 0.2141, 0.1834],\n",
      "        [0.1638, 0.2382, 0.2023, 0.2146, 0.1811],\n",
      "        [0.1626, 0.2370, 0.2024, 0.2158, 0.1822],\n",
      "        [0.1650, 0.2403, 0.2015, 0.2112, 0.1820]], grad_fn=<SoftmaxBackward>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Make sure you run these each time before you (re)initialize the network or data!!! \n",
    "torch.manual_seed(manual_seed)\n",
    "np.random.seed(manual_seed)\n",
    "torch.cuda.manual_seed(manual_seed)\n",
    "\n",
    "\n",
    "## Fake data for testing\n",
    "x = torch.rand((5,10))\n",
    "\n",
    "layers = nn.ModuleList()   ## Again you'll need to assign as: self.layers = nn.ModuleList()   in the initialization function\n",
    "## Two layers\n",
    "for i in range(2):\n",
    "    layers.append(nn.Linear(10,10))\n",
    "    layers.append(nn.ReLU())\n",
    "layers.append(nn.Linear(10,5))  #output layer\n",
    "layers.append(nn.Softmax(dim=-1))\n",
    "\n",
    "## Forward pass:\n",
    "for layer in layers:\n",
    "    x = layer(x)\n",
    "\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Arbitrary depth MLP using nn.ModuleList()\n",
    "rubric={accuracy:2, quality:2}\n",
    "\n",
    "Using nn.ModuleList to build a MLP class that takes input for number of layers, input/output dimensions, hidden units, activation function, and dropout percent and builds the corresponding network.  Output should pass through appropriate layers to a final softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "### My MLP Here!\n",
    "class MLP_Mod(nn.Module): \n",
    "    def __init__(self, input_dim,num_layers=5, hidden_units=20, output_classes=20, nonlin=nn.ReLU(), dropout=.5):  \n",
    "        super(MLP_Mod, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i in range(num_layers):\n",
    "            if i==0:\n",
    "                self.layers.append(nn.Linear(input_dim,hidden_units))\n",
    "            else:\n",
    "                self.layers.append(nn.Linear(hidden_units,hidden_units))\n",
    "            self.layers.append(nonlin)\n",
    "            self.layers.append(nn.Dropout(p=dropout))\n",
    "        self.layers.append(nn.Linear(hidden_units,output_classes))\n",
    "        self.layers.append(nn.Softmax(dim=-1))\n",
    "\n",
    "    def forward(self, X, **kwargs):\n",
    "        for layer in self.layers:\n",
    "            X = layer(X)\n",
    "        return X\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Training and testing a 6 layer MLP\n",
    "rubric={accuracy:1, quality:1}\n",
    "\n",
    "Using the class you built in 2.3, build and train a 6 layer MLP network on the 20 Newgroups training data, and report the Test set accuracy. You should set hidden units per layer to 50 units, dropout percent to 20%, and use sigmoid as your activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m3.0247\u001b[0m       \u001b[32m0.0526\u001b[0m        \u001b[35m2.9990\u001b[0m  4.6299\n",
      "      2        \u001b[36m3.0026\u001b[0m       0.0526        \u001b[35m2.9967\u001b[0m  4.2137\n",
      "      3        3.0048       0.0521        3.0003  4.1292\n",
      "      4        3.0082       0.0526        2.9969  4.2345\n",
      "      5        3.0027       0.0521        3.0210  4.4798\n",
      "      6        3.0129       0.0526        3.0039  4.4293\n",
      "      7        3.0138       0.0521        3.0064  4.2616\n",
      "      8        3.0046       0.0526        \u001b[35m2.9961\u001b[0m  4.2798\n",
      "      9        3.0063       0.0486        3.0011  4.7234\n",
      "     10        3.0046       0.0526        2.9971  4.3684\n",
      "0.0524429102496017\n"
     ]
    }
   ],
   "source": [
    "### My MLP training / testing code!\n",
    "\n",
    "from sklearn import metrics\n",
    "from skorch import NeuralNetClassifier\n",
    "\n",
    "torch.manual_seed(manual_seed)\n",
    "np.random.seed(manual_seed)\n",
    "torch.cuda.manual_seed(manual_seed)\n",
    "\n",
    "net = NeuralNetClassifier(\n",
    "    MLP_Mod(input_dim=X_train.shape[1],num_layers=6, hidden_units=50,nonlin=nn.Sigmoid(),dropout=.2),\n",
    "    max_epochs=10,\n",
    "    lr=0.1,\n",
    "    optimizer=torch.optim.Adam,   \n",
    "    optimizer__weight_decay=0.001,  #roughly equivalent to L2 regularization\n",
    "    iterator_train__shuffle=True,\n",
    "    device=device  #'cpu' or 'cuda'\n",
    ")\n",
    "\n",
    "net.fit(torch.from_numpy(X_train.todense()).float(), torch.tensor(y_train,dtype=torch.long))\n",
    "\n",
    "pred = net.predict(torch.from_numpy(X_test.todense()).float())\n",
    "acc = metrics.accuracy_score(y_test, pred)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Garbage yet again! The B-plot of this weeks DSCI 572 episode is \"Hyperparameter tuning is really important in deep learning!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Very-Short answer questions\n",
    "\n",
    "(Double-click each question block and place your answer at the end of the question) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 What is NumPy? What are the differences between PyTorch's Tensor and NumPy Array?\n",
    "rubric={reasoning:1}\n",
    "NumPy is a Python scientific computing package with support for linear algebra and some machine learning tools. The main difference between PyTorch Tensors and Numpy high dimension arrays is  basically the names that they are called, and the fact that PyTorch can load its tensors onto CUDA capable devices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 What is the key difference between ``torch.LongTensor`` and ``torch.cuda.LongTensor``?\n",
    "rubric={reasoning:1}\n",
    "\n",
    "The cuda version has been loaded onto a GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 What is the default data type of a PyTorch tensor?\n",
    "rubric={accuracy:1}\n",
    "\n",
    "float32  You can check this yourself as below by creating a tensor (without data).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(())\n",
    "print(x.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 What is ``autograd`` in PyTorch? How is it related to computational graph?\n",
    "rubric={reasoning:1}\n",
    "\n",
    "Autograd allows PyTorch to automatically calculate the gradient of functions. In our computational graph certain nodes can store the gradient as they are changed, thereby allowing for fast computation of backward propagation to learn the weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 What is SGD? How is it different from Gradient Descent? Finally what role SGD plays in building machine learning models?\n",
    "rubric={reasoning:1}\n",
    "\n",
    "Stoachastic Gradient Descent is a technique to train a machine learning model by sampling from the training set, calculating the loss of a sample and slightly update the weights of the model based on the loss/gradient from that sample. Generally in SGD you slow down as you see more samples, eventually converging on a stable set of training weights. It's an extremely useful tool in training many machine learning models, particularly neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Not Graded)  Linearities, Non-linearities, Loss functions by Hand\n",
    "\n",
    "This is a quick linear algebra review exercise, if you're a little rusty, or if you haven't taken a linear algebra class, this is a peak under the hood to show the math that Pytorch is doing when it's calculating tensor operations.  (See the separate solutions in the github lab folder to check your work)\n",
    "\n",
    "Sample question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([168.], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "linear_layer = torch.nn.Linear(5, 1)\n",
    "linear_layer.weight.data[0] = torch.tensor([1, 2, 3, 4, 5]) # sets the weight value\n",
    "linear_layer.bias.data = torch.tensor([3]).float() # sets the bias value\n",
    "model_out = linear_layer(torch.tensor([0, 10, 20, 15, 5]).float())\n",
    "print(model_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the values in **model\\_out** by hand. Show your work.\n",
    "\n",
    "Sample answer: (write it in markdown, not as code. if you don't like markdown, you can write the steps in a piece of paper, take a photo and attach an image in the answer block)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "your answer goes here:\n",
    "\n",
    "$model\\_out = A x + b = [1, 2, 3, 4, 5] * [0, 10, 20, 15, 5] + 3 = (1*0 + 2*10 + 3*20 + 4*15 + 5*5) + 3 = 165 + 3 = 168 $\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([165.], grad_fn=<SqueezeBackward3>)\n"
     ]
    }
   ],
   "source": [
    "linear_layer = torch.nn.Linear(5, 1, bias=False)\n",
    "linear_layer.weight.data[0] = torch.tensor([1, 2, 3, 4, 5]) # sets the weight value\n",
    "model_out = linear_layer(torch.tensor([0, 10, 20, 15, 5]).float())\n",
    "print(model_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the values in **model\\_out** by hand. Show your work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "your answer goes here (double-click this block to edit):\n",
    "\n",
    "(There is small error in the example, which we can fix by just transposing the $x$ vector from a row ($1 \\times 5$) vector to a column ($5 \\times 1$) vector).  I'll also start using the convention of calling our weight matrix $W$, this is the same as the $A$ matrix in the example, but a more standard convention.\n",
    "$model\\_out = W x^T + b = [1, 2, 3, 4, 5] * [0, 10, 20, 15, 5]^T = (1*0 + 2*10 + 3*20 + 4*15 + 5*5)  = 165  = 165 $\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "your answer goes here (double-click this block to edit):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.0000, 0.8808], grad_fn=<SigmoidBackward>)\n"
     ]
    }
   ],
   "source": [
    "linear_layer = torch.nn.Linear(5, 2)\n",
    "linear_layer.weight.data[0] = torch.tensor([1, 2, 3, 4, 5]) # sets the weight value\n",
    "linear_layer.weight.data[1] = torch.tensor([1, 0, 0, 0, 1]) # sets the weight value\n",
    "linear_layer.bias.data = torch.tensor([1]).float() # sets the bias value\n",
    "model_out = linear_layer(torch.tensor([0, 10, 20, 15, 1]).float())\n",
    "sigmoid_out = torch.nn.Sigmoid()(model_out)\n",
    "print(sigmoid_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the values in **sigmoid\\_out** by hand. Show your work.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "your answer goes here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "your answer goes here:\n",
    "$W$ is a $2 \\times 5$ matrix with each row consisting of the weights of a particular node.\n",
    "\n",
    "We've defined $x$ as a $1 \\times 5$ row vector, which we'll again need to transpose to match dimensions with $W$.\n",
    "\n",
    "$Wx^T$ after multiplication should have dimensions $2 \\times 1$, which is a column vector, but PyTorch unfortunately adopts a row notation for convenience, so just think of the output as being transposed (in the next problem I'll show how we can re-write things to better align with pytorch)\n",
    "\n",
    "$W x^T + b = [[1, 2, 3, 4, 5],[1,0,0,0,1]] * [0, 10, 20, 15, 1]^T + [1,1]^T $\n",
    "\n",
    "$= [(1*0 + 2*10 + 3*20 + 4*15 + 5*1) + 1,(1*0+0*10+0*20+0*15+1*1)+1]^T = [146,2]^T $  \n",
    "\n",
    "now take the sigmoid of this (applying it element-wise)\n",
    "\n",
    "$sigmoid([146,2]^T) = [\\frac{1}{1+exp(-146)},\\frac{1}{1+exp(-2)}]^T = [1.0,0.88]^T$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000],\n",
      "        [0.9933, 0.0067]], grad_fn=<SoftmaxBackward>)\n"
     ]
    }
   ],
   "source": [
    "linear_layer = torch.nn.Linear(5, 2)\n",
    "linear_layer.weight.data[0] = torch.tensor([1, 2, 3, 4, 5]) # sets the weight value\n",
    "linear_layer.weight.data[1] = torch.tensor([1, 3, 0, 0, 10]) # sets the weight value\n",
    "linear_layer.bias.data = torch.tensor([3]).float() # sets the bias value\n",
    "model_out = linear_layer(torch.tensor([[100, 10, 20, 15, 1], [10, 5, 2, 1, 0]]).float())\n",
    "softmax_out = torch.nn.Softmax(dim=1)(model_out)\n",
    "print(softmax_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the values in **softmax\\_out** by hand. Show your work.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "your answer goes here:\n",
    "\n",
    "For convenience, with batches I tend to personally prefer using a row notation for the matrix of $X$. This happens to align with how pytorch does things, but if you ended up getting your output transposed, just note the convention difference.  In order for row matrix form of $X$ to work, the equation is going to change slightly. Instead of $Wx^T + b$ I'll use $XW^T + b$\n",
    "\n",
    "With $X$ as a $2 \\times 5$ matrix consisting of the rows of training samples.\n",
    "$W^T$ $5 \\times 2$ matrix consisting of the weights for each node now transposed into the columns of the matrix.\n",
    "\n",
    "I'll also use a $2 \\times 2$ matrix of ones to deal with the bias being added (since it is the same for each node)\n",
    "\n",
    "our output will thus be a $2 \\times 2$ matrix with each row consisting of the softmax applied to a particular training sample.\n",
    "\n",
    "$XW^T + b = [[100, 10, 20, 15, 1], [10, 5, 2, 1, 0]]\\times [[1,2,3,4,5],[1,3,0,0,10]]^T + 3 \\times OnesMatrix$\n",
    "\n",
    "$= [[ (100*1+10*2 + 20*3 + 15*4 + 1*5 +3), (100*1 + 10*3 + 20*0 + 15 * 0 + 1*10  +3)],[(10*1+5*2+2*3+1*4+0*5 +3), (10*1+5*3+2*0+1*0+0*0  +3)]]$\n",
    "\n",
    "$ = [[248,143],[33,28]]$\n",
    "\n",
    "Now apply softmax to each row:\n",
    "$softmax([[248,143],[33,28]])  = [[\\frac{exp(248)}{exp(248)+exp(143)}, \\frac{exp(143)}{exp(248)+exp(143)}],[\\frac{exp(33)}{exp(33)+exp(28)},\\frac{exp(28)}{exp(33)+exp(28)}]] = [[1.0,0.0],[0.9933,0.0067]]$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "your answer goes here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(7.7500, grad_fn=<MseLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "linear_layer = torch.nn.Linear(5, 2)\n",
    "linear_layer.weight.data[0] = torch.tensor([1, 2, 3, 4, 5]) # sets the weight value\n",
    "linear_layer.weight.data[1] = torch.tensor([1, 3, 0, 0, 10]) # sets the weight value\n",
    "linear_layer.bias.data = torch.tensor([3]).float() # sets the bias value\n",
    "model_out = linear_layer(torch.tensor([[100, 10, 20, 15, 1], [10, 5, 2, 1, 0]]).float())\n",
    "criterion = torch.nn.MSELoss()\n",
    "loss = criterion(model_out, torch.tensor([[245, 140], [30, 30]]).float())\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "your answer goes here:\n",
    "\n",
    "Same idea as 2.3, but now we'll apply a loss function, the Mean Squared Error, defined as $\\frac{1}{n}\\sum_i^n(\\tilde{y_i} -y_i)^2$ where $\\tilde{y_i}$ is the prediction for one of our test samples, with $n$ being the number of samples.\n",
    "\n",
    "First let's compute the $modelout$ for our inputs:\n",
    "\n",
    "$XW^T + b  = [[100,10,20,15,1],[10,5,2,1,0]] \\times [[1,2,3,4,5],[1,3,0,0,10]]^T  + 3 OnesMatrix$\n",
    "\n",
    "$= [[(100*1+10*2+20*3+15*4+1*5+3),(100*1+10*3+20*0+15*0+1*10+3)],[(10*1+5*2+2*3+1*4+0*5+3),(10*1+5*3+2*0+1*0+0*10+3)]] $\n",
    "\n",
    "$= [[248,143],[33,28]]$\n",
    "\n",
    "Now let's apply the loss:\n",
    "$\\frac{1}{n}\\sum_i^n(\\tilde{y_i} -y_i)^2$\n",
    "\n",
    "$\\frac{1}{2}(mean([(248-245),(143-140)]^2) + mean([(33-30),(28-30)]^2) = \\frac{1}{2} (\\frac{9+9}{2} +\\frac{9+4}{2}) = 7.75$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
