{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework 2 - Supervised Learning II - MDS Computational Linguistics\n",
    "\n",
    "### Assignment Topics\n",
    "- More linearities, non-linearities\n",
    "- Embedding layer\n",
    "- Neural Network for sentiment analysis\n",
    "- Very-short answer questions\n",
    "\n",
    "### Software Requirements\n",
    "- Python (>=3.6)\n",
    "- PyTorch (>=1.2.0) \n",
    "- Jupyter (latest)\n",
    "\n",
    "### Submission Info.\n",
    "- Due Date: 1/23/21 11:59pm Pacific Time\n",
    "\n",
    "## Getting Started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all necessary imports\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# set the seed (allows reproducibility of the results)\n",
    "manual_seed = 572\n",
    "torch.manual_seed(manual_seed) # allows us to reproduce results when using random generation on the cpu\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # creates the device object, either GPU (cuda) or CPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tidy Submission\n",
    "\n",
    "rubric={mechanics:1}\n",
    "\n",
    "To get the marks for tidy submission:\n",
    "- Submit the assignment by filling in this jupyter notebook with your answers embedded\n",
    "- Be sure to follow the [general lab instructions](https://ubc-mds.github.io/resources_pages/general_lab_instructions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T2 Additional NN practice\n",
    "\n",
    "Some Pytorch models expect certain dimensions of input. This can be tricky if they get mixed in with other model that expect something different (e.g. CNNs with RNNs say). Let's look at dealing with this: In the model below complete the forward pass by ensuring that the model passes the data correctly through the following CNN -> ReLU -> RNN -> Linear Layer -> Softmax.\n",
    "\n",
    "Double check the Pytorch documentation to find the input/output dimensions for each of these modules! https://pytorch.org/docs/stable/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Dim: torch.Size([7, 10, 20])\n",
      "CNN Layer: torch.Size([7, 5, 10])\n",
      "RNN Layer: torch.Size([10, 7, 11])\n",
      "FF Layer: torch.Size([7, 10, 9])\n",
      "torch.Size([7, 10, 9])\n"
     ]
    }
   ],
   "source": [
    "#Assume (Batch,Length,Embeddings)\n",
    "x = torch.rand((7,10,20))\n",
    "\n",
    "class DimensionTestModel(nn.Module):\n",
    "  \n",
    "  def __init__(self, input_size, filters, hidden, output_size):\n",
    "    super(DimensionTestModel, self).__init__()\n",
    "    self.cnn = nn.Conv1d(input_size, filters, kernel_size=3, padding =1) # we'll look more at CNNs a little next lab and COLX 585\n",
    "    self.activation = nn.ReLU()\n",
    "    self.rnn = nn.RNN(filters, hidden)  # RNNs in detail next week\n",
    "    self.linear_layer = nn.Linear(hidden, output_size) \n",
    "    self.softmax_layer = nn.LogSoftmax(dim=1)\n",
    "  \n",
    "  def forward(self, x):   #PRINT STATEMENTS ADDED TO ILLUSTRATE HOW THE DIMENSIONS CHANGE AS IT GOES THROUGH\n",
    "    # Your Code Here\n",
    "    print(\"Input Dim: \" + str(x.shape))\n",
    "    x = self.cnn(x.permute(0,2,1))\n",
    "    print(\"CNN Layer: \" + str(x.shape))\n",
    "\n",
    "    x = self.activation(x)\n",
    "    x, _ = self.rnn(x.permute(2,0,1)) \n",
    "    print(\"RNN Layer: \" + str(x.shape))\n",
    "\n",
    "    x = self.linear_layer(x.permute(1,0,2))\n",
    "    print(\"FF Layer: \" + str(x.shape))\n",
    "\n",
    "    x = self.softmax_layer(x)\n",
    "    # Your Code Here\n",
    "    return x\n",
    "\n",
    "model = DimensionTestModel(20,5,11,9)\n",
    "\n",
    "print(model(x).shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Permutations $[N,L,H] \\dashrightarrow [N,H,L] \\dashrightarrow [L,N,H] \\dashrightarrow [N,L,H]$  ending with the same dimensions as the start, but compressing the Hidden/Embedding size from 20 to 9.  \n",
    "\n",
    "Tricky things about this problem: You could have fed size $[L,N,H]$ into the linear layer (it only checks that the $H$ dimension matches), but this is incorrect as it would have applied the FF layer across the batch.  \n",
    "\n",
    "What does this network do?? Well it's just a made up network to test your ability to get dimensions right. But if we wanted to think about it:    \n",
    "\n",
    "The CNN layer will learn a representation with localized features (with kernel size 3, it can 'pay attention' to either side of each of the sequence segments), the RNN layer then can learn a bit more across the entire sequence, the linear layer then changes the hidden dimension size to match our output classes, finally the softmax (dim=1) applies a probability distribution across the entire sequence. Which would mean finding the part of the sequence most likely for each of the 9 output classes.  \n",
    "\n",
    "Probably not useful as it is, but if you squint and replace the linear and softmax layer with something called connectionist temporal classification, you would have the basic skeleton of an early 2010s automatic speech recognition model.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1 Initializing Weights\n",
    "#### 1.1 Default Initialization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer_1 weight:\n",
      " tensor([[-0.1824,  0.0148, -0.2221,  0.1687, -0.3811],\n",
      "        [ 0.3278, -0.3251, -0.3556, -0.2826,  0.2025],\n",
      "        [-0.1652,  0.1674, -0.3796, -0.2713, -0.1642],\n",
      "        [-0.0879, -0.3412,  0.2928, -0.1055,  0.1436]])\n"
     ]
    }
   ],
   "source": [
    "layer_1 = torch.nn.Linear(5, 4)\n",
    "print(\"layer_1 weight:\\n\", layer_1.weight.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From what distribution (and range) the default values in ``layer_1.weight.data`` are sampled from? \n",
    "(Hint: You can look at the [documentation](https://pytorch.org/docs/stable/nn.html#torch.nn.Linear) and/or [source code](https://pytorch.org/docs/stable/_modules/torch/nn/modules/linear.html#Linear) of ``nn.Linear``. And the symbols  $\\textit{U}$ and $\\mathcal{N}$ correspond to uniform and normal (Gaussian) distribution.)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**your answer goes here:**\n",
    "initialized from $ `\\textit{U}(-\\sqrt{k}, \\sqrt{k})` $, where\n",
    "            $ `k = \\frac{1}{\\text{in\\_features}}` $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Reproducibility\n",
    "\n",
    "What happens when you add torch.manual_seed(manual_seed) before the layer is created in the above code? Run it a few times with and without."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**your answer goes here:**\n",
    "\n",
    "Should keep the weights predictable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Write code to initialize parameters (weights and biases) in ``layer_1`` (previous queston, 1.1) with numbers sampled randomly from standard normal distribution (mean 0 and variance 1).\n",
    "\n",
    "Hint: Look at ``torch.randn`` function\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.8576,  2.1321, -0.5056, -0.7988, -0.9592],\n",
      "        [-1.2213, -0.9590,  1.0224, -1.1364, -0.3501],\n",
      "        [ 0.1233,  1.6076, -1.4483,  1.2933,  1.1161],\n",
      "        [ 0.3888,  1.3232,  1.1393,  0.8656,  0.5044]])\n",
      "tensor([[ 1.7822,  1.9736, -0.3101, -0.8211]])\n",
      "tensor([[-0.4055, -0.8368,  1.2277, -0.4297, -0.0306],\n",
      "        [-0.0894, -0.1965, -0.9713,  0.2790, -0.7587],\n",
      "        [ 0.5473,  0.4301,  0.8558,  1.6098, -1.1893],\n",
      "        [ 1.1677,  0.6220,  2.5737, -0.6239, -1.2965]])\n",
      "tensor([[-1.9029,  0.8260, -0.6644,  1.6663]])\n"
     ]
    }
   ],
   "source": [
    "layer_1.weight.data.normal_(0,1) # sets the values of weight matrix for 1st layer (W_1)\n",
    "layer_1.bias.data.normal_(0,1)\n",
    "\n",
    "print(layer_1.weight.data)\n",
    "print(layer_1.bias.data)\n",
    "#you could also solved this using torch.randn as:\n",
    "\n",
    "\n",
    "layer_1.weight.data =torch.randn(4,5)\n",
    "print(layer_1.weight.data)\n",
    "layer_1.bias.data = torch.randn(1,4)\n",
    "print(layer_1.bias.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Embedding layer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings_avg:\n",
      " tensor([[  2.0000,   1.0000,   1.5000],\n",
      "        [  5.5000,  10.5000,  15.5000]])\n"
     ]
    }
   ],
   "source": [
    "# the model\n",
    "embedding_model = nn.Embedding(4, 3) # 4 embeddings with each of three dimensions\n",
    "\n",
    "# set the weights for each of the four embedding\n",
    "embedding_model.weight.data = torch.tensor([[1., 2., 3.], [1., 1., 1.], [3., 0., 0.], [10., 20., 30.]])\n",
    "\n",
    "# data (2 examples each with two inputs)\n",
    "inputs = torch.tensor([[0, 2], [1, 3]])\n",
    "\n",
    "# forward propagation for computing average of input embeddings\n",
    "embeddings_out = embedding_model(inputs)\n",
    "embeddings_avg = embeddings_out.mean(1)\n",
    "print(\"embeddings_avg:\\n\", embeddings_avg.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the values in **embeddings_avg** by hand. Show your work.\n",
    "rubric={accuracy:2}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**your answer goes here:**\n",
    "\n",
    "The inputs select the appropriate weight rows in the embedding model so for input $[0,2]$ we get:  \n",
    "\n",
    "$([1,2,3]+[3,0,0])/2 = [2,1,1.5]$\n",
    "\n",
    "$[1,3]$ we get:  \n",
    "$([1,1,1]+[10,20,30])/2 = [5.5,10.5,15.5]$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Can you reimplement 2.1 by using [``nn.EmbeddingBag``](https://pytorch.org/docs/stable/nn.html#torch.nn.EmbeddingBag) instead of ``nn.Embedding`` by setting the appropriate mode? \n",
    "\n",
    "Hint: You can look at the [documentation](https://pytorch.org/docs/stable/nn.html#torch.nn.EmbeddingBag) and/or [source code](https://pytorch.org/docs/stable/_modules/torch/nn/modules/sparse.html#EmbeddingBag) of ``nn.EmbeddingBag``\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here\n",
    "\n",
    "# same as above but replace Embedding with the following\n",
    "embedding_model = nn.EmbeddingBag(4, 3, mode='mean')\n",
    "\n",
    "# and get rid of \n",
    "embeddings_avg = embeddings_out.mean(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Toy corpus worth 3 sentences (each row excluding the header corresponds to a sentence)\n",
    "\n",
    "|  sentence no. | sentence text |\n",
    "| --------------- | ------------------------------ |\n",
    "| 1  | UBC’s Master of Data Science in Computational Linguistics is the credential to set you apart. |\n",
    "| 2  | Offered at the Vancouver campus, this unique degree is tailored to those with a passion for language and data.|\n",
    "| 3  | Over 10 months, the program combines foundational data science courses with advanced computational linguistics courses—equipping graduates with the skills to turn language-related data into knowledge and to build AI that can interpret human language. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 In the tutorial, we constructed word to index mapping for a one sentence corpus. Write code to build word to index mapping for this toy corpus containing three sentences.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************  word2id dictionary  ********************\n",
      "{'data.': 0, 'credential': 1, 'science': 2, 'in': 3, 'for': 4, 'combines': 5, 'campus,': 6, 'you': 7, 'foundational': 8, 'apart.': 9, 'passion': 10, 'turn': 11, '10': 12, 'those': 13, 'Vancouver': 14, 'linguistics': 15, 'Computational': 16, 'set': 17, 'knowledge': 18, 'Linguistics': 19, 'and': 20, 'interpret': 21, 'of': 22, 'AI': 23, 'Science': 24, 'can': 25, 'to': 26, 'is': 27, 'Over': 28, 'months,': 29, 'courses—equipping': 30, 'Data': 31, 'Master': 32, 'program': 33, 'UBC’s': 34, 'data': 35, 'Offered': 36, 'computational': 37, 'human': 38, 'at': 39, 'skills': 40, 'this': 41, 'unique': 42, 'language': 43, 'that': 44, 'degree': 45, 'build': 46, 'courses': 47, 'tailored': 48, 'with': 49, 'a': 50, 'the': 51, 'advanced': 52, 'language.': 53, 'into': 54, 'graduates': 55, 'language-related': 56} 57\n"
     ]
    }
   ],
   "source": [
    "# let us construct the word to index mapping\n",
    "word2id = {}\n",
    "\n",
    "all_tokens = []\n",
    "sentences_list = [\"UBC’s Master of Data Science in Computational Linguistics is the credential to set you apart.\",\n",
    "            \"Offered at the Vancouver campus, this unique degree is tailored to those with a passion for language and data.\",\n",
    "            \"Over 10 months, the program combines foundational data science courses with advanced computational linguistics courses—equipping graduates with the skills to turn language-related data into knowledge and to build AI that can interpret human language.\"]\n",
    "for sentence in sentences_list:\n",
    "    for word in sentence.split():\n",
    "        all_tokens.append(word)\n",
    "types=list(set(all_tokens))\n",
    "\n",
    "for word in types:\n",
    "    if word not in word2id:\n",
    "        word2id[word] = len(word2id)\n",
    "print(\"*\" *20 ,\" word2id dictionary \", \"*\" *20)\n",
    "print(word2id,len(word2id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 In the tutorial, we constructed the train data (input and output for each training example) for a one sentence corpus. Write code that outputs the train data for CBOW model created from this toy corpus and prints the number of training examples. \n",
    "\n",
    "Note:\n",
    "- Assume the **window size to be 3**. \n",
    "- Use **truecase** of the words. \n",
    "- Use [white space tokenizer](https://kite.com/python/docs/nltk.WhitespaceTokenizer) to get the words from each sentence and no further preprocessing. \n",
    "- A training example is generated from a sentence and doesn't span across multiple sentences. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\"\"\"\n",
    "create a dataset reader\n",
    "\"\"\"\n",
    "class CBOWDataset(Dataset):\n",
    "  \"\"\" one-sentence dataset.\"\"\"\n",
    "  def __init__(self, window_size=2):\n",
    "    # read the corpus\n",
    "    corpus = [\"UBC’s Master of Data Science in Computational Linguistics is the credential to set you apart.\",\n",
    "            \"Offered at the Vancouver campus, this unique degree is tailored to those with a passion for language and data.\",\n",
    "            \"Over 10 months, the program combines foundational data science courses with advanced computational linguistics courses—equipping graduates with the skills to turn language-related data into knowledge and to build AI that can interpret human language.\"]\n",
    "\n",
    "    window = 2 * window_size + 1\n",
    "    token2id = {}\n",
    "    # populate the word to id mapping and generate train inputs/targets\n",
    "    # generate all training samples for this sentence\n",
    "    train_inputs, train_outputs = [], []\n",
    "    for sentence in corpus:\n",
    "      tokens = sentence.strip().split()\n",
    "      for token in tokens:\n",
    "        if token not in token2id.keys(): # add new word\n",
    "          token2id[token] = len(token2id) # new word index = length of token2id\n",
    "    \n",
    "      # map to index\n",
    "      tokens = [token2id[token] for token in tokens]\n",
    "    \n",
    "\n",
    "      for num_win in range(len(tokens) - window + 1):\n",
    "        cur_tokens = tokens[num_win:num_win + window]\n",
    "        tgt_token = cur_tokens[window_size]\n",
    "        train_inputs.append(cur_tokens[0:window_size]+cur_tokens[window_size+1:])\n",
    "        train_outputs.append(tgt_token)\n",
    "    self.token2id = token2id\n",
    "    \n",
    "    # set the vocab. size\n",
    "    self.vocab_size = len(token2id)\n",
    "    \n",
    "    # set the total number of training examples\n",
    "    self.n = len(train_inputs)\n",
    "    \n",
    "    # convert features and labels to torch.tensor\n",
    "    self.features = torch.LongTensor(train_inputs, device=device)\n",
    "    self.labels = torch.LongTensor(train_outputs, device=device)\n",
    "    \n",
    "  # return input and output of a single example\n",
    "  # Input: Feature vectors, where each vector corresponds to a tweet. \n",
    "  # Output: Labels, where each label is one index for each of our tags in the set {positive, negative, neutral}\n",
    "  def __getitem__(self, index):\n",
    "    return self.features[index], self.labels[index]\n",
    "  \n",
    "  # return the total number of examples\n",
    "  def __len__(self):\n",
    "    return self.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of samples in the dataset: 50\n",
      "feature matrix: tensor([[ 0,  1,  2,  4,  5,  6],\n",
      "        [ 1,  2,  3,  5,  6,  7],\n",
      "        [ 2,  3,  4,  6,  7,  8],\n",
      "        [ 3,  4,  5,  7,  8,  9],\n",
      "        [ 4,  5,  6,  8,  9, 10],\n",
      "        [ 5,  6,  7,  9, 10, 11],\n",
      "        [ 6,  7,  8, 10, 11, 12],\n",
      "        [ 7,  8,  9, 11, 12, 13],\n",
      "        [ 8,  9, 10, 12, 13, 14],\n",
      "        [15, 16,  9, 18, 19, 20],\n",
      "        [16,  9, 17, 19, 20, 21],\n",
      "        [ 9, 17, 18, 20, 21,  8],\n",
      "        [17, 18, 19, 21,  8, 22],\n",
      "        [18, 19, 20,  8, 22, 11],\n",
      "        [19, 20, 21, 22, 11, 23],\n",
      "        [20, 21,  8, 11, 23, 24],\n",
      "        [21,  8, 22, 23, 24, 25],\n",
      "        [ 8, 22, 11, 24, 25, 26],\n",
      "        [22, 11, 23, 25, 26, 27],\n",
      "        [11, 23, 24, 26, 27, 28],\n",
      "        [23, 24, 25, 27, 28, 29],\n",
      "        [24, 25, 26, 28, 29, 30],\n",
      "        [31, 32, 33, 34, 35, 36],\n",
      "        [32, 33,  9, 35, 36, 37],\n",
      "        [33,  9, 34, 36, 37, 38],\n",
      "        [ 9, 34, 35, 37, 38, 39],\n",
      "        [34, 35, 36, 38, 39, 24],\n",
      "        [35, 36, 37, 39, 24, 40],\n",
      "        [36, 37, 38, 24, 40, 41],\n",
      "        [37, 38, 39, 40, 41, 42],\n",
      "        [38, 39, 24, 41, 42, 43],\n",
      "        [39, 24, 40, 42, 43, 44],\n",
      "        [24, 40, 41, 43, 44, 24],\n",
      "        [40, 41, 42, 44, 24,  9],\n",
      "        [41, 42, 43, 24,  9, 45],\n",
      "        [42, 43, 44,  9, 45, 11],\n",
      "        [43, 44, 24, 45, 11, 46],\n",
      "        [44, 24,  9, 11, 46, 47],\n",
      "        [24,  9, 45, 46, 47, 37],\n",
      "        [ 9, 45, 11, 47, 37, 48],\n",
      "        [45, 11, 46, 37, 48, 49],\n",
      "        [11, 46, 47, 48, 49, 29],\n",
      "        [46, 47, 37, 49, 29, 11],\n",
      "        [47, 37, 48, 29, 11, 50],\n",
      "        [37, 48, 49, 11, 50, 51],\n",
      "        [48, 49, 29, 50, 51, 52],\n",
      "        [49, 29, 11, 51, 52, 53],\n",
      "        [29, 11, 50, 52, 53, 54],\n",
      "        [11, 50, 51, 53, 54, 55],\n",
      "        [50, 51, 52, 54, 55, 56]])\n",
      "label matrix: tensor([ 3,  4,  5,  6,  7,  8,  9, 10, 11, 17, 18, 19, 20, 21,  8, 22, 11, 23,\n",
      "        24, 25, 26, 27,  9, 34, 35, 36, 37, 38, 39, 24, 40, 41, 42, 43, 44, 24,\n",
      "         9, 45, 11, 46, 47, 37, 48, 49, 29, 11, 50, 51, 52, 53])\n"
     ]
    }
   ],
   "source": [
    "dataset = CBOWDataset(window_size = 3)\n",
    "print(\"number of samples in the dataset:\", dataset.n)\n",
    "print(\"feature matrix:\", dataset.features)\n",
    "print(\"label matrix:\", dataset.labels)\n",
    "# create batch\n",
    "train_loader = DataLoader(dataset=dataset, batch_size=1, shuffle=True, num_workers=1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Write code that\n",
    "- defines the CBOW model\n",
    "- train the CBOW model (update its parameters) with all the training examples (use SGD)\n",
    "- prints the word embedding for one of the word involved in the training before and after training\n",
    "- assumes the hyperparameters: EMBEDDING_SIZE to 3, LEARNING_RATE to 0.5, WINDOW_SIZE to 3 and MAX_EPOCHS to 1.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "create a model for CBOW\n",
    "\"\"\"\n",
    "class CBOWmodel(nn.Module):\n",
    "  \n",
    "  def __init__(self, embedding_size, vocab_size, output_size):\n",
    "    # In the constructor we define the layers for our model\n",
    "    super(CBOWmodel, self).__init__()\n",
    "    self.embedding = nn.Embedding(vocab_size, embedding_size, sparse=True)\n",
    "    \n",
    "    self.embedding.weight.data.normal_(0.0,0.05) # mean=0.0, mu=0.05\n",
    "    \n",
    "    self.linear_layer = nn.Linear(embedding_size, output_size, bias=False) # the layer will not learn an additive bias\n",
    "    self.softmax_layer = nn.LogSoftmax(dim=1)\n",
    "  \n",
    "  def forward(self, x):\n",
    "    # In the forward function we define the forward propagation logic\n",
    "    out = self.embedding(x).mean(1)\n",
    "    out = self.linear_layer(out)\n",
    "    out = self.softmax_layer(out)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameter of CBOW model\n",
    "EMBEDDING_SIZE = 3 # size of the word embedding\n",
    "LEARNING_RATE = 0.5 # learning rate of gradient descent\n",
    "WINDOW_SIZE = 3  # number of words to be considerd before (or after) the target word for making the context\n",
    "MAX_EPOCHS = 1 # number of passes over the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CBOWmodel(\n",
      "  (embedding): Embedding(57, 3, sparse=True)\n",
      "  (linear_layer): Linear(in_features=3, out_features=57, bias=False)\n",
      "  (softmax_layer): LogSoftmax()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# define the loss function (last node of the graph)\n",
    "model = CBOWmodel(EMBEDDING_SIZE, dataset.vocab_size, dataset.vocab_size)\n",
    "model.to(device)\n",
    "print(model)\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "# create an instance of SGD with required hyperparameters\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train logic (similar to that of linear regression model)\n",
    "def train(loader):\n",
    "    total_loss = 0.0\n",
    "    # iterate throught the data loader\n",
    "    num_batches = 0\n",
    "    for batch in loader:\n",
    "        # load the current batch\n",
    "        batch_input, batch_output = batch\n",
    "\n",
    "        # forward propagation\n",
    "        # pass the data through the model\n",
    "        model_outputs = model(batch_input)\n",
    "        # compute the loss\n",
    "        cur_loss = criterion(model_outputs, batch_output)\n",
    "        total_loss += cur_loss.item()\n",
    "\n",
    "        # backward propagation (compute the gradients and update the model)\n",
    "        # clear the buffer\n",
    "        optimizer.zero_grad()\n",
    "        # compute the gradients\n",
    "        cur_loss.backward()\n",
    "        # update the weights\n",
    "        optimizer.step()\n",
    "\n",
    "        num_batches += 1\n",
    "    return total_loss/num_batches\n",
    "\n",
    "# evaluation logic based on classification accuracy\n",
    "def evaluate(loader):\n",
    "    accuracy, num_examples = 0.0, 0\n",
    "    with torch.no_grad(): # impacts the autograd engine and deactivate it. reduces memory usage and speeds up computation\n",
    "        for batch in loader:\n",
    "            # load the current batch\n",
    "            batch_input, batch_output = batch\n",
    "            # forward propagation\n",
    "            # pass the data through the model\n",
    "            model_outputs = model(batch_input)\n",
    "            # identify the predicted class for each example in the batch (row-wise)\n",
    "            _, predicted = torch.max(model_outputs.data, 1) # Returns a (values, indices) \n",
    "            # compare with batch_output (gold labels) to compute accuracy\n",
    "            accuracy += (predicted == batch_output).sum().item()\n",
    "            num_examples += batch_output.size(0)\n",
    "    return accuracy/num_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], Loss: 4.0420\n"
     ]
    }
   ],
   "source": [
    "# start the training\n",
    "for epoch in range(MAX_EPOCHS):\n",
    "  # train the model for one pass over the data\n",
    "  train_loss = train(train_loader)   \n",
    "  # print the loss for every epoch\n",
    "  print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, MAX_EPOCHS, train_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 How many learnable (or updatable) parameters are present in the model defined in 2.3.3. Compute the result by writing code or by hand.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here (if you are writing code, use this block. Otherwise, change this block to a Markdown block.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of parameters: 342\n"
     ]
    }
   ],
   "source": [
    "# Python code to get the same result\n",
    "count = 0\n",
    "\n",
    "for p in model.parameters():\n",
    "    count += p.numel()\n",
    "print(\"the number of parameters:\",count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Neural Network for Sentiment Analysis\n",
    "\n",
    "\n",
    "The multilayer neural network code used in our tutorial is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiLayerNeuralNetworkModel(\n",
      "  (model): Sequential(\n",
      "    (0): Linear(in_features=5000, out_features=50, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=50, out_features=3, bias=True)\n",
      "    (5): LogSoftmax()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# all the necessary imports\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "\n",
    "# set the seed\n",
    "manual_seed = 123\n",
    "torch.manual_seed(manual_seed)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_gpu = torch.cuda.device_count()\n",
    "if n_gpu > 0:\n",
    "    torch.cuda.manual_seed(manual_seed)\n",
    "\n",
    "# hyperparameters\n",
    "BATCH_SIZE = 5\n",
    "MAX_EPOCHS = 15\n",
    "LEARNING_RATE = 0.1\n",
    "MAX_FEATURES = 5000 # x_j, the number of j\n",
    "NUM_CLASSES = 3\n",
    "\n",
    "# dataset\n",
    "DATA_FOLDER = \"data/sentiment-twitter-2016-task4\"\n",
    "TRAIN_FILE = DATA_FOLDER + \"/train.tsv\"\n",
    "VALID_FILE = DATA_FOLDER + \"/dev.tsv\"\n",
    "TEST_FILE = DATA_FOLDER + \"/test.tsv\"\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# function for reading tsv file\n",
    "def read_corpus(file):\n",
    "    corpus = [] \n",
    "    for line in open(file):\n",
    "        content, label = line.strip().split(\"\\t\") # first column is tweet, second column is golden label.\n",
    "        corpus.append(content)\n",
    "    return corpus\n",
    "\n",
    "# reads the train corpus\n",
    "train_corpus = read_corpus(TRAIN_FILE)  # get a list of tweets\n",
    "\n",
    "# define the vectorizer\n",
    "# builds a vocabulary that only considering the top max_features ordered by term frequency across the corpus.\n",
    "vectorizer = TfidfVectorizer(max_features=MAX_FEATURES,ngram_range=(1, 3))\n",
    "\n",
    "# fit the vectorizer on train set\n",
    "vectorizer.fit(train_corpus)\n",
    "\n",
    "# create a new class inheriting torch.utils.data.Dataset\n",
    "class TweetSentimentDataset(Dataset):\n",
    "  \"\"\" sentiment-twitter-2016-task4 dataset.\"\"\"\n",
    "  def __init__(self, file, vectorizer):\n",
    "    # read the corpus\n",
    "    corpus, labels = [], []\n",
    "    for line in open(file):\n",
    "      content, label = line.strip().split(\"\\t\")\n",
    "      corpus.append(content)\n",
    "      labels.append(int(label))\n",
    "    \n",
    "    # set the size of the corpus\n",
    "    self.n = len(corpus)\n",
    "    \n",
    "    # vectorize all the tweets\n",
    "    features = vectorizer.transform(corpus)\n",
    "    \n",
    "    # convert features and labels to torch.tensor\n",
    "    self.features = torch.from_numpy(features.toarray()).float()\n",
    "    self.features.to(device)\n",
    "    self.labels = torch.tensor(labels, device=device, requires_grad=False)\n",
    "    \n",
    "  # return input and output of a single example\n",
    "  # Input: Feature vectors, where each vector corresponds to a tweet. \n",
    "  # Output: Labels, where each label is one index for each of our tags in the set {positive, negative, neutral}\n",
    "  def __getitem__(self, index):\n",
    "    return self.features[index], self.labels[index]\n",
    "  \n",
    "  # return the total number of examples\n",
    "  def __len__(self):\n",
    "    return self.n\n",
    "\n",
    "# create the dataloader object\n",
    "train_loader = DataLoader(dataset=TweetSentimentDataset(TRAIN_FILE, vectorizer), batch_size=BATCH_SIZE, shuffle=True)#, num_workers=2) \n",
    "valid_loader = DataLoader(dataset=TweetSentimentDataset(VALID_FILE, vectorizer), batch_size=BATCH_SIZE, shuffle=False)#, num_workers=1)\n",
    "test_loader = DataLoader(dataset=TweetSentimentDataset(TEST_FILE, vectorizer), batch_size=BATCH_SIZE, shuffle=False)#, num_workers=1) \n",
    "\n",
    "# train logic (similar to that of linear regression model)\n",
    "def train(loader):\n",
    "  total_loss = 0.0\n",
    "  # iterate throught the data loader\n",
    "  num_batches = 0\n",
    "  for batch in loader:\n",
    "    # load the current batch\n",
    "    batch_input, batch_output = batch\n",
    "    \n",
    "    # forward propagation\n",
    "    # pass the data through the model\n",
    "    model_outputs = model(batch_input.to(device))\n",
    "    # compute the loss\n",
    "    cur_loss = criterion(model_outputs, batch_output.to(device))\n",
    "    total_loss += cur_loss.item()\n",
    "    \n",
    "    # backward propagation (compute the gradients and update the model)\n",
    "    # clear the buffer\n",
    "    optimizer.zero_grad()\n",
    "    # compute the gradients\n",
    "    cur_loss.backward()\n",
    "    # update the weights\n",
    "    optimizer.step()\n",
    "    \n",
    "    num_batches += 1\n",
    "  return total_loss/num_batches\n",
    "\n",
    "# evaluation logic based on classification accuracy\n",
    "def evaluate(loader):\n",
    "  accuracy, num_examples = 0.0, 0\n",
    "  with torch.no_grad(): # impacts the autograd engine and deactivate it. reduces memory usage and speeds up computation\n",
    "    for batch in loader:\n",
    "      # load the current batch\n",
    "      batch_input, batch_output = batch\n",
    "      # forward propagation\n",
    "      # pass the data through the model\n",
    "      model_outputs = model(batch_input.to(device))\n",
    "      # identify the predicted class for each example in the batch\n",
    "      _, predicted = torch.max(model_outputs.data, 1)\n",
    "      # compare with batch_output (gold labels) to compute accuracy\n",
    "      accuracy += (predicted == batch_output).sum().item()\n",
    "      num_examples += batch_output.size(0)\n",
    "  return accuracy/num_examples\n",
    "\n",
    "\"\"\"\n",
    "create a custom model class inheriting torch.nn.Module\n",
    "\"\"\"\n",
    "class MultiLayerNeuralNetworkModel(nn.Module):\n",
    "  \n",
    "  def __init__(self, num_inputs, hidden_layers, num_outputs):\n",
    "    # In the constructor we define the layers for our model\n",
    "    super(MultiLayerNeuralNetworkModel, self).__init__()\n",
    "    \n",
    "    modules = [] # stores all the layers for the neural network\n",
    "    input_dim = num_inputs\n",
    "    # add input layer followed by hidden layers (excluding the classification module)\n",
    "    for hidden_layer in hidden_layers:\n",
    "      # add one layer followed by non-linearity (nn.Sigmoid)\n",
    "      modules.append(nn.Linear(input_dim, hidden_layer))\n",
    "      #modules.append(nn.Tanh())\n",
    "      modules.append(nn.ReLU())\n",
    "      input_dim = hidden_layer\n",
    "    # add the classification module\n",
    "    modules.append(nn.Linear(input_dim, num_outputs))\n",
    "    modules.append(nn.LogSoftmax(dim=1))\n",
    "    \n",
    "    # create the model from all the modules\n",
    "    self.model = nn.Sequential(*modules) # container of layers, for more details: https://pytorch.org/docs/stable/nn.html#torch.nn.Sequential\n",
    "  \n",
    "  def forward(self, x):\n",
    "    # In the forward function we define the forward propagation logic\n",
    "    out = self.model(x)\n",
    "    return out\n",
    "\n",
    "# hyperparameter of neural network\n",
    "hidden_layers = [50, 50]  # [num. of hidden units in first layer, num. of hidden units in second layer]\n",
    "#hidden_layers = [100, 100,100,100,100] \n",
    "# define the loss function (last node of the graph)\n",
    "model = MultiLayerNeuralNetworkModel(MAX_FEATURES, hidden_layers, NUM_CLASSES)\n",
    "model.to(device)\n",
    "print(model)\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "# create an instance of SGD with required hyperparameters\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/15], Loss: 0.9924, Training Accuracy: 0.5157, Validation Accuracy: 0.4217\n",
      "Epoch [2/15], Loss: 0.9652, Training Accuracy: 0.5517, Validation Accuracy: 0.4487\n",
      "Epoch [3/15], Loss: 0.8733, Training Accuracy: 0.6648, Validation Accuracy: 0.5143\n",
      "Epoch [4/15], Loss: 0.8072, Training Accuracy: 0.5840, Validation Accuracy: 0.4507\n",
      "Epoch [5/15], Loss: 0.7601, Training Accuracy: 0.7340, Validation Accuracy: 0.5033\n",
      "Epoch [6/15], Loss: 0.7045, Training Accuracy: 0.7255, Validation Accuracy: 0.4907\n",
      "Epoch [7/15], Loss: 0.6577, Training Accuracy: 0.8053, Validation Accuracy: 0.5048\n",
      "Epoch [8/15], Loss: 0.6108, Training Accuracy: 0.8378, Validation Accuracy: 0.4887\n",
      "Epoch [9/15], Loss: 0.5653, Training Accuracy: 0.7482, Validation Accuracy: 0.4462\n",
      "Epoch [10/15], Loss: 0.5189, Training Accuracy: 0.8258, Validation Accuracy: 0.4867\n",
      "Epoch [11/15], Loss: 0.4812, Training Accuracy: 0.7813, Validation Accuracy: 0.4462\n",
      "Epoch [12/15], Loss: 0.4474, Training Accuracy: 0.8922, Validation Accuracy: 0.4977\n",
      "Epoch [13/15], Loss: 0.4028, Training Accuracy: 0.9175, Validation Accuracy: 0.4707\n",
      "Epoch [14/15], Loss: 0.3721, Training Accuracy: 0.8765, Validation Accuracy: 0.4867\n",
      "Epoch [15/15], Loss: 0.3323, Training Accuracy: 0.7765, Validation Accuracy: 0.4652\n"
     ]
    }
   ],
   "source": [
    "# start the training\n",
    "for epoch in range(MAX_EPOCHS):\n",
    "  # train the model for one pass over the data\n",
    "  train_loss = train(train_loader)\n",
    "  # compute the training accuracy\n",
    "  train_acc = evaluate(train_loader)\n",
    "  # compute the validation accuracy \n",
    "  val_acc = evaluate(valid_loader)\n",
    "  # print the loss for every epoch\n",
    "  print('Epoch [{}/{}], Loss: {:.4f}, Training Accuracy: {:.4f}, Validation Accuracy: {:.4f}'.format(epoch+1, MAX_EPOCHS, train_loss, train_acc, val_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 In the original tutorial, we considered only *unigrams* as features to represent a tweet. Change the original tutorial code to consider *bigrams, trigrams* (along with unigrams, which is considered by default) as features to represent a tweet.\n",
    "\n",
    "Hints: \n",
    "- Look at the documentation of [``TfidfVectorizer``](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) to see how to incorporate bigrams, trigrams and so on.\n",
    "- Modify the line ``vectorizer = TfidfVectorizer(max_features=MAX_FEATURES)`` and keep the rest of the code intact.\n",
    "\n",
    "### **Hand in the**\n",
    "- Accuracy on the validation set after training\n",
    "- Python code (ONLY the changed lines)\n",
    "\n",
    "rubric={accuracy:3, quality:1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Report performance of your model here:** (double-click to edit)\n",
    "\n",
    "**3.1.1 Performance of my neural network model on the validation set after training is ~51.58%** on accuracy (fill in your accuracy)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.1.2 Your code:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here (put only the changed lines)\n",
    "vectorizer = TfidfVectorizer(max_features=MAX_FEATURES,ngram_range=(1, 3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2  In the original tutorial, we considered only two hidden layers with 50 units each. Change the original tutorial code to consider *five hidden layers* with *100 dimensions each*.\n",
    "\n",
    "Hints: \n",
    "- Modify the line ``hidden_layers = [50, 50]`` and keep the rest of the code intact.\n",
    "\n",
    "### **Hand in the**\n",
    "- Accuracy on the validation set after training\n",
    "- Python code (ONLY the changed lines)\n",
    "\n",
    "rubric={accuracy:3, quality:1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Report performance of your model here:** (double-click to edit)\n",
    "\n",
    "**3.2.1  Performance of my neural network model on the validation set after training is ~50.43%** on accuracy (fill in your accuracy)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.2.2 Your code:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here (put only the changed lines)\n",
    "hidden_layers = [100, 100,100,100,100] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 In the original tutorial, we used [*Sigmoid*](https://pytorch.org/docs/stable/nn.functional.html#torch.nn.functional.sigmoid) as the activation function. Change the original tutorial code to consider other nonlinearities such as [*ReLU*](https://pytorch.org/docs/stable/nn.functional.html#torch.nn.functional.relu) and [*Tanh*](https://pytorch.org/docs/stable/nn.functional.html#torch.nn.functional.tanh) and report the nonlinearity that gives the best validation performance.\n",
    "\n",
    "Hints: \n",
    "- Modify the line ``modules.append(nn.Sigmoid())`` and keep the rest of the code intact.\n",
    "\n",
    "### **Hand in the**\n",
    "- Nonlinearity function that gives the best validation performance\n",
    "- Accuracy on the validation set after training with the best nonlinearity\n",
    "- Python code (ONLY the changed lines)\n",
    "\n",
    "rubric={accuracy:4, quality:1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Report your results here:** (double-click to edit)\n",
    "\n",
    "**3.3.1. Report the nonlinearity function that gives the best validation performance: nn.ReLU()** There is a chance with your random state to have gotten nn.Tanh() as your best performance.\n",
    "\n",
    "**3.3.2. Report performance of my neural network model on the validation set after training is ~51.43%** on accuracy (fill in your accuracy)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.3.3 Your code:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here (put only the changed lines)\n",
    "modules.append(nn.Tanh())\n",
    "\n",
    "#or\n",
    "\n",
    "modules.append(nn.ReLU())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 In the original tutorial, we used *learning rate of 0.5*. Change the original tutorial code by trying out different learning rates preferably *between 0.0001 and 1*. Report the learning rate that gives the best validation performance.\n",
    "\n",
    "Hints: \n",
    "- Modify the line ``LEARNING_RATE = 0.5`` and keep the rest of the code intact.\n",
    "\n",
    "### **Hand in the**\n",
    "- Learning rate that gives the best validation performance\n",
    "- Accuracy on the validation set after training with the best learning rate\n",
    "- Python code (ONLY the changed lines)\n",
    "\n",
    "rubric={accuracy:6, quality:1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Report your resuls here:** (double-click to edit)\n",
    "\n",
    "**3.4.1. Report the learning rate that gives the best validation performance: This depends, but should be around ~.7\n",
    "\n",
    "**3.4.2. Report the performance of my neural network model on the validation set after training is ~52%** on accuracy (fill in your accuracy)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.4.3 Your code:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This answer depends on what learning rates you tried. You should be getting ~52%ish"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: Very-Short answer questions\n",
    "\n",
    "(Double-click each question block and place your answer at the end of the question) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Can we build deep neural network without any nonlinearity layers? What's the nature of such deep neural network without any nonlinearity layers?\n",
    "rubric={reasoning:2}\n",
    "\n",
    "\n",
    "I mean they aren't really neural networks without the nonlinearity, they are just fancy ways of doing linear regression since they need the nonlinearity to be able to deal with non-linearly separable data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 What is the distinction between single layer neural network and multilayer neural network?\n",
    "rubric={reasoning:2}\n",
    "\n",
    "Multilayer networks can have multiple hidden layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 What is the difference between hidden layer and hidden unit (or neuron)?\n",
    "rubric={reasoning:2}\n",
    "\n",
    "Layers are made up of units."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 What is the gradient of sigmoid function over a scalar ($\\nabla \\sigma(a)$)? What is the interesting property of this gradient?\n",
    "rubric={reasoning:2}\n",
    "\n",
    "Sigmoid's gradient is flat near the edges ($+ \\infty$ or $-\\infty$ and very steep near the center (x=0) this means things will be pushed to either +1 or 0. Another interesting property is that it has some interesting form when you take the derivative. Derivative of sigmoid $\\sigma'(x)=\\sigma(x)(1-\\sigma(x))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
