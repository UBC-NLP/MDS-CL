{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework 3 - Supervised Learning II - MDS Computational Linguistics\n",
    "\n",
    "### Assignment Topics\n",
    "- Recurrent Neural Networks\n",
    "- Long Short-Term Memory\n",
    "- Saving and Loading NN models using Pytorch\n",
    "- Very-short answer questions\n",
    "\n",
    "### Software Requirements\n",
    "- Python (>=3.6)\n",
    "- PyTorch (>=1.2.0) \n",
    "- Jupyter (latest)\n",
    "\n",
    "### Submission Info.\n",
    "- Due Date: 1/30/21 11:59 PM PST\n",
    "\n",
    "## Getting Started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all the necessary imports\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torchtext\n",
    "from torchtext.data import Field, LabelField\n",
    "from torchtext.data import TabularDataset\n",
    "from torchtext.data import Iterator, BucketIterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the seed\n",
    "manual_seed = 572\n",
    "torch.manual_seed(manual_seed)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tidy Submission\n",
    "\n",
    "rubric={mechanics:1}\n",
    "\n",
    "To get the marks for tidy submission:\n",
    "- Submit the assignment by filling in this jupyter notebook with your answers embedded\n",
    "- Be sure to follow the [general lab instructions](https://ubc-mds.github.io/resources_pages/general_lab_instructions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T3 - CNN's for text classification\n",
    "\n",
    "Convolutional Neural Networks (CNNs) are essentially a special case of a normal feed forward network, instead of being \"densely\" connected (note you may see people refer to Feed Forward networks as \"dense layers\"), nodes in CNNs connect to a smaller set of nodes, defined by the \"filters size\" of the network. CNNs thus have a smaller local windows to look at data, but make up for it by generally using many additional filters, which might be able to learn different aspects of the data. These networks turn out to be extremely useful for processing images, audio, and anything with some sort of spatial properties to the data.\n",
    "\n",
    "It turns out they can also be used for any sentence classification task. Words in a sentence it turns out have a sort of 1D spatial ordering, which means some classification tasks can benefit from this CNNs ability to operate over the length of the sequence. In addition, because of the sparsity of the connections, you end up being able to make much smaller networks that retain a great deal of power.\n",
    "\n",
    "<img src=\"images/cnn.png\" alt=\"cnn\" title=\"CNN vs FF-net comparison\" />\n",
    "\n",
    "<p style=\"text-align: center;\">Top: A CNN with filter size of 3, Bottom: A Feedforward neural net. (From Goodfellow et al. 2016)</p>\n",
    "\n",
    "This illustrates a single size 3 filter, your network might have many additional filters that would all pass over the length of the data, each one producing an output channel to be passed to the next part of the network.  \n",
    "\n",
    "<img src=\"images/1dcnn.png\" alt=\"1dcnn\" title=\"Pytorch 1DCNN overview\" />\n",
    "\n",
    "*We'll get into 2D CNNs later (as an additional topic in COLX 585) but here is a quick taste for how you could use a 1D CNN in the context of text-based NLP tasks.*\n",
    "\n",
    "\n",
    "#### Pytorch 1D CNNs and max-pooling\n",
    "\n",
    "Here's a quick example of how these networks function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.0115, 0.1652, 0.0000, 0.0210, 0.1613, 0.3903, 0.0000, 0.1766,\n",
      "          0.1741, 0.2820],\n",
      "         [0.0000, 0.0000, 0.0083, 0.1430, 0.0000, 0.0000, 0.2834, 0.0585,\n",
      "          0.2041, 0.0000],\n",
      "         [0.0456, 0.4565, 0.1949, 0.3690, 0.0888, 0.2262, 0.0732, 0.5068,\n",
      "          0.5604, 0.3049],\n",
      "         [0.0000, 0.0000, 0.0170, 0.1959, 0.0000, 0.0990, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000]],\n",
      "\n",
      "        [[0.2387, 0.2856, 0.0000, 0.0000, 0.2042, 0.3228, 0.0630, 0.0000,\n",
      "          0.0000, 0.2929],\n",
      "         [0.0556, 0.2714, 0.0000, 0.0000, 0.0000, 0.1017, 0.1376, 0.1463,\n",
      "          0.0000, 0.0000],\n",
      "         [0.1951, 0.6214, 0.0341, 0.6379, 0.3405, 0.5166, 0.4090, 0.2046,\n",
      "          0.3178, 0.5490],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.2466]]], grad_fn=<ReluBackward0>)\n",
      "Take the highest value in each window using max pool\n",
      "tensor([[[0.1652, 0.1613, 0.3903, 0.2820],\n",
      "         [0.0000, 0.1430, 0.2834, 0.2041],\n",
      "         [0.4565, 0.3690, 0.5068, 0.5604],\n",
      "         [0.0000, 0.1959, 0.0990, 0.0000]],\n",
      "\n",
      "        [[0.2856, 0.2042, 0.3228, 0.2929],\n",
      "         [0.2714, 0.0000, 0.1463, 0.0000],\n",
      "         [0.6214, 0.6379, 0.5166, 0.5490],\n",
      "         [0.0000, 0.0000, 0.0000, 0.2466]]], grad_fn=<SqueezeBackward1>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "x = torch.rand((2,5,10))   # batch size 2 with length 10 and 5 dim embedding.\n",
    "\n",
    "in_dim = 5\n",
    "filters = 4\n",
    "\n",
    "cnn1d = nn.Conv1d(in_dim,filters,kernel_size=3,padding=1) \n",
    "max_pool = nn.MaxPool1d(kernel_size=3, padding=1)\n",
    "activation = nn.ReLU()\n",
    "x = cnn1d(x)\n",
    "x = activation(x)\n",
    "print(x)\n",
    "print(\"Take the highest value in each window using max pool\")\n",
    "print(max_pool(x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we've used a 1D CNN and max pooling to \"summarize\" our data, boiling a length 10 series to only 4 items. One could repeat combinations of CNNs, activation functions, with and without pooling layers to build up a network in the same way that we have worked with FF nets.\n",
    "\n",
    "But first one quick check about 1D CNNs\n",
    "\n",
    "#### 1D CNN parameters\n",
    "rubric={reasoning:1}  \n",
    "  \n",
    "Including bias weights how many trainable parameters are in our example CNN (think about how many weights would be needed to operate over your kernel size and each segment of data)?  Explain your reasoning in your answer, and consider checking it by printing out those weights/biases.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for param in cnn1d.parameters():\n",
    "    if param.requires_grad==True:\n",
    "        count+= param.numel()\n",
    "print(count)\n",
    "\n",
    "# 4 filters X 5 embedding size X 3 filter size + 4 filters X 1 bias per filter = 64 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your Answer Here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Size through CNNs\n",
    "rubric={accuracy:1, reasoning:1}\n",
    "\n",
    "CNNs can be a little tricky because as the data passes through them the dimensions might change based on number of filters, padding, and two other things we won't talk about now: stride and dilation. MaxPooling also quickly can decrease the size of the data. This is useful especially as a way to \"feature extract\" or \"dimensionality reduction\" but it's important to make sure we get the right output dimensions. \n",
    "\n",
    "For an tensor with batch size of 10, embedding dimension of 25, and length 30 that is fed to a 1D CNN + Maxpool combination, what parameters of the 1D CNN (kernelsize, number of filters, padding) and MaxPool (kernel_size, padding) would we need to get a output that has an output embedding dimension of 5 and a length of 10?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your Answer Here*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 25, 30])\n",
      "torch.Size([10, 5, 10])\n"
     ]
    }
   ],
   "source": [
    "## Test your answer by making a random tensor of the appropriate size, \n",
    "## passing it through your proposed CNN+Maxpool and printing out the final size.\n",
    "\n",
    "## Your Code Here ##\n",
    "\n",
    "x = torch.randn((10,25,30))  # our example tensor\n",
    "cnn1 = nn.Conv1d(25,5,kernel_size=3,padding=1)\n",
    "maxpool = nn.MaxPool1d(kernel_size=3,padding=1)\n",
    "\n",
    "print(x.size())\n",
    "print(maxpool(cnn1(x)).size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note we can ignore batch (N) in the problem, there should be a number of different ways to get the same output Length (L) depending on how you set the kernel_sizes and padding, but giving the maxpool a kernel size of 3 is the easiest way.\n",
    "\n",
    "Output hidden/embedding dimension (H) is dependent on the number of filters in the CNN, which means it must be set to 5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1D CNN Model for Sentiment Analysis\n",
    "rubric={accuracy:3, quality:1}\n",
    "\n",
    "Based on our tutorial code that we've used for RNNs, modify our network to use 1D cnns instead of FF networks. We'll search over the architecture in the next question, but for now just make it as a basic 2 layer CNN network with 5 filters kernel size of 3 and using ReLU activation (no max Pooling). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext\n",
    "from torchtext.data import Field, LabelField\n",
    "from torchtext.data import TabularDataset\n",
    "\n",
    "# define the white space tokenizer to get tokens\n",
    "def tokenize_en(tweet):\n",
    "    \"\"\"\n",
    "    Tokenizes English tweet from a string into a list of strings (tokens)\n",
    "    \"\"\"\n",
    "    return tweet.strip().split()\n",
    "\n",
    "# define the TorchText's fields\n",
    "TEXT = Field(sequential=True, tokenize=tokenize_en, lower=True)\n",
    "LABEL = Field(sequential=False, unk_token = None)\n",
    "\n",
    "\n",
    "train, val, test = TabularDataset.splits(\n",
    "    path=\"./data/sentiment-twitter-2016-task4/\", # the root directory where the data lies\n",
    "    train='train.tsv', validation=\"dev.tsv\", test=\"test.tsv\", # file names\n",
    "    format='tsv',\n",
    "    skip_header=False, # if your tsv file has a header, make sure to pass this to ensure it doesn't get proceesed as data!\n",
    "    fields=[('tweet', TEXT), ('label', LABEL)])\n",
    "\n",
    "TEXT.build_vocab(train, min_freq=3) # builds vocabulary based on all the words that occur at least twice in the training set\n",
    "LABEL.build_vocab(train)\n",
    "\n",
    "train_iter, val_iter, test_iter = BucketIterator.splits(\n",
    " (train, val, test), # we pass in the datasets we want the iterator to draw data from\n",
    " batch_sizes=(4,64,64),\n",
    " sort_key=lambda x: len(x.tweet), \n",
    " sort=True,\n",
    "# A key to use for sorting examples in order to batch together examples with similar lengths and minimize padding. \n",
    " sort_within_batch=True\n",
    ")\n",
    "\n",
    "VOCAB_SIZE = len(TEXT.vocab.stoi)\n",
    "\n",
    "WORD_VEC_SIZE=300\n",
    "# Note, the parameters to Embedding class below are:\n",
    "# num_embeddings (int): size of the dictionary of embeddings\n",
    "# embedding_dim (int): the size of each embedding vector\n",
    "# For more details on Embedding class, see: https://github.com/pytorch/pytorch/blob/master/torch/nn/modules/sparse.py\n",
    "\n",
    "\n",
    "class ConvNet(nn.Module):\n",
    "  \n",
    "  def __init__(self, VOCAB_SIZE, WORD_VEC_SIZE,output_size, config):  \n",
    "    \"\"\"Build a 1D CNN for classification.\n",
    "\n",
    "    Keyword arguments:\n",
    "    config -- a dictionary of parameters for the model\n",
    "        layers -- a tuple containing the filter#, filtersize, and dropout% per layer\n",
    "        nonlin -- nonlinearity\n",
    "        pool_mode -- None, nn.MaxPool1d, or nn.AvgPool1d\n",
    "    \"\"\"\n",
    "    super(ConvNet, self).__init__()\n",
    "    self.embedding = nn.Embedding(VOCAB_SIZE, WORD_VEC_SIZE, sparse=True)\n",
    "    self.embedding.weight.data.normal_(0.0,0.05)\n",
    "    self.layers = nn.ModuleList()\n",
    "    self.nonlin = config[\"nonlin\"]\n",
    "    for i in range(len(config[\"layers\"])):\n",
    "        if i == 0:\n",
    "            self.layers.append(nn.Conv1d(WORD_VEC_SIZE, config[\"layers\"][i][0], config[\"layers\"][i][1], padding=(config[\"layers\"][i][1]-1)//2))\n",
    "        else:\n",
    "            self.layers.append(nn.Conv1d(config[\"layers\"][i-1][0],config[\"layers\"][i][0],config[\"layers\"][i][1], padding=(config[\"layers\"][i][1]-1)//2))      \n",
    "        self.layers.append(self.nonlin)\n",
    "        self.layers.append(nn.Dropout(p=config[\"layers\"][i][2]))\n",
    "        if config[\"pool_mode\"]:\n",
    "                self.layers.append(config[\"pool_mode\"](kernel_size=2,padding=1))\n",
    "    self.max_layer = nn.AdaptiveMaxPool1d(1)\n",
    "    self.output = nn.Linear(config[\"layers\"][-1][0], output_size)\n",
    "    self.softmax = nn.LogSoftmax(dim=1) \n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.embedding(x.permute(1,0)).permute(0,2,1)\n",
    "    for layer in self.layers:\n",
    "        x = layer(x)\n",
    "    x = self.max_layer(x).squeeze(dim=-1)\n",
    "    x =self.softmax(self.output(x))  # [N,H]\n",
    "    \n",
    "    return x\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "def train(loader,model,criterion,optimizer,device):\n",
    "    total_loss = 0.0\n",
    "    # iterate throught the data loader\n",
    "    num_sample = 0\n",
    "    for batch in loader:\n",
    "        # load the current batch\n",
    "        batch_input = batch.tweet\n",
    "        batch_output = batch.label\n",
    "        \n",
    "        batch_input = batch_input.to(device)\n",
    "        batch_output = batch_output.to(device)\n",
    "        # forward propagation\n",
    "        # pass the data through the model\n",
    "        model_outputs = model(batch_input)\n",
    "        # compute the loss\n",
    "        cur_loss = criterion(model_outputs, batch_output)\n",
    "        total_loss += cur_loss.item()\n",
    "\n",
    "        # backward propagation (compute the gradients and update the model)\n",
    "        # clear the buffer\n",
    "        optimizer.zero_grad()\n",
    "        # compute the gradients\n",
    "        cur_loss.backward()\n",
    "        # update the weights\n",
    "        optimizer.step()\n",
    "\n",
    "        num_sample += batch_output.shape[0]\n",
    "    return total_loss/num_sample\n",
    "\n",
    "# evaluation logic based on classification accuracy\n",
    "def evaluate(loader,model,criterion,device):\n",
    "    all_pred=[]\n",
    "    all_label = []\n",
    "    with torch.no_grad(): # impacts the autograd engine and deactivate it. reduces memory usage and speeds up computation\n",
    "        for batch in loader:\n",
    "             # load the current batch\n",
    "            batch_input = batch.tweet\n",
    "            batch_output = batch.label\n",
    "\n",
    "            batch_input = batch_input.to(device)\n",
    "            # forward propagation\n",
    "            # pass the data through the model\n",
    "            model_outputs = model(batch_input)\n",
    "            # identify the predicted class for each example in the batch\n",
    "            probabilities, predicted = torch.max(model_outputs.cpu().data, 1)\n",
    "            # put all the true labels and predictions to two lists\n",
    "            all_pred.extend(predicted)\n",
    "            all_label.extend(batch_output)\n",
    "            \n",
    "    accuracy = accuracy_score(all_label, all_pred)\n",
    "    return accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1D CNN Performance\n",
    "rubric={accuracy:2, quality:1}\n",
    "\n",
    "Based on our initial network we'd like to compare how depth matters vs number of filters in a given layer. For this section (consider spliting up the trials similar to Lab 1, with group members running a set on different random start states), perform a random search (total trials: 20) over the parameters space (number of layers, kernel size, number of filters, activation, etc.) and report your results (ranges searched, best performance) on the sentiment task. Hint: Feel free to use Skorch + sklearn + scipy.stats (for distributions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here.\n",
    "\n",
    "import scipy.stats\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "MAX_EPOCHS=20 # can be higher\n",
    "\n",
    "np.random.seed(manual_seed)  #Note sets for both Numpy and Scipy \n",
    "torch.manual_seed(manual_seed) \n",
    "\n",
    "def random_search(num_iter):    \n",
    "    results = []\n",
    "    for i in range(num_iter):\n",
    "        \n",
    "        #generate a number of layers\n",
    "        layers = []\n",
    "        layer_num = scipy.stats.randint.rvs(1,5)\n",
    "        for _ in range(layer_num):\n",
    "            # appending (FILTERS# , FILTERSIZE , DROPOUT%) per each layer\n",
    "            layers.append((scipy.stats.randint.rvs(10,200),np.random.choice([1,3,5,7]),scipy.stats.uniform.rvs()))\n",
    "        config = {\n",
    "            #define hyperparameters here\n",
    "            \"layers\": layers,\n",
    "            \"lr\": scipy.stats.loguniform.rvs(10**-3,1),\n",
    "            #\"l2\": scipy.stats.loguniform.rvs(10**-5,10**-2),\n",
    "            \"nonlin\" : np.random.choice([nn.ReLU(),nn.Tanh()]),\n",
    "            \"pool_mode\" : np.random.choice([None,nn.MaxPool1d,nn.AvgPool1d])\n",
    "        }\n",
    "        \n",
    "        print(\"new config\")\n",
    "        print(config)\n",
    "        model = ConvNet(output_size=3, config=config, VOCAB_SIZE=VOCAB_SIZE, WORD_VEC_SIZE=WORD_VEC_SIZE)\n",
    "        model.to(device)\n",
    "        criterion = nn.NLLLoss()\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=config[\"lr\"])\n",
    "\n",
    "    \n",
    "        max_val = 0\n",
    "        best_epoch = 0\n",
    "        for epoch in range(MAX_EPOCHS):\n",
    "        # train the model for one pass over the data\n",
    "            train_loss = train(train_iter,model,criterion,optimizer,device)  \n",
    "        # compute the training accuracy\n",
    "            train_acc = evaluate(train_iter,model,criterion,device)\n",
    "        # compute the validation accuracy\n",
    "            val_acc = evaluate(val_iter,model,criterion,device)\n",
    "            if val_acc > max_val:\n",
    "                max_val = val_acc\n",
    "                best_epoch = epoch+1\n",
    "        # print the loss for every epoch\n",
    "            print('Epoch [{}/{}], Loss: {:.4f}, Training Accuracy: {:.4f}, Validation Accuracy: {:.4f}'.format(epoch+1, MAX_EPOCHS, train_loss, train_acc, val_acc))\n",
    "        results.append((max_val,best_epoch,config))\n",
    "        print(\"Best validation score for iterations #{}: {}\".format(i,max_val))\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_search(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Highest config was around ~50.6% on my run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1D CNN summary\n",
    "rubric={reasoning:1}\n",
    "\n",
    "Based on the performance on the task how does the CNN compare to the Feed Forward network from Lab 2? What factors seemed to be most important to the performance of the model (number of filters, depth, size of filters...? ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* You answer here *\n",
    "\n",
    "Because the results from your random search might be a bit different, you might have found different configurations that worked better. Ultimately as long as you are able to compare the two, and identify important factors in getting the CNN config to work well you will receive full credit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Building a Recurrent Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, we provide a corpus from the [CL-Aff shared task](https://sites.google.com/view/affcon2019/cl-aff-shared-task?authuser=0). HappyDB is a dataset of about 100,000 `happy moments` crowd-sourced via Amazon’s Mechanical Turk where each worker was asked to describe in a complete sentence `what made them happy in the past 24 hours`. Each user was asked to describe three such moments. \n",
    "In this exercise, we focus on sociality classification. We only use labelled dataset which include 10,562 labelled samples. \n",
    "\n",
    "We have already preprocessed (tokenization, removing URLs, mentions, hashtags and so on) the tweets and placed it under ``data/happy_db`` folder in three files as ``train.tsv``, ``dev.tsv`` and ``test.tsv``."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Write code to  define a `whitespace_tokenizer` whose input is a tweet text.\n",
    "\n",
    "rubric={accuracy:1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def whitespace_tokenize(text):\n",
    "    # your code goes here \n",
    "    \"\"\"\n",
    "    Tokenizes English text from a string into a list of strings (tokens)\n",
    "    \"\"\"\n",
    "    return text.strip().split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Wrote code to define `TorchText's Fields` handle how data should be processed.  \n",
    "Hint: You need 2 Fields `TEXT` and `LABEL` for tweet text and label respectively. \n",
    "\n",
    "* The tokenizer is the whitespace_tokenizer in 1.1.\n",
    "* Use the truecase of words. \n",
    "\n",
    "rubric={accuracy:2} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here \n",
    "TEXT = Field(sequential=True, tokenize=whitespace_tokenize, lower=False)\n",
    "LABEL = Field(sequential=False, unk_token = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 Write code to use `TabularDataset class`  and `Fields` to process the tsv files (train, dev, and test). \n",
    "\n",
    "Hint 1: `Fields` will call tokenizer, and convert tokens to numerical index. \n",
    "\n",
    "Hint 2: Use `TabularDataset.splits(...)` to load train, dev, and test sets.\n",
    "\n",
    "rubric={accuracy:2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here \n",
    "train, val, test = TabularDataset.splits(\n",
    "               path=\"./data/happy_db/\", # the root directory where the data lies\n",
    "               train='train.tsv', validation=\"dev.tsv\", test=\"test.tsv\", # file names\n",
    "               format='tsv',\n",
    "               skip_header=True, # if your tsv file has a header, make sure to pass this to ensure it doesn't get proceesed as data!\n",
    "               fields=[('tweet', TEXT), ('label', LABEL)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4  Write code to build your vocabulary to map words and labels to integers. The maximum size of vocabulary is 5,000 (not count `<pad>` and `<unk>` in).\n",
    "Hint: Use `build_vocab` to build vocabularies for `TEXT` field and `LABEL` field respectively. \n",
    "\n",
    "rubric={accuracy:2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here \n",
    "TEXT.build_vocab(train, max_size=5000)\n",
    "LABEL.build_vocab(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.5 Write code to print the sizes of your two vocabularies (i.e., `TEXT` and `LABEL`) individually. \n",
    "\n",
    "rubric={accuracy:2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here \n",
    "print(\"Vocabulary size of TEXT:\",len(TEXT.vocab.stoi))\n",
    "print(\"Vocabulary size of LABEL:\",len(LABEL.vocab.stoi))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.6 Write code to construct the Iterators to get the train, dev, and test splits. Use `BucketIterator` to initialize the Iterators for the train, dev, and test data.\n",
    "* apply same batch size of 32 on train, dev, and test set.\n",
    "* Samples are sorted by length.\n",
    "* Sort samples within each batch\n",
    "\n",
    "Hint: Use `BucketIterator.splits(...)`\n",
    "\n",
    "rubric={accuracy:2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here \n",
    "train_iter, val_iter, test_iter = BucketIterator.splits(\n",
    " (train, val, test), # we pass in the datasets we want the iterator to draw data from\n",
    " batch_sizes=(32,32,32),\n",
    " sort_key=lambda x: len(x.tweet), \n",
    " sort=True,\n",
    "# A key to use for sorting examples in order to batch together examples with similar lengths and minimize padding. \n",
    " sort_within_batch=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.7.1 Write code to create a class called `LSTMmodel` to define a classifier of the task. In this model, you should have:\n",
    "\n",
    "1. An embedding layer with input dimention equal to the size of your `TEXT` vocabulary, and that represent each token in a 300-dimentional vector.  The parameters of this embedding layer should be randomly initialized with numbers sampled from a normal distribution (mean 0 and variance 0.05).\n",
    "\n",
    "2. Two uni-directional `LSTM` layers. Each layer has 500 hidden unites.\n",
    "\n",
    "3. Pass the last hidden of last layer into a `Tanh` activation function, then feed the output of `Tanh` to a linear layer whose dimensionality is equal to the number of classes in the dataset (i.e., 2 in our case).\n",
    "\n",
    "4. Then, a `LogSoftmax` layer on top of the outcome of `linear layer`.\n",
    "\n",
    "5. Return the output of `LogSoftmax` layer.\n",
    "\n",
    "Hint: `Tanh` might not be the ideal function to use, but we want you to explore it. (Usually `ReLU` works well).\n",
    "\n",
    "rubric={accuracy:8, quality:4}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here \n",
    "\n",
    "class LSTMmodel(nn.Module):\n",
    "  \n",
    "  def __init__(self, embedding_size, vocab_size, output_size, hidden_size, num_layers):\n",
    "    # In the constructor we define the layers for our model (same as our previous RNN)\n",
    "    super(LSTMmodel, self).__init__()\n",
    "    # word embedding lookup table\n",
    "    self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_size)\n",
    "    self.embedding.weight.data.normal_(0.0,0.05) # mean=0.0, mu=0.05\n",
    "    # core LSTM module\n",
    "    self.lstm_rnn = nn.LSTM(input_size=embedding_size, hidden_size=hidden_size, num_layers=num_layers) # input_size, hidden_size, num_layers\n",
    "    self.activation_fn = nn.Tanh()\n",
    "    self.linear_layer = nn.Linear(hidden_size, output_size) \n",
    "    self.softmax_layer = nn.LogSoftmax(dim=-1)  \n",
    "  \n",
    "  def forward(self, x):\n",
    "    # In the forward function we define the forward propagation logic\n",
    "    out = self.embedding(x)\n",
    "    out, (h_state, c_state) = self.lstm_rnn(out) # h_0 initialized to zeros by default\n",
    "    # classify based on the hidden representation at the last token\n",
    "    out = out[-1] # unsqueeze converts 1D input (D dimension) into 2D input (1xD) \n",
    "    out = self.linear_layer(out)\n",
    "    out = self.softmax_layer(out) # accepts 2D or more dimensional inputs\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.7.2 Write code to Instantiate the model class with aforementioned hyper-parameters and print out the model architecture.\n",
    "rubric={accuracy:3}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint: Your output should generally look like the following (with correct names of the model, functions, etc.):\n",
    "```\n",
    "modelName (\n",
    "  (embedding): Embedding(xxx, xxx)\n",
    "  (lstm_rnn): MODEL(xxx, xxx, num_layers=xxx)\n",
    "  (activation_fn): Tanh()\n",
    "  (linear_layer): Linear(in_features=xxx, out_features=xxx, bias=True)\n",
    "  (softmax_layer): somethingRelatedToSoftmax()\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here \n",
    "EMBEDDING_SIZE = 300 \n",
    "VOCAB_SIZE = 5002\n",
    "NUM_CLASSES = 2\n",
    "HIDDEN_SIZE = 500\n",
    "NUM_LAYERS = 2\n",
    "model = LSTMmodel(EMBEDDING_SIZE, VOCAB_SIZE, NUM_CLASSES, HIDDEN_SIZE, NUM_LAYERS)\n",
    "model = model.to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create an optimizer for training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.1\n",
    "criterion = nn.NLLLoss()\n",
    "# create an instance of SGD with required hyperparameters\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.8.1 How many learnable (or updatable) parameters are present in the model defined in 1.7. Compute the result by writing code.\n",
    "rubric={accuracy:1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here \n",
    "count = 0\n",
    "for p in model.parameters():\n",
    "    count += p.numel()\n",
    "print(\"the number of parameters:\",count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.8.2 OPTIONAL QUESTION: How many megabyte memory will this model use? Please show your work. \n",
    "\n",
    "Hint1: Each parameter is a `torch.float32` tensor which is 32-bit floating point. \n",
    "\n",
    "Hint2: All the parameters of this model are learnable (or updatable) parameters.\n",
    "\n",
    "rubric={spark:2}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write your answer here.\n",
    "$5109602 * 32 bits = 163507264 bits$\n",
    "\n",
    "$1bit = 1.25e-7 Megabyte$\n",
    "\n",
    "$163507264 bits = 20.438408 Megabyte$ \n",
    "\n",
    "Actully, you can find the size of the checkpoint (i.e., model_23.pt) is 20.4 MB."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To facilitate your work for the next questions (1.9 and 1.10), we provide two functions for training and evaluation.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def train(loader):\n",
    "    total_loss = 0.0\n",
    "    # iterate throught the data loader\n",
    "    num_sample = 0\n",
    "    for batch in loader:\n",
    "        # load the current batch\n",
    "        batch_input = batch.tweet\n",
    "        batch_output = batch.label\n",
    "        \n",
    "        batch_input = batch_input.to(device)\n",
    "        batch_output = batch_output.to(device)\n",
    "        # forward propagation\n",
    "        # pass the data through the model\n",
    "        model_outputs = model(batch_input)\n",
    "        # compute the loss\n",
    "        cur_loss = criterion(model_outputs, batch_output)\n",
    "        total_loss += cur_loss.item()\n",
    "\n",
    "        # backward propagation (compute the gradients and update the model)\n",
    "        # clear the buffer\n",
    "        optimizer.zero_grad()\n",
    "        # compute the gradients\n",
    "        cur_loss.backward()\n",
    "        # update the weights\n",
    "        optimizer.step()\n",
    "\n",
    "        num_sample += batch_output.shape[0]\n",
    "    return total_loss/num_sample\n",
    "\n",
    "# evaluation logic based on classification accuracy\n",
    "def evaluate(loader):\n",
    "    all_pred=[]\n",
    "    all_label = []\n",
    "    with torch.no_grad(): # impacts the autograd engine and deactivate it. reduces memory usage and speeds up computation\n",
    "        for batch in loader:\n",
    "             # load the current batch\n",
    "            batch_input = batch.tweet\n",
    "            batch_output = batch.label\n",
    "\n",
    "            batch_input = batch_input.to(device)\n",
    "            # forward propagation\n",
    "            # pass the data through the model\n",
    "            model_outputs = model(batch_input)\n",
    "            # identify the predicted class for each example in the batch\n",
    "            probabilities, predicted = torch.max(model_outputs.cpu().data, 1)\n",
    "            # put all the true labels and predictions to two lists\n",
    "            all_pred.extend(predicted)\n",
    "            all_label.extend(batch_output)\n",
    "            \n",
    "    accuracy = accuracy_score(all_label, all_pred)\n",
    "    f1score = f1_score(all_label, all_pred, average='macro') \n",
    "    return accuracy,f1score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.9 Please read [Pytorch documentation](https://pytorch.org/tutorials/beginner/saving_loading_models.html) and write code to load the trained model checkpoint from directory: `./ckpt/model_23.pt`.\n",
    "\n",
    "Note this ckpt is a dictionary which includes three keys: epoch, model_state_dict, and optimizer_state_dict.\n",
    "The model parameters are the values of \"model_state_dictm\".\n",
    "\n",
    "\n",
    "\n",
    "rubric={accuracy:2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here \n",
    "checkpoint = torch.load(\"./ckpt/model_23.pt\")\n",
    "model.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.10 Write code to evaluate the trained model on test set and report the test accuracy and F-score. \n",
    "Report the performance of the trained model on the test set is XX.XXX% on accuracy (fill in your accuracy).\n",
    "#### Because there were some issues with the pre-trained model, as long as you loaded it, you'll receive credit. Should be ~50%\n",
    "Report the performance of the trained model on the test set is XX.XXX on F1_score (fill in your F1_score).\n",
    "\n",
    "rubric={accuracy:2,quality:1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here \n",
    "print(evaluate(test_iter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Short Answer Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Non-Linearity Review\n",
    "rubric={reasoning:2}\n",
    "\n",
    "There are a number of different non-linearities that can be used in our neural networks, for instance: **Sigmoid, tanh, and ReLU**. Different variations and tweaks to these non-linearities get introduced fairly frequently, and it pays to have a sense of why you might pick one non-linearity for your network instead of another. **Explain how these three (Sigmoid, tanh, and ReLU) are different and what might be some pros and cons of using them in a neural network**. (There are some great blog/quora posts talking about this topic, but **if you are going to be summarizing from a post or other online resource please include a link to the resource**).\n",
    "\n",
    "\n",
    "Hint: Your answer should be a maximum of 2-3 paragraphs. Short answers are just fine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write your answer here.\n",
    "\n",
    "- Sigmoid and Tanh can both run into issues where the gradients get very small when input gets close to (+inf or -inf), this is the vanishing gradient problem.\n",
    "\n",
    "- Tanh is between (-1,1) which makes training a little bit nice than sigmoid.\n",
    "\n",
    "- ReLU is super fast to compute and doesn't have the vanishing gradient issue.\n",
    "\n",
    "There are of course other things you could have found in your research, mainly looking that you are able to look through some resources and weigh the pros and cons of the different non-linearities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 ReLU Variations\n",
    "rubric={reasoning:1}\n",
    "\n",
    "PyTorch supports several other non-linearities, find two variations on ReLU that PyTorch implements (see https://pytorch.org/docs/stable/nn.html) and explain how they are different from standard ReLU. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write your answer here.\n",
    "Picking one as an example: \n",
    "\n",
    "- Leaky ReLU is like ReLU but allows for some small (tiny!) value when the input is negative. This might be important if you still want to differentiate between negative input values and could potentially be slightly more powerful than ReLU at the expense of being slightly more expensive to compute. \n",
    "\n",
    "There are a lot of other choices, but just consider what might be some benefits for these other \"advanced\" activation functions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Wait why are we using Logs?\n",
    "rubric={reasoning:1}\n",
    "\n",
    "What is the purpose of taking logs of probabilities, as in the case of NLLLoss and LogSoftmax?\n",
    "\n",
    "Hint: A short answer is just fine.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write your answer here.\n",
    "\n",
    "You can run into underflow issues with probabilities. likewise logs of probabilities allow you to calculate things in terms of sums (cheap) as opposed to multiplication, potentially allowing for faster computation of values. If you just care about the biggest probability, the biggest log probability is still going to be larger than the rest, so we can do everything with logs that we could with non-logs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Softmax\n",
    "rubric={reasoning:2}\n",
    "\n",
    "For a multiclass problem with a Softmax layer why might we care about values corresponding to more than one of these classes?\n",
    "\n",
    "Hint: Sometimes knowing what the top n most likely classes is of interest. Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write your answer here.\n",
    "\n",
    "There are a couple of reasons why you might care about the other numbers though, one reason might be if you want to say handle ambiguous situations slightly differently, say you have the top two classes close together, but both fairly low, in some situations you might want to handle things without just predicting the top class.\n",
    "\n",
    "In machine translation you will need to decode the probability outputs to determine what the most likely translation over the course of the sequence. Because each timestep is not independent of earlier timesteps you'll sometimes need to look at outputs other than the most likely output for each timestep. One example of how this is done is using something called beam search, where you keep track of the top-N output sequencences during the decoding.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 RNNs\n",
    "rubric={reasoning:1}\n",
    "\n",
    "RNNs are quite powerful for dealing with certain types of data (such as sequences), however, they have some drawbacks. What are some of these drawbacks? (List two drawbacks)\n",
    "\n",
    "Hint: Look at RNN slides, including the last few slides.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write your answer here.\n",
    "\n",
    "Clear issues include dealing with long sequences and the runtime of the algorithm (it can't be parallelized)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 LSTMs vs RNNs\n",
    "\n",
    "rubric={reasoning:2}\n",
    "\n",
    "LSTMs help alleviate one of the issues with RNNs (see 2.5). What do they alleviate? Do you see any problems that the model might still have (just reason generally about the model based on your understanding).\n",
    "\n",
    "Hint: Short answer is good.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write your answer here.\n",
    "\n",
    "LSTMs are better at sequence length issues, [although attention based methods which we will get to later in the semester turn out to be much better. They will still run into issues with very very long sequence lengths, although now they are able to learn to pay attention to certain things over the course of the sequence. They still have the same weakness in terms of speed, as they can't be parallelized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
