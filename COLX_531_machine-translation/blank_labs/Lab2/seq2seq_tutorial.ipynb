{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NLjpbXW5DRBB"
   },
   "source": [
    "# Seq2Seq model and Evaluation metric - Machine Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4yMxWc16DRBE"
   },
   "source": [
    "### Tutorial Topics\n",
    "- Machine Translation:\n",
    "    - Seq2Seq model\n",
    "    - Evaluation metric\n",
    "\n",
    "### Software Requirements\n",
    "- Python (>=3.6)\n",
    "- PyTorch (>=1.2.0) \n",
    "- Jupyter (latest)\n",
    "- torchtext\n",
    "- NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "YoUOTMQF4_QQ",
    "outputId": "ae0da2ec-cd83-4e93-b6a7-70e64e5488a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_4a1pOgPENwx"
   },
   "source": [
    "## Seq2Seq model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tk6h6fp2DRBP"
   },
   "source": [
    "In this project, we will introduce a neural network to translate from French to English.\n",
    "\n",
    "We will introduce a important architecture in machine translation: [sequence to sequence network](http://arxiv.org/abs/1409.3215), in which two recurrent neural networks work together to transform one sequence to another. An encoder network condenses an input sequence into a **single vector**, and a decoder network unfolds that vector into a new sequence in target language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GpeU8nufDRBP"
   },
   "source": [
    "# Sequence to Sequence Learning\n",
    "\n",
    "A [Sequence to Sequence network](http://arxiv.org/abs/1409.3215), or seq2seq network, or [Encoder Decoder network](https://arxiv.org/pdf/1406.1078v3.pdf), is a model consisting of two separate RNNs called the **`encoder`** and **`decoder`**. The `encoder` reads an input sequence one token at a time, and outputs a vector at each step. The final output of the encoder is kept as the **context** vector. In classification task, we use this **context** vector as the \"summarization\" of input sequence. In seq2seq model, the decoder uses this context vector as the initial state to generate translation. We will dicuss the details in the later section.  \n",
    "\n",
    "![](https://i.imgur.com/tVtHhNp.png)\n",
    "\n",
    " Picture Courtesy: https://i.imgur.com/tVtHhNp.png\n",
    " \n",
    "When using a single RNN, there is a one-to-one relationship between `inputs` and `outputs`. But there are not directly one-to-one relationship between source language and target language. \n",
    "\n",
    "Consider a simple sentence \"`Je ne suis pas le chat noir\"` &rarr; \"`I am not the black cat`\". Many of the words have a pretty direct translation, like \"chat\" &rarr; \"cat\". However the differing grammars cause words to be in different orders, e.g. \"chat noir\" and \"black cat\". There is also the \"ne ... pas\" &rarr; \"not\" construction that makes the two sentences have different lengths.\n",
    "\n",
    "With the seq2seq model, by encoding many source inputs into one vector, and decoding from one vector into many target outputs, we are freed from the constraints of sequence order and length. The encoded sequence is represented by a single vector which is a $N$ dimensional representation. In an ideal case, this vector can be considered as the `\"summarization\"` of the sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L9VR524FDRBQ"
   },
   "source": [
    "### Required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-D5ocUsBDRBR"
   },
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import time\n",
    "import datetime\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence\n",
    "import torchtext\n",
    "from torchtext.datasets import TranslationDataset\n",
    "\n",
    "import spacy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GxjuYtN0DRBU"
   },
   "source": [
    "Here we will also define a constant to decide whether to use the GPU (with CUDA specifically) or the CPU. \n",
    "\n",
    "If you don't have a GPU, set this as CPU. Later when we create tensors, this variable will be used to decide whether we keep them on CPU or move them to GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "AZONroUKDRBV",
    "outputId": "f4a3ab59-a35f-4014-e294-ba1cd8e2b7fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QmhYeZCdDRBY"
   },
   "source": [
    "## 1. Preparing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nh7pBWq_DRBa"
   },
   "source": [
    "***Define tokenizers:***\n",
    "we create the tokenizers. A tokenizer is used to turn a string containing a sentence into a list of individual tokens.\n",
    "\n",
    "`spaCy` has model for each language (\"fr\" for French and \"en\" for English) which need to be loaded so we can access the tokenizer of each model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "O9DyXb3eDRBa"
   },
   "source": [
    "***Note***: the models must first be downloaded using the following on the command line:\n",
    "\n",
    "```\n",
    "python -m spacy download en_core_web_sm\n",
    "python -m spacy download fr_core_news_sm\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "colab_type": "code",
    "id": "qQbDo_trz-yX",
    "outputId": "2acd6e12-1bb9-4a9c-93bc-00fcd83bab99"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: en_core_web_sm==2.1.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.1.0/en_core_web_sm-2.1.0.tar.gz#egg=en_core_web_sm==2.1.0 in /usr/local/lib/python3.6/dist-packages (2.1.0)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n",
      "Requirement already satisfied: fr_core_news_sm==2.1.0 from https://github.com/explosion/spacy-models/releases/download/fr_core_news_sm-2.1.0/fr_core_news_sm-2.1.0.tar.gz#egg=fr_core_news_sm==2.1.0 in /usr/local/lib/python3.6/dist-packages (2.1.0)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('fr_core_news_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm\n",
    "!python -m spacy download fr_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "htgoVt6HDRBb"
   },
   "outputs": [],
   "source": [
    "import fr_core_news_sm\n",
    "import en_core_web_sm\n",
    "\n",
    "spacy_fr = fr_core_news_sm.load()\n",
    "spacy_en = en_core_web_sm.load()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HoOzzEjE3LLa"
   },
   "source": [
    "Next, we create the tokenizer functions. These functions can be provided to TorchText and will take in the sentence as a string and return the sentence as a list of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5i5mYv6h3L7B"
   },
   "outputs": [],
   "source": [
    "def tokenize_fr(text):\n",
    "    \"\"\"\n",
    "    Tokenizes French text from a string into a list of strings (tokens)\n",
    "    \"\"\"\n",
    "    return [tok.text for tok in spacy_fr.tokenizer(text)]\n",
    "\n",
    "def tokenize_en(text):\n",
    "    \"\"\"\n",
    "    Tokenizes English text from a string into a list of strings (tokens)\n",
    "    \"\"\"\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jXWp5zbi3PhB"
   },
   "source": [
    "`TorchText`'s Fields handle how data should be processed. You can read all of the possible arguments [here](https://github.com/pytorch/text/blob/master/torchtext/data/field.py#L61).\n",
    "\n",
    "We set the tokenize argument to the corresponding tokenization function for each, with French being the `SRC` (source) field and English being the `TRG` (target) field. The field also appends the \"start of sequence\" (\\<sos\\>) and \"end of sequence\" (\\<eos\\>) tokens via the `init_token` and `eos_token` arguments, and converts all words to lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CFRXhTeU3OIM"
   },
   "outputs": [],
   "source": [
    "SRC = torchtext.data.Field(tokenize = tokenize_fr, \n",
    "            init_token = '<sos>', \n",
    "            eos_token = '<eos>', \n",
    "            lower = True)\n",
    "TRG = torchtext.data.Field(tokenize = tokenize_en, \n",
    "            init_token = '<sos>', \n",
    "            eos_token = '<eos>', \n",
    "            lower = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OnOUGpiM3YCm"
   },
   "source": [
    "Next, we load the train, validation and test data.\n",
    "\n",
    "The dataset we'll be using is the [Multi30k](https://github.com/multi30k/dataset) dataset. This is a dataset with ~30,000 parallel English, French and German sentences. The length of sentence is around 12 words. You can find more information in [WMT18](http://www.statmt.org/wmt18/multimodal-task.html). This corpus was officially split to Training (29,000 sentences), Validation (1,014 sentences), and multiple Test sets. We provide Test 2016 (1,000 sentences). \n",
    "\n",
    "I extracted the raw dataset to three `.tsv` files. Each file includes two column, 'English' and 'French'. We use `torchtext.data.TabularDataset` to load these tsv files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "opWSiWmN3XKt"
   },
   "outputs": [],
   "source": [
    "train, val, test = torchtext.data.TabularDataset.splits(\n",
    "    path='./drive/My Drive/Colab Notebooks/eng-fre/', train='train_eng_fre.tsv',validation='val_eng_fre.tsv', test='test_eng_fre.tsv', \n",
    "    format='tsv', skip_header=True, fields=[('TRG', TRG), ('SRC', SRC)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L1WwLAWb5XPt"
   },
   "source": [
    "We can double check that we've loaded the right number of examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "lALu0PWW4J4W",
    "outputId": "45c12c80-ddc2-445f-e75b-f05da8a7dea1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 29000\n",
      "Number of validation examples: 1014\n",
      "Number of testing examples: 1000\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of training examples: {len(train.examples)}\")\n",
    "print(f\"Number of validation examples: {len(val.examples)}\")\n",
    "print(f\"Number of testing examples: {len(test.examples)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VkAicvV95kAE"
   },
   "source": [
    "We can also print out an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "1VIBOMQr5X7Q",
    "outputId": "7a52fcd3-8f5f-4b8e-a6e6-b0b18916d9f0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'TRG': ['two', 'young', ',', 'white', 'males', 'are', 'outside', 'near', 'many', 'bushes', '.'], 'SRC': ['deux', 'jeunes', 'hommes', 'blancs', 'sont', 'dehors', 'près', 'de', 'buissons', '.']}\n"
     ]
    }
   ],
   "source": [
    "print(vars(train.examples[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "sJtbuYXD5ltS",
    "outputId": "dcecb5b0-06a3-417f-a61a-121f690473ce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'TRG': ['an', 'older', ',', 'overweight', 'man', 'flips', 'a', 'pancake', 'while', 'making', 'breakfast', '.'], 'SRC': ['un', 'homme', 'âgé', 'en', 'surpoids', 'fait', 'sauter', 'une', 'crêpe', 'en', 'préparant', 'le', 'petit', 'déjeuner', '.']}\n"
     ]
    }
   ],
   "source": [
    "print(vars(val.examples[100]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "soAYLiNT51fe"
   },
   "source": [
    "Next, we'll build the vocabulary for the source and target languages. \n",
    "\n",
    "The vocabulary is used to associate each unique token with an index and this is used to build a one-hot encoding for each token. The vocabularies of the source and target languages are minimal overlap."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zAC6IHWm51k4"
   },
   "source": [
    "Using the `min_freq` argument, we only allow tokens that appear at least 2 times to appear in our vocabulary. Tokens that appear only once are converted into an `<unk>` (unknown) token.\n",
    "\n",
    "It is important to note that your vocabulary should only be built from the `training set` and not the `validation/test set`. This prevents **\"information leakage\"** into your model, giving you artifically inflated validation/test scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zkltTmyK5o8s"
   },
   "outputs": [],
   "source": [
    "TRG.build_vocab(train,min_freq=2)\n",
    "SRC.build_vocab(train,min_freq=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "_Bg8RlqE58yw",
    "outputId": "9428ede6-d535-40c8-ea3e-101932a4c4e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique tokens in source (fr) vocabulary: 6473\n",
      "Unique tokens in target (en) vocabulary: 5893\n"
     ]
    }
   ],
   "source": [
    "print(f\"Unique tokens in source (fr) vocabulary: {len(SRC.vocab)}\")\n",
    "print(f\"Unique tokens in target (en) vocabulary: {len(TRG.vocab)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-orLmAvC51oi"
   },
   "source": [
    "`TRG.vocab.stoi` is the dictionary of word to index. For example, the index of `<pad>` is 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "C0cjuor56E5y",
    "outputId": "eb979a35-d48c-46b4-929f-2d4878b56b61"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(TRG.vocab.stoi['<pad>'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dEePNI326JF1"
   },
   "source": [
    "The final step of preparing the data is to create the `iterators` to generate batches. These can be iterated on to return a batch of data. The text of both source and target text will be converted to two sequence of corresponding indexes, using the vocabularies.\n",
    "\n",
    "\n",
    "We also need to define a `torch.device`. This indicate whether the input `tensors` should be sent to `GPU` or not. We already defined the `device` variable before. \n",
    "\n",
    "Finally, the output of the iterator will be `padded`. \n",
    "\n",
    "We use a `BucketIterator` to creates batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "exU8AMSw7arW"
   },
   "outputs": [],
   "source": [
    "train_iter, val_iter, test_iter = torchtext.data.BucketIterator.splits(\n",
    "    (train, val, test), # we pass in the datasets we want the iterator to draw data from\n",
    "    batch_sizes=(16, 256, 256),device = device,\n",
    "    sort_key=lambda x: len(x.SRC), # the BucketIterator needs to be told what function it should use to group the data.\n",
    "    sort_within_batch=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lPpt9Fgb7glp"
   },
   "source": [
    "Each batch will include two **tensors**: tensor of source language and tensor of target language. The size of each tensor is **[max_length, batch_size]**. Each example is already **padded** within batch dynamically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "ZonpfR9l7c2c",
    "outputId": "a62fd295-3cac-4763-e74f-5dc434559792"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor size of source language: torch.Size([15, 16])\n",
      "tensor size of target language: torch.Size([18, 16])\n",
      "the tensor of first example in target language: tensor([  2,   4, 154,  91,  18, 201,   7, 676,  65,   4, 365,   5,   3,   1,\n",
      "          1,   1,   1,   1], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# batch example of training data\n",
    "for batch in train_iter:\n",
    "    src = batch.SRC\n",
    "    trg = batch.TRG\n",
    "    print('tensor size of source language:', src.shape)\n",
    "    print('tensor size of target language:', trg.shape)\n",
    "    print('the tensor of first example in target language:', trg[:,0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "df3gTAlzvO1l"
   },
   "source": [
    "We save our Fields for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pHw0elIXvM-S"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"./drive/My Drive/Colab Notebooks/ckpt/TRG.Field\",\"wb\")as f:\n",
    "     pickle.dump(TRG,f)\n",
    "\n",
    "with open(\"./drive/My Drive/Colab Notebooks/ckpt/SRC.Field\",\"wb\")as f:\n",
    "     pickle.dump(SRC,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J73SrWVN7nU4"
   },
   "source": [
    "## Building the Seq2Seq Model\n",
    "\n",
    "## 2. Encoder\n",
    "\n",
    "First, we'll build the encoder. We use a single layer `Uni-directional LSTM`.\n",
    "\n",
    "As classifiction task, we only pass the output of embedding layer to the LSTM layer. The LSTM layer returns `outputs`, `hidden` and `cell`. The `hidden` is the final hidden state of LSTM layer. The `cell` is the final cell state of LSTM layer. `hidden` and `cell` can be considered as the **context** representation of source language. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xY9pEqwL7iKs"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, enc_hid_dim,n_layers, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        self.emb_dim = emb_dim\n",
    "        self.enc_hid_dim = enc_hid_dim\n",
    "        self.dropout = dropout\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        self.lstm = nn.LSTM(emb_dim, enc_hid_dim, n_layers, dropout=dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, src):\n",
    "        \n",
    "        #src = [src len, batch size]\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "        \n",
    "        #embedded = [src len, batch size, emb dim]\n",
    "        \n",
    "        outputs, (hidden, cell) = self.lstm(embedded)\n",
    "       \n",
    "        # outputs are always from the top hidden layer, if bidirectional outputs are concatenated.\n",
    "        # outputs shape [sequence_length, batch_size, hidden_dim * num_directions]\n",
    "        # hidden is of shape [num_layers * num_directions, batch_size, hidden_size]\n",
    "        # cell is of shape [num_layers * num_directions, batch_size, hidden_size]\n",
    "        \n",
    "        return hidden, cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gV9fweNEKNcp"
   },
   "source": [
    "## 3. Decoder\n",
    "\n",
    "Next up is the decoder. Decoder is a `uni-directional LSTM`.\n",
    "\n",
    "\n",
    "At time step $t$, the input of decoder LSTM is embeded word vector of $t$th target word , $y_t$, the previous decoder hidden state, $h_{t-1}$, and the previous decoder hidden cell, $c_{t-1}$.\n",
    "\n",
    "$$h_t, c_t = \\text{DecoderLSTM}(y_t, (h_{t-1}, c_{t-1}))$$\n",
    "\n",
    "Specially, we will use the last `hidden state` and `cell state` as the initial states of decoder LSTM (i.e., $h_{0}, c_{0}$) rather than randomly initialize them. \n",
    "\n",
    "We then pass hidden state of LSTM layer, $h_t$, through the linear layer, $f$, to make a prediction of the next word in the target sentence, $\\hat{y}_{t+1}$. \n",
    "\n",
    "$$\\hat{y}_{t+1} = f(h_t)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bDL3CtVjKKS3"
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, dec_hid_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        self.emb_dim = emb_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.dec_hid_dim = dec_hid_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        self.lstm = nn.LSTM(emb_dim, dec_hid_dim, n_layers, dropout=dropout)\n",
    "        self.fc_out = nn.Linear(dec_hid_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, input, hidden, cell):\n",
    "             \n",
    "        # input is of shape [batch_size]\n",
    "        # hidden is of shape [n_layer * num_directions, batch_size, hidden_size]\n",
    "        # cell is of shape [n_layer * num_directions, batch_size, hidden_size]\n",
    "        \n",
    "        input = input.unsqueeze(0)\n",
    "        \n",
    "        # input shape is [1, batch_size]. reshape is needed rnn expects a rank 3 tensors as input.\n",
    "        # so reshaping to [1, batch_size] means a batch of batch_size each containing 1 index.\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        \n",
    "        #embedded = [1, batch size, emb dim]    \n",
    "        output, (hidden, cell) = self.lstm(embedded, (hidden, cell))\n",
    "        \n",
    "        # output shape is [sequence_len, batch_size, hidden_dim * num_directions]\n",
    "        # hidden shape is [num_layers * num_directions, batch_size, hidden_dim]\n",
    "        # cell shape is [num_layers * num_directions, batch_size, hidden_dim]\n",
    "\n",
    "        # sequence_len and num_directions will always be 1 in the decoder.\n",
    "        # output shape is [1, batch_size, hidden_dim]\n",
    "        # hidden shape is [num_layers, batch_size, hidden_dim]\n",
    "        # cell shape is [num_layers, batch_size, hidden_dim]\n",
    "        \n",
    "        prediction = self.fc_out(hidden.squeeze(0)) # linear expects as rank 2 tensor as input\n",
    "        # predicted shape is [batch_size, output_dim]\n",
    "        \n",
    "        return prediction, hidden, cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E5sxYN1dYIrs"
   },
   "source": [
    "## 4. Seq2Seq\n",
    "\n",
    "The `encoder` returns both the final `hidden state` and `cell state` to be used as the initial `hidden state` and `cell state` for the `decoder`.\n",
    "\n",
    "Briefly going over all of the steps:\n",
    "- the `outputs` tensor is created to hold all predictions, $\\hat{Y} = \\{\\hat{y_0}, \\hat{y_1} ... \\hat{y_t}\\}$;\n",
    "- the source sequence, $X = \\{x_0,x_1,..., x_t\\}$, is fed into the encoder to receive last hidden state, $h^{Encoder}_t$, and last cell state $c^{Encoder}_t$;\n",
    "- the initial decoder hidden state is set to be the $h^{Encoder}_t$, and the initial decoder cell state is set to be the $c^{Encoder}_t$. (i.e., $h^{Decoder}_0$ = $h^{Encoder}_t$; $c^{Decoder}_0$ = $c^{Encoder}_t$);\n",
    "- we use a batch of `<sos>` tokens as the first `input` (i.e., $y_0$);\n",
    "- we then decode within a loop:\n",
    "\n",
    " for i in range(1,t): t is the maximal length of target language\n",
    "  - inserting the input token $y_i$, previous hidden state, $h^{Decoder}_{i-1}$, and previous cell state, $c^{Decoder}_{i-1}$, into the decoder;\n",
    "  - receiving a prediction, $\\hat{y}_{i+1}$, which is the most likely output sequence, a new hidden state, $h^{Decoder}_{i}$, and a new cell state, $c^{Decoder}_{i}$;\n",
    "  - we then decide if we are going to **teacher force** or not, setting the next input as appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Qu6YDrp0Y2Rc"
   },
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    ''' This class contains the implementation of complete sequence to sequence network.\n",
    "    It uses to encoder to produce the context vectors.\n",
    "    It uses the decoder to produce the predicted target sentence.\n",
    "    Args:\n",
    "        encoder: A Encoder class instance.\n",
    "        decoder: A Decoder class instance.\n",
    "    '''\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
    "        # src is of shape [sequence_len, batch_size]\n",
    "        # trg is of shape [sequence_len, batch_size]\n",
    "        # if teacher_forcing_ratio is 0.5 we use ground-truth inputs 50% of time and 50% time we use decoder outputs.\n",
    "\n",
    "        batch_size = trg.shape[1]\n",
    "        max_len = trg.shape[0]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "\n",
    "        # to store the outputs of the decoder\n",
    "        outputs = torch.zeros(max_len, batch_size, trg_vocab_size).to(self.device)\n",
    "\n",
    "        # context vector, last hidden and cell state of encoder to initialize the decoder\n",
    "        hidden, cell = self.encoder(src)\n",
    "\n",
    "        # first input to the decoder is the <sos> tokens\n",
    "        input = trg[0, :]\n",
    "\n",
    "        for t in range(1, max_len):\n",
    "            output, hidden, cell = self.decoder(input, hidden, cell)\n",
    "            outputs[t] = output\n",
    "            use_teacher_force = random.random() < teacher_forcing_ratio\n",
    "            top1 = output.max(1)[1]\n",
    "            input = (trg[t] if use_teacher_force else top1)\n",
    "\n",
    "        # outputs is of shape [sequence_len, batch_size, output_dim]\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c3NXkVmygfhv"
   },
   "source": [
    "## 5. Training the Seq2Seq Model\n",
    "We instantiate our encoder, decoder and seq2seq model (placing it on the GPU if we have one). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "id": "q_ZqMyitGX-5",
    "outputId": "67b7634f-0a35-49a9-cdbc-0fe659ef6c04"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py:50: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py:50: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    }
   ],
   "source": [
    "INPUT_DIM = len(SRC.vocab)\n",
    "OUTPUT_DIM = len(TRG.vocab)\n",
    "ENC_EMB_DIM = 256\n",
    "DEC_EMB_DIM = 256\n",
    "ENC_HID_DIM = 512\n",
    "DEC_HID_DIM = 512\n",
    "ENC_DROPOUT = 0.5\n",
    "DEC_DROPOUT = 0.3\n",
    "N_LAYERS = 1\n",
    "LEARNING_RT = 0.001\n",
    "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, N_LAYERS, ENC_DROPOUT)\n",
    "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, DEC_HID_DIM, N_LAYERS, DEC_DROPOUT)\n",
    "\n",
    "model = Seq2Seq(enc, dec,device).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jMIV3Tnsh3lU"
   },
   "source": [
    "We use a simplified version of the **weight initialization scheme**. Here, we will initialize all biases to zero and all weights from $\\mathcal{N}(0, 0.05)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "colab_type": "code",
    "id": "k5tsPrp-GZmh",
    "outputId": "c8a93d7d-7f62-4e07-e18b-8544c166386f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): Encoder(\n",
       "    (embedding): Embedding(6473, 256)\n",
       "    (lstm): LSTM(256, 512, dropout=0.5)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (embedding): Embedding(5893, 256)\n",
       "    (lstm): LSTM(256, 512, dropout=0.3)\n",
       "    (fc_out): Linear(in_features=512, out_features=5893, bias=True)\n",
       "    (dropout): Dropout(p=0.3, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 120,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            nn.init.normal_(param.data, mean=0, std=0.01)\n",
    "        else:\n",
    "            nn.init.constant_(param.data, 0)\n",
    "            \n",
    "model.apply(init_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6fMQZFQTiCjx"
   },
   "source": [
    "Calculate the number of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "f9w79-_fiByo",
    "outputId": "cde12214-7c0f-4ed5-94e4-28dc129f595b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 9,342,725 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M9aNXTTZiHts"
   },
   "source": [
    "Create an optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zkdoXq8aiEoF"
   },
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr = LEARNING_RT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xMICykcSiN_k"
   },
   "source": [
    "Initialize the loss function. The pad token will be ignore. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "xbjJMMSZiL7e",
    "outputId": "96687623-8aa8-4042-f461-77fada8b2bd6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pad> token index:  1\n"
     ]
    }
   ],
   "source": [
    "TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]\n",
    "print('<pad> token index: ',TRG_PAD_IDX)\n",
    "## we will ignor the pad token in true target set\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eB5clLvrieXF"
   },
   "source": [
    "### Testing Model \n",
    "We will run the model with first training batch to test our code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "4yA7zAVBidXt",
    "outputId": "73ad2d11-b501-42ed-c3bf-494ab7ade8cb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5426, device='cuda:0', grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "clip = 1\n",
    "model.train()\n",
    "\n",
    "for i, batch in enumerate(train_iter):\n",
    "\n",
    "    src = batch.SRC\n",
    "    trg = batch.TRG\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    output = model(src, trg)\n",
    "    #trg = [trg len, batch size]\n",
    "    #output = [trg len, batch size, output dim]\n",
    "\n",
    "    output_dim = output.shape[-1]\n",
    "\n",
    "    output = output[1:].view(-1, output_dim)\n",
    "    trg = trg[1:].view(-1)\n",
    "\n",
    "    #trg = [(trg len - 1) * batch size]\n",
    "    #output = [(trg len - 1) * batch size, output dim]\n",
    "\n",
    "    loss = criterion(output, trg)\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "    print(loss/src.shape[1])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tl_MI5-Hlp60"
   },
   "source": [
    "## Fully training process\n",
    "If we test our code successfully. We will start the fully training loop as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dg1-WRQ3lqRA"
   },
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, clip):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for i, batch in enumerate(iterator):\n",
    "        \n",
    "        src = batch.SRC\n",
    "        trg = batch.TRG\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(src, trg)\n",
    "        \n",
    "        #trg = [trg len, batch size]\n",
    "        #output = [trg len, batch size, output dim]\n",
    "        \n",
    "        output_dim = output.shape[-1]\n",
    "        \n",
    "        output = output[1:].view(-1, output_dim)\n",
    "        trg = trg[1:].view(-1)\n",
    "        \n",
    "        # loss function works only 2d logits, 1d targets\n",
    "        # so flatten the trg, output tensors. Ignore the <sos> token\n",
    "        # trg shape shape should be [(sequence_len - 1) * batch_size]\n",
    "        # output shape should be [(sequence_len - 1) * batch_size, output_dim]\n",
    "        \n",
    "        loss = criterion(output, trg)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XSQfQpQclx1C"
   },
   "source": [
    "...and the evaluation loop, remembering to set the model to `eval` mode and turn off teaching forcing (i.e., teach forcing rate = 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HIy-h7i5lwd4"
   },
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for i, batch in enumerate(iterator):\n",
    "\n",
    "            src = batch.SRC\n",
    "            trg = batch.TRG\n",
    "\n",
    "            output = model(src, trg, 0) #turn off teacher forcing\n",
    "\n",
    "            #trg = [trg len, batch size]\n",
    "            #output = [trg len, batch size, output dim]\n",
    "\n",
    "            output_dim = output.shape[-1]\n",
    "            \n",
    "            output = output[1:].view(-1, output_dim)\n",
    "            trg = trg[1:].view(-1)\n",
    "\n",
    "            #trg = [(trg len - 1) * batch size]\n",
    "            #output = [(trg len - 1) * batch size, output dim]\n",
    "\n",
    "            loss = criterion(output, trg)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iUeJF_N1NnCD"
   },
   "source": [
    "Count the running time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H3rv8T7Vly1c"
   },
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gQ8OyS2aNs3_"
   },
   "source": [
    "## Training model. \n",
    "\n",
    "We will train the model for 10 epochs. At the end of each epoch, we will save a checkpoint and evaluate on the development set. We will print out the loss and perplexity of train and dev set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 527
    },
    "colab_type": "code",
    "id": "oaDCA9HOl4XP",
    "outputId": "0794969f-57d2-476a-9982-5219bd5f2e6d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Time: 0m 57s\n",
      "\tTrain Loss: 4.363 | Train PPL:  78.457\n",
      "\t Val. Loss: 4.357 |  Val. PPL:  78.048\n",
      "Epoch: 02 | Time: 0m 57s\n",
      "\tTrain Loss: 3.508 | Train PPL:  33.396\n",
      "\t Val. Loss: 3.946 |  Val. PPL:  51.707\n",
      "Epoch: 03 | Time: 0m 57s\n",
      "\tTrain Loss: 3.021 | Train PPL:  20.503\n",
      "\t Val. Loss: 3.691 |  Val. PPL:  40.074\n",
      "Epoch: 04 | Time: 0m 57s\n",
      "\tTrain Loss: 2.652 | Train PPL:  14.177\n",
      "\t Val. Loss: 3.439 |  Val. PPL:  31.153\n",
      "Epoch: 05 | Time: 0m 57s\n",
      "\tTrain Loss: 2.364 | Train PPL:  10.630\n",
      "\t Val. Loss: 3.347 |  Val. PPL:  28.430\n",
      "Epoch: 06 | Time: 0m 57s\n",
      "\tTrain Loss: 2.132 | Train PPL:   8.433\n",
      "\t Val. Loss: 3.289 |  Val. PPL:  26.818\n",
      "Epoch: 07 | Time: 0m 56s\n",
      "\tTrain Loss: 1.937 | Train PPL:   6.936\n",
      "\t Val. Loss: 3.298 |  Val. PPL:  27.066\n",
      "Epoch: 08 | Time: 0m 56s\n",
      "\tTrain Loss: 1.776 | Train PPL:   5.908\n",
      "\t Val. Loss: 3.283 |  Val. PPL:  26.667\n",
      "Epoch: 09 | Time: 0m 57s\n",
      "\tTrain Loss: 1.627 | Train PPL:   5.090\n",
      "\t Val. Loss: 3.382 |  Val. PPL:  29.437\n",
      "Epoch: 10 | Time: 0m 56s\n",
      "\tTrain Loss: 1.492 | Train PPL:   4.448\n",
      "\t Val. Loss: 3.432 |  Val. PPL:  30.927\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 10\n",
    "CLIP = 1\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss = train(model, train_iter, optimizer, criterion, CLIP)\n",
    "    valid_loss = evaluate(model, val_iter, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    # Create checkpoint at end of each epoch\n",
    "    state_dict_model = model.state_dict() \n",
    "    state = {\n",
    "        'epoch': epoch,\n",
    "        'state_dict': state_dict_model,\n",
    "        'optimizer': optimizer.state_dict()\n",
    "        }\n",
    "\n",
    "    torch.save(state, \"./drive/My Drive/Colab Notebooks/ckpt/seq2seq_\"+str(epoch+1)+\".pt\")\n",
    "\n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X7oPEKhFwIR3"
   },
   "source": [
    "## 6. Load Checkpoint\n",
    "We will use the best model for the following process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MDDP7hB8wZId"
   },
   "source": [
    "Load the saved TRG and SRC fields:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3kAeYOfCwW_U"
   },
   "outputs": [],
   "source": [
    "with open(\"./drive/My Drive/Colab Notebooks/ckpt/TRG.Field\",\"rb\") as f:\n",
    "     TRG_saved = pickle.load(f)\n",
    "\n",
    "with open(\"./drive/My Drive/Colab Notebooks/ckpt/SRC.Field\",\"rb\") as f:\n",
    "     SRC_saved = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yGMLkVAzzLIW"
   },
   "source": [
    "Load trained model to `model_best` and put model on device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "id": "TCp-dPjIzP3p",
    "outputId": "2ad2b579-054c-424b-8d1a-ed5ae7a2b011"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py:50: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py:50: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    }
   ],
   "source": [
    "INPUT_DIM = len(SRC_saved.vocab)\n",
    "OUTPUT_DIM = len(TRG_saved.vocab)\n",
    "ENC_EMB_DIM = 256\n",
    "DEC_EMB_DIM = 256\n",
    "ENC_HID_DIM = 512\n",
    "DEC_HID_DIM = 512\n",
    "ENC_DROPOUT = 0.5\n",
    "DEC_DROPOUT = 0.3\n",
    "N_LAYERS = 1\n",
    "LEARNING_RT = 0.001\n",
    "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, N_LAYERS, ENC_DROPOUT)\n",
    "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, DEC_HID_DIM, N_LAYERS, DEC_DROPOUT)\n",
    "\n",
    "model_best = Seq2Seq(enc, dec, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wBmOZKhlzP6x"
   },
   "outputs": [],
   "source": [
    "model_best.load_state_dict(torch.load('./drive/My Drive/Colab Notebooks/ckpt/seq2seq_7.pt')['state_dict'])\n",
    "model_best = model_best.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fR52wcB71MTN"
   },
   "source": [
    "Pre-process source language and get input tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "2B2UZ-Aj0_7P",
    "outputId": "8df45b12-5b33-42ae-a749-674362cc0bc8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src_token: ['une', 'femme', 'avec', 'un', 'gros', 'sac', 'passe', 'par', 'une', 'porte', '.']\n",
      "shape of source language:  torch.Size([13, 1])\n"
     ]
    }
   ],
   "source": [
    "model_best.eval()\n",
    "src_token = SRC_saved.preprocess('Une femme avec un gros sac passe par une porte.')\n",
    "print(\"src_token:\", src_token)\n",
    "src_tensor = SRC_saved.process([src_token],device=device)\n",
    "print(\"shape of source language: \", src_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4x0XCWoY1zPZ"
   },
   "source": [
    "We assume this is a test sample so we don't have the gold target text. We create a placeholder for target language which only includes 64 `<pad>` tokens (i.e., the maximal length of our translated generation is 64)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "AplyDtKz1KS9",
    "outputId": "c895ec9b-da59-4c29-ea67-fc378ed88c13"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of target language:  torch.Size([66, 1])\n"
     ]
    }
   ],
   "source": [
    "trg_token = ['<pad>']*64\n",
    "trg_tensor = TRG_saved.process([trg_token],device=device)\n",
    "print(\"shape of target language: \", trg_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hAg8yOj61HKL"
   },
   "outputs": [],
   "source": [
    "output = model_best(src_tensor, trg_tensor,teacher_forcing_ratio = 0.0)\n",
    "output_dim = output.shape[-1]\n",
    "# get translation results, we ignor first token <sos> in both translation and target sentences. \n",
    "# output_translate = [(trg len - 1), batch, output dim] output dim is size of target vocabulary. \n",
    "output_translate = output[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V9iSIpaV5bzr"
   },
   "source": [
    "Detach the source input tensor to CPU device because our following process will operate on CPU. Then, squeeze the shape of `output_translate` to `[(trg len - 1),output dim]`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KtXb6Cz75C09"
   },
   "outputs": [],
   "source": [
    "source_languege = src_tensor[:,0].cpu().numpy()\n",
    "translation_logit = output_translate[:,0,:].squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "80F7qWb05Prb",
    "outputId": "b06d31bf-0d2b-4fb4-ce36-3718d308c038"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   4   14   13    4   59  645   10   41   49    4 1024    5    3    3\n",
      "    5    3    3    3    3    3    3    3    3    3    3    3    3    3\n",
      "    3    3    3    3    3    3    3    3    3    3    3    3    3    3\n",
      "    3    3    3    3    3    3    3    3    3    3    3    3    3    3\n",
      "    3    3    3    3    3    3    3    3    3]\n"
     ]
    }
   ],
   "source": [
    "# Choose top 1 word from decoder's output, we get the probability and index of the word\n",
    "prob, token_id = translation_logit.data.topk(1)\n",
    "token_id = token_id.squeeze(1).cpu().numpy()\n",
    "print(token_id) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9Ns0cqJA1G3X"
   },
   "source": [
    "To get the translation in text, we will use a dictionary to convert index to word.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "2_tawtQC4XP8",
    "outputId": "4ed5c5c8-4294-4ab2-c93d-c34449965549"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source language: ['<sos>', 'une', 'femme', 'avec', 'un', 'gros', 'sac', 'passe', 'par', 'une', 'porte', '.']\n",
      "Our model translation:  ['a', 'woman', 'with', 'a', 'large', 'purse', 'is', 'walking', 'by', 'a', 'gate', '.']\n",
      "Golden translation:  a woman with a large purse is walking by a gate.\n"
     ]
    }
   ],
   "source": [
    "# get source langauge in text\n",
    "src_langauge = []\n",
    "\n",
    "for i in source_languege:\n",
    "    if i == SRC.vocab.stoi['<eos>']:\n",
    "        break\n",
    "    else:\n",
    "        token = SRC.vocab.itos[i]\n",
    "        src_langauge.append(token)\n",
    "print(\"Source language:\", src_langauge)\n",
    "\n",
    "# get machine translation in text\n",
    "trans_langauge = []\n",
    "for i in token_id:\n",
    "    if i == TRG.vocab.stoi['<eos>']:\n",
    "        break\n",
    "    else:\n",
    "        token = TRG.vocab.itos[i]\n",
    "        trans_langauge.append(token)\n",
    "print(\"Our model translation: \", trans_langauge)\n",
    "print(\"Golden translation: \", \"a woman with a large purse is walking by a gate.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "86njWz6Z_m1a"
   },
   "source": [
    "## 7. Evaluation \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L1WSy4bF_pm8"
   },
   "source": [
    "### Evaluation with perplexity\n",
    "\n",
    "[Perplexity](https://en.wikipedia.org/wiki/Perplexity) is an information theoretic measurement of how well probability model predicts. Low perplexity values indicate a better fit. In language modeling, we build a language genration system with state distribution `q` which tries to mimic the real-world language with the state distribution `p` as much as possible. However, in practice we do not know the exact `p`, we only know $\\tilde{p}$  which is a sampled distribution (i.e., our dataset) from the real-world system. We use a perplexity with cross entropy $PPL( \\tilde{p} ,q)$. Perplexity is defined as: \n",
    "\n",
    "$PPL( \\tilde{p} ,q) = e^{Entropy[\\tilde{p},q]}$\n",
    "\n",
    "where $Entropy[\\tilde{p},q]$ is the `CrossEntropy` loss of our translation model, q is our translation system.\n",
    "We can directly use `math.exp(train_loss)` get the perplexity of our model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JFxWpf6R_gHo"
   },
   "source": [
    "### Evaluation with BLEU score\n",
    "\n",
    "The Bilingual Evaluation Understudy Score, or BLEU for short, is a metric for evaluating a generated sentence to a reference sentence.\n",
    "\n",
    "NLTK provides the [sentence_bleu()](http://www.nltk.org/api/nltk.translate.html#nltk.translate.bleu_score.sentence_bleu) function for evaluating a candidate sentence against one or more reference sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Uli2x7KsCUZZ"
   },
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d6U9KrtSN1YT"
   },
   "source": [
    " Let's take a look at a simple example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "fbrmfOKRN9BI",
    "outputId": "e4b6e63a-c0ce-4ef3-edaa-4b74f53f8832"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# two references for one document\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "references = [[['this', 'is', 'a', 'test'], ['this', 'is' 'test']]]\n",
    "candidates = [['this', 'is', 'a', 'test']]\n",
    "score = corpus_bleu(references, candidates)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2MOjyg8AOAHP"
   },
   "source": [
    "By default, the sentence_bleu() and corpus_bleu() scores calculate the cumulative 4-gram BLEU score, also called **BLEU-4**. The weights for the BLEU-4 are 1/4 (25%) or 0.25 for each of the 1-gram, 2-gram, 3-gram and 4-gram scores. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "xU-NaJr0N9wf",
    "outputId": "69c4a96c-f49c-4491-ac72-8512f18973c0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens:\t ['a', 'woman', 'with', 'a', 'large', 'purse', 'is', 'walking', 'by', 'a', 'gate', '.']\n",
      "Our model translation:\t ['a', 'woman', 'with', 'a', 'large', 'purse', 'is', 'walking', 'by', 'a', 'gate', '.']\n",
      "BLEU score: 0.6389431042462724\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "# now, use BLEU evaluate our translation\n",
    "\n",
    "# tokenize our golden sentence\n",
    "tokens = TRG.preprocess(\"a woman with a large purse is walking by a gate.\")\n",
    "print(\"tokens:\\t\", tokens)\n",
    "reference = [[tokens]]\n",
    "print(\"Our model translation:\\t\", trans_langauge)\n",
    "our_translation = [trans_langauge]\n",
    "\n",
    "score = corpus_bleu(references, our_translation) #corpus_bleu(reference, our_translation, weights=(0.25, 0.25, 0.25, 0.25))\n",
    "print(\"BLEU score:\",score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NX7COgIhOPnt"
   },
   "source": [
    "The cumulative and individual 1-gram BLEU use the same weights, e.g. (1, 0, 0, 0). The 2-gram weights assign a 50% to each of 1-gram and 2-gram and the 3-gram weights are 33% for each of the 1, 2 and 3-gram scores.\n",
    "\n",
    "Let’s make this concrete by calculating the cumulative scores for BLEU-1, BLEU-2, BLEU-3 and BLEU-4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "id": "o9OZ3SxrOQCb",
    "outputId": "8daa48f9-1244-46f3-d889-9ff7fdfdf9d0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cumulative 1-gram: 0.166667\n",
      "Cumulative 2-gram: 0.408248\n",
      "Cumulative 3-gram: 0.553618\n",
      "Cumulative 4-gram: 0.638943\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "# cumulative BLEU scores\n",
    "print('Cumulative 1-gram: %f' % corpus_bleu(references, our_translation, weights=(1, 0, 0, 0)))\n",
    "print('Cumulative 2-gram: %f' % corpus_bleu(references, our_translation, weights=(0.5, 0.5, 0, 0)))\n",
    "print('Cumulative 3-gram: %f' % corpus_bleu(references, our_translation, weights=(0.33, 0.33, 0.33, 0)))\n",
    "print('Cumulative 4-gram: %f' % corpus_bleu(references, our_translation, weights=(0.25, 0.25, 0.25, 0.25)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bTBzai_C9tN0"
   },
   "source": [
    "## Reference \n",
    "* https://pytorch.org/docs/stable/nn.html\n",
    "* https://github.com/bentrevett/pytorch-seq2seq/blob/master/1%20-%20Sequence%20to%20Sequence%20Learning%20with%20Neural%20Networks.ipynb\n",
    "* https://arxiv.org/abs/1409.3215\n",
    "* https://github.com/graviraja/seq2seq\n",
    "* https://github.com/eladhoffer/seq2seq.pytorch\n",
    "* https://github.com/spro/practical-pytorch/tree/master/seq2seq-translation\n",
    "* http://mlexplained.com/2018/02/08/a-comprehensive-tutorial-to-torchtext/\n",
    "* https://machinelearningmastery.com/calculate-bleu-score-for-text-python/\n",
    "* https://leimao.github.io/blog/Entropy-Perplexity/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G7lQdLyN6977"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "seq2seq_tutorial.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
