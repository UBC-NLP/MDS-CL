{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "baseline_machine_translation_vanilla_seq2seq.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdDpixSte1yy",
        "colab_type": "text"
      },
      "source": [
        "# Baseline code for Machine Translation system using standard Seq2seq model\n",
        "\n",
        "In this notebook, we will provide the implementation of a machine translation system using standard Seq2seq model (discussed in Week 2 of our course). This baseline code can be considered as a starting point for the first question of Lab4 where we need to build an effective model than can translate sentences from Portuguese language to English language. Alternatively, we can ignore this baseline code and build our machine translation model by writing modules from scratch. Look at Lab4.ipynb for futher information.\n",
        "\n",
        "This notebook assumes the following:\n",
        "* the data is at `./drive/My Drive/machine_translation`\n",
        "* the model checkpoint is at each epoch and other intermediate files will be stored at `./drive/My Drive/Colab Notebooks/ckpt_mt_lab4`\n",
        "* the scorer.py is at `./drive/My Drive/scorer.py`\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zPwQ0ODRfWqz",
        "colab_type": "code",
        "outputId": "4682b670-738b-4787-e443-258f31339b56",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "De5_yaam200J",
        "colab_type": "code",
        "outputId": "557c6fdb-033f-4f27-ce34-29ab8b0d16cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# required libraries\n",
        "\n",
        "import unicodedata\n",
        "import string\n",
        "import re\n",
        "import random\n",
        "import time\n",
        "import datetime\n",
        "import math\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence\n",
        "import torchtext\n",
        "from torchtext.datasets import TranslationDataset\n",
        "\n",
        "import spacy\n",
        "import numpy as np\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "\n",
        "# set the device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PSVn4bw-3D5-",
        "colab_type": "text"
      },
      "source": [
        "## Data preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wK6scNsy3FxR",
        "colab_type": "code",
        "outputId": "b6a6b32e-da98-46a0-e7fb-29b924db6ffb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365
        }
      },
      "source": [
        "'''\n",
        "tokenization code\n",
        "'''\n",
        "\n",
        "!python -m spacy download pt_core_news_sm\n",
        "!python -m spacy download en_core_web_sm\n",
        "\n",
        "import pt_core_news_sm\n",
        "import en_core_web_sm\n",
        "\n",
        "spacy_pt = pt_core_news_sm.load()\n",
        "spacy_en = en_core_web_sm.load()\n",
        "\n",
        "def tokenize_pt(text):\n",
        "    \"\"\"\n",
        "    Tokenizes Portuguese text from a string into a list of strings (tokens)\n",
        "    \"\"\"\n",
        "    return [tok.text for tok in spacy_pt.tokenizer(text)]\n",
        "\n",
        "def tokenize_en(text):\n",
        "    \"\"\"\n",
        "    Tokenizes English text from a string into a list of strings (tokens)\n",
        "    \"\"\"\n",
        "    return [tok.text for tok in spacy_en.tokenizer(text)]\n",
        "\n",
        "'''\n",
        "define field\n",
        "'''\n",
        "SRC = torchtext.data.Field(tokenize = tokenize_pt, \n",
        "            init_token = '<sos>', \n",
        "            eos_token = '<eos>', \n",
        "            lower = True)\n",
        "TRG = torchtext.data.Field(tokenize = tokenize_en, \n",
        "            init_token = '<sos>', \n",
        "            eos_token = '<eos>', \n",
        "            lower = True)\n",
        "\n",
        "'''\n",
        "load the data\n",
        "'''\n",
        "train_data = torchtext.data.TabularDataset(\n",
        "    path='./drive/My Drive/machine_translation/mt-train-pt2en.tsv', \n",
        "    format='tsv', skip_header=True, fields=[('SRC', SRC), ('TRG', TRG)])\n",
        "valid_data = torchtext.data.TabularDataset(\n",
        "    path='./drive/My Drive/machine_translation/mt-valid-pt2en.tsv', \n",
        "    format='tsv', skip_header=True, fields=[('SRC', SRC), ('TRG', TRG)])\n",
        "test_data = torchtext.data.TabularDataset(\n",
        "    path='./drive/My Drive/machine_translation/mt-test-pt2en_onlySRCpt.tsv', \n",
        "    format='tsv', skip_header=True, fields=[('SRC', SRC)]) # blind test data (that is, no targets)\n",
        "\n",
        "print(f\"Number of training examples: {len(train_data.examples)}\")\n",
        "print(f\"Number of validation examples: {len(valid_data.examples)}\")\n",
        "print(f\"Number of testing examples: {len(test_data.examples)}\")\n",
        "\n",
        "'''\n",
        "build the vocabulary\n",
        "'''\n",
        "TRG.build_vocab(train_data, min_freq=2)\n",
        "SRC.build_vocab(train_data, min_freq=2)\n",
        "print(f\"Unique tokens in source (fr) vocabulary: {len(SRC.vocab)}\")\n",
        "print(f\"Unique tokens in target (en) vocabulary: {len(TRG.vocab)}\")\n",
        "\n",
        "'''\n",
        "create the iterator\n",
        "'''\n",
        "train_iter = torchtext.data.BucketIterator(train_data, batch_size=16, device=device, sort_key=lambda x: len(x.SRC), sort_within_batch=True)\n",
        "valid_iter = torchtext.data.BucketIterator(valid_data, batch_size=256, device=device, sort_key=lambda x: len(x.SRC), sort_within_batch=True)\n",
        "test_iter = torchtext.data.Iterator(test_data, batch_size=256, device=device, sort=False, sort_key=None, shuffle=False, sort_within_batch=False)\n",
        "\n",
        "'''\n",
        "print sample batch\n",
        "'''\n",
        "# print first batch of training data\n",
        "print('training batch')\n",
        "for batch in train_iter:\n",
        "    src = batch.SRC\n",
        "    trg = batch.TRG\n",
        "    print('tensor size of source language:', src.shape)\n",
        "    print('tensor size of target language:', trg.shape)\n",
        "    break\n",
        "\n",
        "# print first batch of validation data\n",
        "print('validation batch')\n",
        "for batch in valid_iter:\n",
        "    src = batch.SRC\n",
        "    trg = batch.TRG\n",
        "    print('tensor size of source language:', src.shape)\n",
        "    print('tensor size of target language:', trg.shape)\n",
        "    break\n",
        "\n",
        "# print first batch of test data\n",
        "print('(blind) test batch')\n",
        "for batch in test_iter:\n",
        "    src = batch.SRC\n",
        "    print('tensor size of source language:', src.shape)\n",
        "    break\n",
        "\n",
        "# save the field\n",
        "import pickle\n",
        "with open(\"./drive/My Drive/Colab Notebooks/ckpt_mt_lab4/TRG.Field\",\"wb\")as f:\n",
        "     pickle.dump(TRG,f)\n",
        "\n",
        "with open(\"./drive/My Drive/Colab Notebooks/ckpt_mt_lab4/SRC.Field\",\"wb\")as f:\n",
        "     pickle.dump(SRC,f)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pt_core_news_sm==2.1.0 from https://github.com/explosion/spacy-models/releases/download/pt_core_news_sm-2.1.0/pt_core_news_sm-2.1.0.tar.gz#egg=pt_core_news_sm==2.1.0 in /usr/local/lib/python3.6/dist-packages (2.1.0)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('pt_core_news_sm')\n",
            "Requirement already satisfied: en_core_web_sm==2.1.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.1.0/en_core_web_sm-2.1.0.tar.gz#egg=en_core_web_sm==2.1.0 in /usr/local/lib/python3.6/dist-packages (2.1.0)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "Number of training examples: 85918\n",
            "Number of validation examples: 4772\n",
            "Number of testing examples: 4738\n",
            "Unique tokens in source (fr) vocabulary: 7891\n",
            "Unique tokens in target (en) vocabulary: 2320\n",
            "training batch\n",
            "tensor size of source language: torch.Size([10, 16])\n",
            "tensor size of target language: torch.Size([12, 16])\n",
            "validation batch\n",
            "tensor size of source language: torch.Size([8, 256])\n",
            "tensor size of target language: torch.Size([11, 256])\n",
            "(blind) test batch\n",
            "tensor size of source language: torch.Size([16, 256])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZGoARLtC9QV",
        "colab_type": "text"
      },
      "source": [
        "## Seq2seq model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WpPua3bKDWU2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, emb_dim, enc_hid_dim,n_layers, dropout):\n",
        "        super().__init__()\n",
        "\n",
        "        self.emb_dim = emb_dim\n",
        "        self.enc_hid_dim = enc_hid_dim\n",
        "        self.dropout = dropout\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
        "        self.lstm = nn.LSTM(emb_dim, enc_hid_dim, n_layers, dropout=dropout)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, src):\n",
        "        \n",
        "        #src = [src len, batch size]\n",
        "        \n",
        "        embedded = self.dropout(self.embedding(src))\n",
        "        \n",
        "        #embedded = [src len, batch size, emb dim]\n",
        "        \n",
        "        outputs, (hidden, cell) = self.lstm(embedded)\n",
        "       \n",
        "        # outputs are always from the top hidden layer, if bidirectional outputs are concatenated.\n",
        "        # outputs shape [sequence_length, batch_size, hidden_dim * num_directions]\n",
        "        # hidden is of shape [num_layers * num_directions, batch_size, hidden_size]\n",
        "        # cell is of shape [num_layers * num_directions, batch_size, hidden_size]\n",
        "        \n",
        "        return hidden, cell\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_dim, emb_dim, dec_hid_dim, n_layers, dropout):\n",
        "        super().__init__()\n",
        "\n",
        "        self.emb_dim = emb_dim\n",
        "        self.output_dim = output_dim\n",
        "        self.dec_hid_dim = dec_hid_dim\n",
        "        self.n_layers = n_layers\n",
        "        self.dropout = dropout\n",
        "\n",
        "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
        "        self.lstm = nn.LSTM(emb_dim, dec_hid_dim, n_layers, dropout=dropout)\n",
        "        self.fc_out = nn.Linear(dec_hid_dim, output_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, input, hidden, cell):\n",
        "             \n",
        "        # input is of shape [batch_size]\n",
        "        # hidden is of shape [n_layer * num_directions, batch_size, hidden_size]\n",
        "        # cell is of shape [n_layer * num_directions, batch_size, hidden_size]\n",
        "        \n",
        "        input = input.unsqueeze(0)\n",
        "        \n",
        "        # input shape is [1, batch_size]. reshape is needed rnn expects a rank 3 tensors as input.\n",
        "        # so reshaping to [1, batch_size] means a batch of batch_size each containing 1 index.\n",
        "        \n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "        \n",
        "        #embedded = [1, batch size, emb dim]    \n",
        "        output, (hidden, cell) = self.lstm(embedded, (hidden, cell))\n",
        "        \n",
        "        # output shape is [sequence_len, batch_size, hidden_dim * num_directions]\n",
        "        # hidden shape is [num_layers * num_directions, batch_size, hidden_dim]\n",
        "        # cell shape is [num_layers * num_directions, batch_size, hidden_dim]\n",
        "\n",
        "        # sequence_len and num_directions will always be 1 in the decoder.\n",
        "        # output shape is [1, batch_size, hidden_dim]\n",
        "        # hidden shape is [num_layers, batch_size, hidden_dim]\n",
        "        # cell shape is [num_layers, batch_size, hidden_dim]\n",
        "        \n",
        "        prediction = self.fc_out(hidden.squeeze(0)) # linear expects as rank 2 tensor as input\n",
        "        # predicted shape is [batch_size, output_dim]\n",
        "        \n",
        "        return prediction, hidden, cell\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "    ''' This class contains the implementation of complete sequence to sequence network.\n",
        "    It uses to encoder to produce the context vectors.\n",
        "    It uses the decoder to produce the predicted target sentence.\n",
        "    Args:\n",
        "        encoder: A Encoder class instance.\n",
        "        decoder: A Decoder class instance.\n",
        "    '''\n",
        "    def __init__(self, encoder, decoder, device):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "\n",
        "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
        "        # src is of shape [sequence_len, batch_size]\n",
        "        # trg is of shape [sequence_len, batch_size]\n",
        "        # if teacher_forcing_ratio is 0.5 we use ground-truth inputs 50% of time and 50% time we use decoder outputs.\n",
        "\n",
        "        batch_size = trg.shape[1]\n",
        "        max_len = trg.shape[0]\n",
        "        trg_vocab_size = self.decoder.output_dim\n",
        "\n",
        "        # to store the outputs of the decoder\n",
        "        outputs = torch.zeros(max_len, batch_size, trg_vocab_size).to(self.device)\n",
        "\n",
        "        # context vector, last hidden and cell state of encoder to initialize the decoder\n",
        "        hidden, cell = self.encoder(src)\n",
        "\n",
        "        # first input to the decoder is the <sos> tokens\n",
        "        input = trg[0, :]\n",
        "\n",
        "        for t in range(1, max_len):\n",
        "            output, hidden, cell = self.decoder(input, hidden, cell)\n",
        "            outputs[t] = output\n",
        "            use_teacher_force = random.random() < teacher_forcing_ratio\n",
        "            top1 = output.max(1)[1]\n",
        "            input = (trg[t] if use_teacher_force else top1)\n",
        "\n",
        "        # outputs is of shape [sequence_len, batch_size, output_dim]\n",
        "        return outputs\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "coD24BprDszs",
        "colab_type": "text"
      },
      "source": [
        "## Model training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x6k65PC7DuuD",
        "colab_type": "code",
        "outputId": "411ace12-220e-40cf-cd55-de79fd5b7468",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 400
        }
      },
      "source": [
        "'''\n",
        "hyperparameters\n",
        "'''\n",
        "INPUT_DIM = len(SRC.vocab)\n",
        "OUTPUT_DIM = len(TRG.vocab)\n",
        "ENC_EMB_DIM = 128\n",
        "DEC_EMB_DIM = 128\n",
        "ENC_HID_DIM = 256\n",
        "DEC_HID_DIM = 256\n",
        "ENC_DROPOUT = 0.5\n",
        "DEC_DROPOUT = 0.5\n",
        "N_LAYERS = 1\n",
        "LEARNING_RT = 0.001\n",
        "N_EPOCHS = 5\n",
        "CLIP = 1\n",
        "\n",
        "'''\n",
        "instantiate the model\n",
        "'''\n",
        "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, N_LAYERS, ENC_DROPOUT)\n",
        "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, DEC_HID_DIM, N_LAYERS, DEC_DROPOUT)\n",
        "model = Seq2Seq(enc, dec,device).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr = LEARNING_RT)\n",
        "TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]\n",
        "print('<pad> token index: ',TRG_PAD_IDX)\n",
        "## we will ignore the pad token in true target set\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)\n",
        "\n",
        "'''\n",
        "initialize the model weights\n",
        "'''\n",
        "def init_weights(m):\n",
        "    for name, param in m.named_parameters():\n",
        "        if 'weight' in name:\n",
        "            nn.init.normal_(param.data, mean=0, std=0.01)\n",
        "        else:\n",
        "            nn.init.constant_(param.data, 0)\n",
        "model.apply(init_weights)\n",
        "\n",
        "'''\n",
        "calculate the number of parameters\n",
        "'''\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')\n",
        "\n",
        "'''\n",
        "compute the loss function with first training batch\n",
        "'''\n",
        "clip = 1\n",
        "model.train()\n",
        "\n",
        "for i, batch in enumerate(train_iter):\n",
        "\n",
        "    src = batch.SRC\n",
        "    trg = batch.TRG\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    output = model(src, trg)\n",
        "    #trg = [trg len, batch size]\n",
        "    #output = [trg len, batch size, output dim]\n",
        "\n",
        "    output_dim = output.shape[-1]\n",
        "\n",
        "    output = output[1:].view(-1, output_dim)\n",
        "    trg = trg[1:].view(-1)\n",
        "\n",
        "    #trg = [(trg len - 1) * batch size]\n",
        "    #output = [(trg len - 1) * batch size, output dim]\n",
        "\n",
        "    loss = criterion(output, trg)\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), CLIP)\n",
        "\n",
        "    optimizer.step()\n",
        "    \n",
        "    print('loss from first training batch')\n",
        "    print(loss/src.shape[1])\n",
        "    break\n",
        "\n",
        "'''\n",
        "Full training helper functions\n",
        "'''\n",
        "\n",
        "def train(model, iterator, optimizer, criterion, clip):\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    \n",
        "    for i, batch in enumerate(iterator):\n",
        "        \n",
        "        src = batch.SRC\n",
        "        trg = batch.TRG\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        output = model(src, trg)\n",
        "        \n",
        "        #trg = [trg len, batch size]\n",
        "        #output = [trg len, batch size, output dim]\n",
        "        \n",
        "        output_dim = output.shape[-1]\n",
        "        \n",
        "        output = output[1:].view(-1, output_dim)\n",
        "        trg = trg[1:].view(-1)\n",
        "        \n",
        "        # loss function works only 2d logits, 1d targets\n",
        "        # so flatten the trg, output tensors. Ignore the <sos> token\n",
        "        # trg shape shape should be [(sequence_len - 1) * batch_size]\n",
        "        # output shape should be [(sequence_len - 1) * batch_size, output_dim]\n",
        "        \n",
        "        loss = criterion(output, trg)\n",
        "        \n",
        "        loss.backward()\n",
        "        \n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), CLIP)\n",
        "        \n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator)\n",
        "\n",
        "def evaluate(model, iterator, criterion):\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for i, batch in enumerate(iterator):\n",
        "\n",
        "            src = batch.SRC\n",
        "            trg = batch.TRG\n",
        "\n",
        "            output = model(src, trg, 0) #turn off teacher forcing\n",
        "\n",
        "            #trg = [trg len, batch size]\n",
        "            #output = [trg len, batch size, output dim]\n",
        "\n",
        "            output_dim = output.shape[-1]\n",
        "            \n",
        "            output = output[1:].view(-1, output_dim)\n",
        "            trg = trg[1:].view(-1)\n",
        "\n",
        "            #trg = [(trg len - 1) * batch size]\n",
        "            #output = [(trg len - 1) * batch size, output dim]\n",
        "\n",
        "            loss = criterion(output, trg)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator)\n",
        "\n",
        "# convert index to text string\n",
        "def convert_itos(convert_vocab, token_ids):\n",
        "    list_string = []\n",
        "    for i in token_ids:\n",
        "        if i == convert_vocab.vocab.stoi['<eos>']:\n",
        "            break\n",
        "        else:\n",
        "            token = convert_vocab.vocab.itos[i]\n",
        "            list_string.append(token)\n",
        "    return list_string\n",
        "\n",
        "def evaluate_bleu(model, eval_iter, src_vocab, trg_vocab, attention = True, max_trg_len = 64):\n",
        "    '''\n",
        "    Function for evaluating translation using BLEU\n",
        "\n",
        "    Input: \n",
        "    model: translation model;\n",
        "    eval_iter: iterator over the evaluation data\n",
        "    src_vocab: Source torchtext Field\n",
        "    trg_vocab: Target torchtext Field\n",
        "    attention: the model returns attention weights or not.\n",
        "    max_trg_len: the maximal length of translation text (optinal), default = 64\n",
        "\n",
        "    Output:\n",
        "    Corpus BLEU score.\n",
        "    '''\n",
        "\n",
        "    model.eval()\n",
        "    all_trg = []\n",
        "    all_translated_trg = []\n",
        "\n",
        "    TRG_PAD_IDX = trg_vocab.vocab.stoi[trg_vocab.pad_token]\n",
        "\n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for i, batch in enumerate(eval_iter):\n",
        "\n",
        "            src = batch.SRC\n",
        "            #src = [src len, batch size]\n",
        "\n",
        "            trg = batch.TRG\n",
        "            #trg = [trg len, batch size]\n",
        "\n",
        "            batch_size = trg.shape[1]\n",
        "\n",
        "            # create a placeholder for traget language with shape of [max_trg_len, batch_size] where all the elements are the index of <pad>. Then send to device\n",
        "            trg_placeholder = torch.Tensor(max_trg_len, batch_size)\n",
        "            trg_placeholder.fill_(TRG_PAD_IDX)\n",
        "            trg_placeholder = trg_placeholder.long().to(device)\n",
        "            if attention == True:\n",
        "              output,_ = model(src, trg_placeholder, 0) #turn off teacher forcing\n",
        "            else:\n",
        "              output = model(src, trg_placeholder, 0) #turn off teacher forcing\n",
        "            # get translation results, we ignor first token <sos> in both translation and target sentences. \n",
        "            # output_translate = [(trg len - 1), batch, output dim] output dim is size of target vocabulary.\n",
        "            output_translate = output[1:]\n",
        "            # store gold target sentences to a list \n",
        "            all_trg.append(trg[1:].cpu())\n",
        "\n",
        "            # Choose top 1 word from decoder's output, we get the probability and index of the word\n",
        "            prob, token_id = output_translate.data.topk(1)\n",
        "            translation_token_id = token_id.squeeze(2).cpu()\n",
        "\n",
        "            # store predicted target sentences to a list \n",
        "            all_translated_trg.append(translation_token_id)\n",
        "      \n",
        "    all_gold_text = []\n",
        "    all_translated_text = []\n",
        "    for i in range(len(all_trg)): \n",
        "        cur_gold = all_trg[i]\n",
        "        cur_translation = all_translated_trg[i]\n",
        "        for j in range(cur_gold.shape[1]):\n",
        "            gold_convered_strings = convert_itos(trg_vocab,cur_gold[:,j])\n",
        "            trans_convered_strings = convert_itos(trg_vocab,cur_translation[:,j])\n",
        "\n",
        "            all_gold_text.append(gold_convered_strings)\n",
        "            all_translated_text.append(trans_convered_strings)\n",
        "\n",
        "    corpus_all_gold_text = [[item] for item in all_gold_text]\n",
        "    corpus_bleu_score = corpus_bleu(corpus_all_gold_text, all_translated_text)  \n",
        "    return corpus_bleu_score\n",
        "\n",
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs\n",
        "\n",
        "'''\n",
        "kickstart full training\n",
        "'''\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "for epoch in range(N_EPOCHS):\n",
        "    \n",
        "    start_time = time.time()\n",
        "    \n",
        "    train_loss = train(model, train_iter, optimizer, criterion, CLIP)\n",
        "    valid_loss = evaluate(model, valid_iter, criterion)\n",
        "    valid_bleu = evaluate_bleu(model, valid_iter, SRC, TRG, attention = False, max_trg_len = 64)\n",
        "\n",
        "    end_time = time.time()\n",
        "    \n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    \n",
        "    # Create checkpoint at end of each epoch\n",
        "    state_dict_model = model.state_dict() \n",
        "    state = {\n",
        "        'epoch': epoch,\n",
        "        'state_dict': state_dict_model,\n",
        "        'optimizer': optimizer.state_dict()\n",
        "        }\n",
        "\n",
        "    torch.save(state, \"./drive/My Drive/Colab Notebooks/ckpt_mt_lab4/seq2seq_\"+str(epoch+1)+\".pt\")\n",
        "\n",
        "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f} | Val. BLEU: {valid_bleu:7.3f}')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py:50: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "<pad> token index:  1\n",
            "The model has 2,693,776 trainable parameters\n",
            "loss from first training batch\n",
            "tensor(0.4843, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Epoch: 01 | Time: 1m 49s\n",
            "\tTrain Loss: 3.590 | Train PPL:  36.217\n",
            "\t Val. Loss: 4.542 |  Val. PPL:  93.832 | Val. BLEU:   0.044\n",
            "Epoch: 02 | Time: 1m 52s\n",
            "\tTrain Loss: 1.743 | Train PPL:   5.717\n",
            "\t Val. Loss: 4.976 |  Val. PPL: 144.883 | Val. BLEU:   0.090\n",
            "Epoch: 03 | Time: 1m 52s\n",
            "\tTrain Loss: 0.740 | Train PPL:   2.095\n",
            "\t Val. Loss: 5.575 |  Val. PPL: 263.731 | Val. BLEU:   0.092\n",
            "Epoch: 04 | Time: 1m 51s\n",
            "\tTrain Loss: 0.332 | Train PPL:   1.393\n",
            "\t Val. Loss: 6.112 |  Val. PPL: 451.131 | Val. BLEU:   0.100\n",
            "Epoch: 05 | Time: 1m 52s\n",
            "\tTrain Loss: 0.184 | Train PPL:   1.201\n",
            "\t Val. Loss: 6.543 |  Val. PPL: 694.373 | Val. BLEU:   0.102\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zEHRD0kVQb4b",
        "colab_type": "text"
      },
      "source": [
        "## Preparing Predictions from Test Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_h6fUiSCDuEz",
        "colab_type": "code",
        "outputId": "945b9c98-27ca-46fb-fb4f-b796e03ecf18",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "'''\n",
        "load fields saved during preprocessing\n",
        "'''\n",
        "with open(\"./drive/My Drive/Colab Notebooks/ckpt_mt_lab4/TRG.Field\",\"rb\") as f:\n",
        "     TRG_saved = pickle.load(f)\n",
        "\n",
        "with open(\"./drive/My Drive/Colab Notebooks/ckpt_mt_lab4/SRC.Field\",\"rb\") as f:\n",
        "     SRC_saved = pickle.load(f)\n",
        "\n",
        "'''\n",
        "hyperparameters (ensure the following hyperparameters match with those used during training of the best model)\n",
        "'''\n",
        "INPUT_DIM = len(SRC_saved.vocab)\n",
        "OUTPUT_DIM = len(TRG_saved.vocab)\n",
        "ENC_EMB_DIM = 128\n",
        "DEC_EMB_DIM = 128\n",
        "ENC_HID_DIM = 256\n",
        "DEC_HID_DIM = 256\n",
        "ENC_DROPOUT = 0.5\n",
        "DEC_DROPOUT = 0.5\n",
        "N_LAYERS = 1\n",
        "\n",
        "'''\n",
        "instantiate the model\n",
        "'''\n",
        "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, N_LAYERS, ENC_DROPOUT)\n",
        "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, DEC_HID_DIM, N_LAYERS, DEC_DROPOUT)\n",
        "model_best = Seq2Seq(enc, dec,device).to(device)\n",
        "\n",
        "'''\n",
        "load the checkpoint corresponding to the best epoch (usually epoch with highest validation BLEU score)\n",
        "'''\n",
        "model_best.load_state_dict(torch.load('./drive/My Drive/Colab Notebooks/ckpt_mt_lab4/seq2seq_1.pt')['state_dict'])\n",
        "model_best = model_best.to(device)\n",
        "\n",
        "'''\n",
        "generate translations for all the sentences in test data\n",
        "'''\n",
        "def generate_translations(model, eval_iter, trg_vocab, attention = True, max_trg_len = 64):\n",
        "  '''\n",
        "    Function for generating translation by model inference\n",
        "\n",
        "    Input: \n",
        "    model: translation model;\n",
        "    eval_iter: iterator over the evaluation data\n",
        "    trg_vocab: Target torchtext Field\n",
        "    attention: the model returns attention weights or not.\n",
        "    max_trg_len: the maximal length of translation text (optional), default = 64\n",
        "\n",
        "    Output:\n",
        "    List of translated sentences\n",
        "  '''\n",
        "  model.eval()\n",
        "  all_translation_word_ids = []\n",
        "  for batch in test_iter:\n",
        "    src = batch.SRC\n",
        "    #src = [src len, batch size]\n",
        "    batch_size = src.shape[1]\n",
        "\n",
        "    # create a placeholder for target language with shape of [max_trg_len, batch_size] where all the elements are the index of <pad>. Then send to device\n",
        "    trg_placeholder = torch.Tensor(max_trg_len, batch_size)\n",
        "    trg_placeholder.fill_(TRG_PAD_IDX)\n",
        "    trg_placeholder = trg_placeholder.long().to(device)\n",
        "    if attention == True:\n",
        "      output,_ = model(src, trg_placeholder, 0) #turn off teacher forcing\n",
        "    else:\n",
        "      output = model(src, trg_placeholder, 0) #turn off teacher forcing\n",
        "    # get translation results, we ignore first token <sos> in both translation and target sentences. \n",
        "    # output_translate = [(trg len - 1), batch, output dim] output dim is size of target vocabulary.\n",
        "    output_translate = output[1:]\n",
        "\n",
        "    # Choose top 1 word from decoder's output, we get the probability and index of the word\n",
        "    prob, token_id = output_translate.data.topk(1)\n",
        "    translation_token_id = token_id.squeeze(2).cpu()\n",
        "\n",
        "    # store gold target sentences to a list \n",
        "    all_translation_word_ids.append(translation_token_id)\n",
        "  \n",
        "  all_translation_text = []\n",
        "  for i in range(len(all_translation_word_ids)):\n",
        "    cur_translation_batch = all_translation_word_ids[i]\n",
        "    for j in range(cur_translation_batch.shape[1]):\n",
        "      trans_convered_strings = convert_itos(trg_vocab, cur_translation_batch[:,j])\n",
        "      all_translation_text.append(' '.join(trans_convered_strings)) # convert list of words to text\n",
        "  \n",
        "  return all_translation_text\n",
        "\n",
        "# translate all the sentences in the test set      \n",
        "test_predictions = generate_translations(model, test_iter, TRG_saved, attention = False, max_trg_len = 64)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py:50: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aRRKzAtiu7jR",
        "colab_type": "text"
      },
      "source": [
        "## Create submission file using test predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rXQGGeFBu77v",
        "colab_type": "code",
        "outputId": "5c748ef5-86bf-4a5e-e1ba-ec39d13fbff0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "'''\n",
        "write the translations to a file. this function will help you generate the submission file for the first question.\n",
        "'''\n",
        "def out_prediction(first_name, last_name, save_directory, prediction_list):\n",
        "    \"\"\"\n",
        "    out_prediction takes four input varibles: first_name, last_name, save_directory, prediction_list\n",
        "    <first_name>, string, your first name, e.g., Tom\n",
        "    <last_name>, string, your last name, e.g., Smith\n",
        "    <save_directory>, string, directory to save the submission file, e.g., ./drive/My Drive/Colab Notebooks/ckpt_mt_lab4\n",
        "    <prediction_list>, list of string which includes all your predictions (or translations) of TEST samples\n",
        "          e.g., ['This is the translation of my first sentence','This is the translation of my second sentence',...]\n",
        "                        \n",
        "    Generate a file is named with <yourfirstname>_<yourlastname>_MACHINE_TRANSLATION_PRED.txt in current directory\n",
        "    \"\"\"\n",
        "    absolute_file_path = \"{}/{}_{}_MACHINE_TRANSLATION_PRED.txt\".format(save_directory, first_name,last_name)\n",
        "    output_file = open(absolute_file_path,'w')\n",
        "    output_file.write(\"English (trg)\\n\")\n",
        "    for item in prediction_list:\n",
        "        output_file.write(item+\"\\n\")\n",
        "    output_file.close()\n",
        "    print(\"submission file for the first question successfully saved at %s\"%absolute_file_path)\n",
        "\n",
        "# provide your firstname and lastname as arguments to out_prediction\n",
        "out_prediction('firstname', 'lastname', './drive/My Drive/Colab Notebooks/ckpt_mt_lab4', test_predictions)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "submission file for the first question successfully saved at ./drive/My Drive/Colab Notebooks/ckpt_mt_lab4/firstname_lastname_MACHINE_TRANSLATION_PRED.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ktoW-82qwQeK",
        "colab_type": "text"
      },
      "source": [
        "## Compute BLEU score using scorer.py\n",
        "\n",
        "We will compute BLEU score based on dummy test predictions (dummy-example-mt-test-gold.tsv) and dummy gold test data (dummy-example-mt-test-gold.tsv)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JL3W5crFwQkI",
        "colab_type": "code",
        "outputId": "10e76ce9-0f38-4bdc-bdee-4a79d2d26ce7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "\"\"\"\n",
        "Python code to evaluate the system outputs using BLEU\n",
        "\n",
        "usage format:\n",
        "> python scorer.py <task> <gold-file> <pred_file>\n",
        "\n",
        "example usage for Machine Translation (Problem 1):\n",
        "> python scorer.py mt prob1/dummy-example-mt-test-gold.tsv prob1/dummy-example-mt-test-pred.tsv\n",
        "\"\"\"\n",
        "\n",
        "!python ./drive/\"My Drive\"/scorer.py mt ./drive/\"My Drive\"/machine_translation/dummy-example-mt-test-gold.tsv ./drive/\"My Drive\"/machine_translation/dummy-example-mt-test-pred.tsv\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "OVERALL SCORES:\n",
            "Cumulative 1-gram: 0.847155\n",
            "Cumulative 2-gram: 0.801876\n",
            "Cumulative 3-gram: 0.750307\n",
            "Cumulative 4-gram: 0.695157\n",
            "BLEU (default, that is, Cumulative 4-gram): 0.695157\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}