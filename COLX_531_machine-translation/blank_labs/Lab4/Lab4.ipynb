{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework 4 - Machine Translation - MDS Computational Linguistics\n",
    "\n",
    "### Assignment Goals\n",
    "- Practice training a deep learning system for machine translation task\n",
    "- Practice training a deep learning system for paraphrase generation task\n",
    "- Analyze the machine translation model errors\n",
    "- Gain more familiarity with week 4 topics\n",
    "- Gain more familiarity with the code, and be able to train an MT system and run related code\n",
    "\n",
    "### Software Requirements\n",
    "- Python (>=3.6)\n",
    "- PyTorch (>=1.2.0) \n",
    "- Jupyter (latest)\n",
    "- spaCy (latest)\n",
    "- nltk (latest)\n",
    "- colab access (https://colab.research.google.com)\n",
    "\n",
    "### Submission Info.\n",
    "- Due Date: March 26, 2020, 18:00:00 (Vancouver time)\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tidy Submission\n",
    "\n",
    "rubric={mechanics:1}\n",
    "\n",
    "To get the marks for tidy submission:\n",
    "- Submit the assignment by filling in this jupyter notebook with your answers embedded\n",
    "- Be sure to follow the [general lab instructions](https://ubc-mds.github.io/resources_pages/general_lab_instructions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1 Machine Translation task\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Portuguese to English translation\n",
    "rubric={accuracy:40}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This exercise is a competition of real-world machine translation task. The goal of this task is to build a machine translation model that can effectively translate sentences in Portuguese language to English language. This task follows the same spirit as the Yelp review rating prediction task (a Lab4 question from Supervised learning II). \n",
    "\n",
    "We will provide 85918 Portuguese-English sentence pairs for training your model (`train set`). We also provide 4772 Portuguese-English sentence pairs for selecting the best hyperparameters of your model (`validation set`). We provide 4738 Portuguese sentences for which we are seeking the corresponding English sentences (or translations) from you (`blind test set`).\n",
    "\n",
    "In directory `./data/machine_translation/`, you will find the following files:\n",
    "* `mt-train-pt2en.tsv` - Train set is a tsv file containing 85919 lines. First line is a header line containing *Portuguese (src)* and *English (trg)*. Each of the following 85918 lines in the file contains the Portuguese sentence followed by the corresponding English sentence.\n",
    "* `mt-valid-pt2en.tsv` - Validation set is a tsv file containing 4773 lines. First line is a header line containing *Portuguese (src)* and *English (trg)*. Each of the following 4772 lines in the file contains the Portuguese sentence followed by the corresponding English sentence.\n",
    "* `mt-test-pt2en_onlySRCpt.tsv`- *Blind* test set is a tsv file containing 4739 lines. First line is a header line containing *Portuguese (src)*. Each of the following 4738 lines in the file contains a Portuguese sentence for which you need to predict the corresponding English sentence and submit your predictions.\n",
    "* `dummy-example-mt-test-pred.tsv` - This tsv file is a dummy example file whose purpose is to provide you the format of your submission file containing test predictions. This file contains 4739 lines. First line is a header line containing *English (trg)*. Each of the following 4738 lines in the file contains a English sentence. \n",
    "* `dummy-example-mt-test-gold.tsv` - This tsv file is a dummy example file whose purpose is to provide you the format of the undisclosed **gold** English translation. This file contains 4739 lines. First line is a header line containing *English (trg)*. Each of the following 4738 lines in the file contains a English sentence. \n",
    "\n",
    "**The performance of your submitted systems will be evaluated on English translations for each Portuguese sentence in TEST set (`mt-test-pt2en_onlySRCpt.tsv`). We will use the default BLEU score (BLEU-4 computed using nltk's `corpus_bleu` function) as our the evaluation metric.** \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Development Instruction**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We provide the implementation of a machine translation system using standard Seq2seq model (discussed in Week 2 of our course) at `baseline_machine_translation_vanilla_seq2seq.ipynb` (a colab notebook file). This baseline code can be considered as a starting point for this question of Lab4 where you need to build an effective model than can translate sentences from Portuguese language to English language. Alternatively, you can ignore this baseline code and build your machine translation model by writing modules from scratch.\n",
    "\n",
    "Regardless of where you start your development, you need to follow these instructions: \n",
    "\n",
    "**1. Data and preprocessing: Use `torchtext` to load and pre-process dataset. Prepare the batch iterators. You are allowed to use only spaCy for tokenizing Portuguese (strictly, pt_core_news_sm model) and English (strictly, en_core_web_sm model) sentences. **\n",
    "\n",
    "Note: BLEU metric is sensitive to the tokenization used in preprocessing the text and hence we ask all the students to use the same tokenizer (i.e, spaCy). For more details, check [this paper](https://arxiv.org/pdf/1804.08771.pdf).\n",
    "\n",
    "**2. Model selection and hyper-parameter tuning. You need to select the architecture you want to use. You may need to search the optimal hyper-parameter set to improve your model performance.**\n",
    "\n",
    "Hints: You can use the validation set (`mt-valid-pt2en.tsv`) to estimate your model performance. \n",
    "\n",
    "There are many possible strategies you could take to improve performance:   \n",
    "\n",
    "a. Changing vocabulary size, batch size. Using pre-training word embedding model as your embedding weights (e.g., [google news word2vec](https://code.google.com/archive/p/word2vec/), [GloVe](https://nlp.stanford.edu/projects/glove/), [fastText](https://fasttext.cc/)).\n",
    "\n",
    "Hint: In our tutorial, we use the embedding layer with randomly initialized weights. If you initialize your embedding layer with word embedding weights from a pre-trained word embedding model from the ones listed above, you may get improvements.\n",
    "\n",
    "b. Changing the encoder/decoder model architecture, such as RNN, GRUs, LSTM, bi-RNN, bi-GRU, bi-LSTM, Transformer.\n",
    "\n",
    "c. Changing the encoder/decoder neural network structure, such as changing hidden dimension size, number of layers, dropout rate, activation function.\n",
    "\n",
    "d. Changing the training procedure, such as number of epochs, learning rate, clipping parameter, adding regularization and momentum (or RMSProp, or Adam).\n",
    "\n",
    "e. Adding attention mechanism, such as Dot-Product/Multiplicative, Concatenative/Additive or [others](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html).\n",
    "\n",
    "f. You may find some novel ideas in the state-of-the-art MT systems [here](http://www.statmt.org/wmt19/papers.html).\n",
    "\n",
    "Hint: Due to the high requirement of computational resource, we suggest you to run your experiments on [Google Colab](https://colab.research.google.com/notebooks/intro.ipynb#recent=true). Please read `Colab instructions` for more information of Colab\n",
    "\n",
    "**3. When you get your best model on validation set, you will use this model to predict the translations of Portuguese sentences in the test set (`mt-test-pt2en_onlySRCpt.tsv`) and submit your prediction.**\n",
    "\n",
    "**4. For prediction submission, please read `submission instruction`.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Colab Instruction**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Google Colab](https://colab.research.google.com/notebooks/intro.ipynb#recent=true) will allow you to train your model on a GPU. \n",
    "\n",
    "You can follow the steps to use Colab:\n",
    "\n",
    "1. You can create a new notebook (`lab4_MT_colab.ipynb`) for your experiments on Colab. You should develop your system on `lab4_MT_colab.ipynb` instead of current jupyter notebook. If you are using the baseline colab code (`baseline_machine_translation_vanilla_seq2seq.ipynb`), you can rename the notebook to  `lab4_MT_colab.ipynb` before using in colab.\n",
    "2. Go to [Google colab](https://colab.research.google.com).\n",
    "3. Create an account or login your account.\n",
    "4. Set the hardware: \n",
    "**Go to the navigation bar, click Runtime --> Change runtime type --> Hardware accelerator --> Select GPU.**\n",
    "\n",
    "5. You don't need to install any packages. Google prepared everything for you.\n",
    "6. You can find all your generations in `Files`. You can download your notebook and files.\n",
    "\n",
    "Suggestion: \n",
    "1. You can download the notebook from Colab and submit as **`lab4_MT_colab.ipynb`**. \n",
    "2. If you train your model on GPU, please make sure your model, input and loss is sent to GPU using XXX.to(device) where device is `cuda`. \n",
    "3. If you want to send the GPU varibles to CPU, please use XXX.cpu() to detach from GPU. You can find more related information [here](https://pytorch.org/docs/stable/notes/cuda.html). \n",
    "4. Google colab keeps disconnecting after 30 mins automatically if you do not respond. You can find some solutions [here](https://stackoverflow.com/questions/57113226/how-to-prevent-google-colab-from-disconnecting). \n",
    "\n",
    "``Warning``: Running on CPU will be slow. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Submission Instruction**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. In directory `./data/machine_translation/`,  `dummy-example-mt-test-gold.tsv` and `dummy-example-mt-test-pred.tsv` are examples of gold and prediction files which can be used with the ``scorer.py`` provided (description below). Your submission should have excactly the same structure as **`dummy-example-mt-test-pred.tsv`** (i.e., First line is a header line containing *English (trg)*. Each of the following 4738 lines in the file contains a English sentence..) This is important.\n",
    "\n",
    "2. `./data/scorer.py`\n",
    "\n",
    "The scoring script (scorer.py) is provided at the root directory of the released data. `scorer.py` is a python script that will take in two text files containing true English sentences and predicted English sentences along with the task identifier and will output BLEU-1, BLEU-2, BLEU-3 and BLEU-4. (Note that the evaluation metric is BLEU-4 score).  The scoring script is used for evaluating your TEST prediction.\n",
    "\n",
    "```\n",
    "usage format:\n",
    "> python scorer.py <task> <gold-file> <pred_file>\n",
    "\n",
    "In the dataset directory, there are example\n",
    "gold and prediction files. If they are used with the scorer,\n",
    "they should produce the following results:\n",
    "\n",
    "python scorer.py mt machine_translation/dummy-example-mt-test-gold.tsv machine_translation/dummy-example-mt-test-pred.tsv\n",
    "\n",
    "OVERALL SCORES:\n",
    "Cumulative 1-gram: 0.847155\n",
    "Cumulative 2-gram: 0.801876\n",
    "Cumulative 3-gram: 0.750307\n",
    "Cumulative 4-gram: 0.695157\n",
    "BLEU (default, that is, Cumulative 4-gram): 0.695157\n",
    "```\n",
    "\n",
    "**Requirements:**\n",
    "1. Your submission must have **same** structure as `dummy-example-mt-test-pred.tsv`. It contains spaCy **tokenized** English sentences.\n",
    "\n",
    "2. Put your prediction txt file in this lab directory. The prediction txt file should be named with `<yourfirstname>_<yourlastname>_MACHINE_TRANSLATION_PRED.txt`. We will use ``scorer.py`` to evaluate your submission.\n",
    "\n",
    "Hint: We provide a function `out_prediction` to help you generate the submission file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "write the translations to a file. this function will help you generate the submission file for the first question.\n",
    "'''\n",
    "def out_prediction(first_name, last_name, save_directory, prediction_list):\n",
    "    \"\"\"\n",
    "    out_prediction takes four input varibles: first_name, last_name, save_directory, prediction_list\n",
    "    <first_name>, string, your first name, e.g., Tom\n",
    "    <last_name>, string, your last name, e.g., Smith\n",
    "    <save_directory>, string, directory to save the submission file, e.g., ./drive/My Drive/Colab Notebooks/ckpt_mt_lab4\n",
    "    <prediction_list>, list of string which includes all your predictions (or translations) of TEST samples\n",
    "          e.g., ['This is the translation of my first sentence','This is the translation of my second sentence',...]\n",
    "                        \n",
    "    Generate a file is named with <yourfirstname>_<yourlastname>_MACHINE_TRANSLATION_PRED.txt in current directory\n",
    "    \"\"\"\n",
    "    absolute_file_path = \"{}/{}_{}_MACHINE_TRANSLATION_PRED.txt\".format(save_directory, first_name,last_name)\n",
    "    output_file = open(absolute_file_path,'w')\n",
    "    output_file.write(\"English (trg)\\n\")\n",
    "    for item in prediction_list:\n",
    "        output_file.write(item+\"\\n\")\n",
    "    output_file.close()\n",
    "    print(\"submission file for the first question successfully saved at %s\"%absolute_file_path)\n",
    "\n",
    "# provide your firstname and lastname as arguments to out_prediction\n",
    "out_prediction('firstname', 'lastname', './drive/My Drive/Colab Notebooks/ckpt_mt_lab4', test_predictions)\n",
    "\n",
    "'''\n",
    "sample output:\n",
    "submission file for the first question successfully saved at ./drive/My Drive/Colab Notebooks/ckpt_mt_lab4/firstname_lastname_MACHINE_TRANSLATION_PRED.txt\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Please clearly describe the system you submitted in 1.1 (i.e., your best model) within 300 words.\n",
    "rubric={mechanics:5,resoning:5}\n",
    "\n",
    "Hints: \n",
    "1. Describe all the hyper-parameters of your submitted system. You may follow the structure of Development Instruction.\n",
    "2. The range of each of the hyper-parameters that you tested on (ex. Learning Rate: [0.001-0.1])\n",
    "3. List the strategies you attempted. What strategies did work? What did not work? (this should be about a paragraph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Write your answer here.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Please organize your code in `lab4_mt_colab.ipynb` and only keep the code that you used to train your submitted system in 1.1. Submit `lab4_mt_colab.ipynb`.\n",
    "rubric={mechanics:5}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Monolingual Paraphrase Generation (Bonus Question)\n",
    "rubric={accuracy:10}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paraphrases convey the same meaning as the original sentence, but with different expressions in the same language. Paraphrase generation aims to synthesize paraphrases of a given sentence automatically. This task is important for many downstream applications. For example, chatbot engines can leverage paraphrases to diversify the responses. Question answering systems can use paraphrases to understand the question better. For more details, look at [this sample paper](http://people.ee.duke.edu/~lcarin/emnlp_gap.pdf).\n",
    "\n",
    "Some example paraphrases in English language:\n",
    "\n",
    "(a)\n",
    "source (original): How do I improve my English speaking ?\n",
    "target (paraphrased): How do I speak English fluently ?\n",
    "\n",
    "(b)\n",
    "source (original): What is the easiest way to lose weight faster?\n",
    "target (paraphrased): What is the best way to loose weight quickly ?\n",
    "\n",
    "This exercise is a competition of real-world paraphrase generation task. The goal of this task is to build a paraphrase generation model that can effectively take a sentence in Portuguese language and generate its paraphrase in Portuguese language. This task follows the same spirit as the previous exercise.\n",
    "\n",
    "We will provide 85171 Portuguese-Portuguese sentence pairs for training your model (`train set`). We also provide 4724 Portuguese-Portuguese sentence pairs for selecting the best hyperparameters of your model (`validation set`). We provide 4692 Portuguese sentences for which we are seeking the corresponding paraphrases (predictions) in Portuguese language from you (`blind test set`).\n",
    "\n",
    "In directory `./data/paraphrase_generation/`, you will find the following files:\n",
    "* `pp-train.tsv` - Train set is a tsv file containing 85172 lines. First line is a header line containing *source* and *target*. Each of the following 85171 lines in the file contains the Portuguese sentence followed by its paraphrase.\n",
    "* `pp-valid.tsv` - Validation set is a tsv file containing 4725 lines. First line is a header line containing *source* and *target*. Each of the following 4724 lines in the file contains the Portuguese sentence followed by its paraphrase.\n",
    "* `pp-test-onlySRC.tsv`- *Blind* test set is a tsv file containing 4693 lines. First line is a header line containing *Portuguese (src)*. Each of the following 4692 lines in the file contains a Portuguese sentence for which you need to predict the corresponding paraphrase and submit your predictions.\n",
    "* `dummy-example-pp-test-pred.tsv` - This tsv file is a dummy example file whose purpose is to provide you the format of your submission file containing test predictions. This file contains 4693 lines. First line is a header line containing *Portuguese (trg)*. Each of the following 4692 lines in the file contains a Portuguese sentence. \n",
    "* `dummy-example-pp-test-gold.tsv` - This tsv file is a dummy example file whose purpose is to provide you the format of the undisclosed **gold** paraphrases. This file contains 4693 lines. First line is a header line containing *Portuguese (trg-ref0)*, ..., *Portuguese (trg-ref23)*. Each of the following 4692 lines in the file contains a Portuguese sentence. \n",
    "\n",
    "**The performance of your submitted systems will be evaluated on paraphrases for each Portuguese sentence in TEST set (`pp-test-onlySRC.tsv`). We will use the default BLEU score (BLEU-4 computed using nltk's `corpus_bleu` function) as our the evaluation metric.** \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Development Instruction**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We provide the implementation of a paraphrase generation system using standard Seq2seq model (discussed in Week 2 of our course) at `baseline_paraphrase_generation_vanilla_seq2seq.ipynb` (a colab notebook file). This baseline code can be considered as a starting point for this question of Lab4 where you need to build an effective model than can generate paraphrase for a sentence in Portuguese language. Alternatively, you can ignore this baseline code and build your paraphrase generation model by writing modules from scratch.\n",
    "\n",
    "Regardless of where you start your development, you need to follow these instructions: \n",
    "\n",
    "**1. Data and preprocessing: Use `torchtext` to load and pre-process dataset. Prepare the batch iterators. You are allowed to use only spaCy for tokenizing Portuguese (strictly, pt_core_news_sm model) sentences. **\n",
    "\n",
    "Note: BLEU metric is sensitive to the tokenization used in preprocessing the text and hence we ask all the students to use the same tokenizer (i.e, spaCy). For more details, check [this paper](https://arxiv.org/pdf/1804.08771.pdf).\n",
    "\n",
    "**2. Model selection and hyper-parameter tuning. You need to select the architecture you want to use. You may need to search the optimal hyper-parameter set to improve your model performance.**\n",
    "\n",
    "Hints: You can use the validation set (`pp-valid.tsv`) to estimate your model performance. \n",
    "\n",
    "There are many possible strategies you could take to improve performance:   \n",
    "\n",
    "a. Changing vocabulary size, batch size. Using pre-training word embedding model as your embedding weights (e.g., [google news word2vec](https://code.google.com/archive/p/word2vec/), [GloVe](https://nlp.stanford.edu/projects/glove/), [fastText](https://fasttext.cc/)).\n",
    "\n",
    "Hint: In our tutorial, we use the embedding layer with randomly initialized weights. If you initialize your embedding layer with word embedding weights from a pre-trained word embedding model from the ones listed above, you may get improvements.\n",
    "\n",
    "b. Changing the encoder/decoder model architecture, such as RNN, GRUs, LSTM, bi-RNN, bi-GRU, bi-LSTM, Transformer.\n",
    "\n",
    "c. Changing the encoder/decoder neural network structure, such as changing hidden dimension size, number of layers, dropout rate, activation function.\n",
    "\n",
    "d. Changing the training procedure, such as number of epochs, learning rate, clipping parameter, adding regularization and momentum (or RMSProp, or Adam).\n",
    "\n",
    "e. Adding attention mechanism, such as Dot-Product/Multiplicative, Concatenative/Additive or [others](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html).\n",
    "\n",
    "Hint: Due to the high requirement of computational resource, we suggest you to run your experiments on [Google Colab](https://colab.research.google.com/notebooks/intro.ipynb#recent=true). Please read `Colab instructions` for more information of Colab\n",
    "\n",
    "**3. When you get your best model on validation set, you will use this model to generate the paraphrases of Portuguese sentences in the test set (`pp-test-onlySRC.tsv`) and submit your prediction.**\n",
    "\n",
    "**4. For prediction submission, please read `submission instruction`.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suggestion: \n",
    "1. You can download the notebook from Colab and and submit as **`lab4_PP_colab.ipynb`**. \n",
    "2. If you train your model on GPU, please make sure your model, input and loss is sent to GPU using XXX.to(device) where device is `cuda`. \n",
    "3. If you want to send the GPU varibles to CPU, please use XXX.cpu() to detach from GPU. You can find more related information [here](https://pytorch.org/docs/stable/notes/cuda.html). \n",
    "4. Google colab keeps disconnecting after 30 mins automatically if you do not respond. You can find some solutions [here](https://stackoverflow.com/questions/57113226/how-to-prevent-google-colab-from-disconnecting). \n",
    "\n",
    "``Warning``: Running on CPU will be slow. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Submission Instruction**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. In directory `./data/paraphrase_generation/`,  `dummy-example-pp-test-gold.tsv` and `dummy-example-pp-test-pred.tsv` are examples of gold and prediction files which can be used with the ``scorer.py`` provided (description below). Your sumbission should have excactly the same structure as **`dummy-example-pp-test-pred.tsv`** (i.e., First line is a header line containing *Portuguese (trg)*. Each of the following 4692 lines in the file contains a English sentence.) This is important.\n",
    "\n",
    "2. `./data/scorer.py`\n",
    "\n",
    "The scoring script (scorer.py) is provided at the root directory of the released data. `scorer.py` is a python script that will take in two text files containing true Portuguese paraphrase and predicted Portuguese paraphrase along with the task identifier and will output BLEU-1, BLEU-2, BLEU-3 and BLEU-4. (Note that the evaluation metric is BLEU-4 score).  The scoring script is used for evaluating your TEST prediction.\n",
    "\n",
    "```\n",
    "usage format:\n",
    "> python scorer.py <task> <gold-file> <pred_file>\n",
    "\n",
    "In the dataset directory, there are example\n",
    "gold and prediction files. If they are used with the scorer,\n",
    "they should produce the following results:\n",
    "\n",
    "python scorer.py pp paraphrase_generation/dummy-example-pp-test-gold.tsv paraphrase_generation/dummy-example-pp-test-pred.tsv\n",
    "\n",
    "OVERALL SCORES:\n",
    "Cumulative 1-gram: 0.960037\n",
    "Cumulative 2-gram: 0.909792\n",
    "Cumulative 3-gram: 0.859479\n",
    "Cumulative 4-gram: 0.806065\n",
    "BLEU (default, that is, Cumulative 4-gram): 0.806065\n",
    "```\n",
    "\n",
    "**Requirements:**\n",
    "1. Your submission must have **same** structure as `dummy-example-pp-test-pred.tsv`. It contains spaCy **tokenized** Portuguese sentences.\n",
    "\n",
    "2. Put your prediction txt file in this lab directory. The prediction txt file should be named with `<yourfirstname>_<yourlastname>_PARAPHRASE_GENERATION_PRED.txt`. We will use ``scorer.py`` to evaluate your submission.\n",
    "\n",
    "Hint: We provide a function `out_prediction` to help you generate the submission file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "write the paraphrases to a file. this function will help you generate the submission file for the second question.\n",
    "'''\n",
    "def out_prediction(first_name, last_name, save_directory, prediction_list):\n",
    "    \"\"\"\n",
    "    out_prediction takes four input varibles: first_name, last_name, save_directory, prediction_list\n",
    "    <first_name>, string, your first name, e.g., Tom\n",
    "    <last_name>, string, your last name, e.g., Smith\n",
    "    <save_directory>, string, directory to save the submission file, e.g., ./drive/My Drive/Colab Notebooks/ckpt_pp_lab4\n",
    "    <prediction_list>, list of string which includes all your predictions (or paraphrases) of TEST samples\n",
    "          e.g., ['Esta é a paráfrase da minha primeira frase','Esta é a paráfrase da minha segunda frase',...]\n",
    "                        \n",
    "    Generate a file is named with <yourfirstname>_<yourlastname>_PARAPHRASE_GENERATION_PRED.txt in current directory\n",
    "    \"\"\"\n",
    "    absolute_file_path = \"{}/{}_{}_PARAPHRASE_GENERATION_PRED.txt\".format(save_directory, first_name,last_name)\n",
    "    output_file = open(absolute_file_path,'w')\n",
    "    output_file.write(\"Portuguese (trg)\\n\")\n",
    "    for item in prediction_list:\n",
    "        output_file.write(item+\"\\n\")\n",
    "    output_file.close()\n",
    "    print(\"submission file for the second question successfully saved at %s\"%absolute_file_path)\n",
    "\n",
    "# provide your firstname and lastname as arguments to out_prediction\n",
    "out_prediction('firstname', 'lastname', './drive/My Drive/Colab Notebooks/ckpt_pp_lab4', test_predictions)\n",
    "\n",
    "'''\n",
    "sample output:\n",
    "submission file for the second question successfully saved at ./drive/My Drive/Colab Notebooks/ckpt_pp_lab4/firstname_lastname_PARAPHRASE_GENERATION_PRED.txt\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 (BONUS) Please clearly describe the system you submitted in 2.1 (i.e., your best model) within 300 words.\n",
    "rubric={mechanics:5,resoning:5}\n",
    "\n",
    "Hints: \n",
    "1. Describe all the hyper-parameters of your submitted system. You may follow the structure of Development Instruction.\n",
    "2. The range of each of the hyper-parameters that you tested on (ex. Learning Rate: [0.001-0.1])\n",
    "3. List the strategies you attempted. What strategies did work? What did not work? (~a paragraph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Write your answer here.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 (BONUS) Please organize your code in `lab4_pp_colab.ipynb` and only keep the code that you used to train your submitted system in 2.1. Submit `lab4_pp_colab.ipynb`.\n",
    "rubric={mechanics:5}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3 Analyzing the MT model errors\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1  For this exercise, please read [this Atlantic article](https://www.theatlantic.com/technology/archive/2018/01/the-shallowness-of-google-translate/551570/) and then try to identify 2-3 issues you have noticed while working with Machine Translation models in any of the labs so far in this course. Such an effort will go through the following stages:\n",
    "rubric={reasoning:6}\n",
    "* Pick any MT model you have have trained.\n",
    "* Manually inspect a handful of examples.\n",
    "* Identify any 2-3 issues/problems with the model output translation. The issues can be ones that are not covered in the article as well. \n",
    "* For each issue, write a short description of what the issue is and give an example from your actual model output. 3 sentences for each issue are enough.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4 Conceptual Questions\n",
    "## 4.1 Non-whitespace languages (research question)\n",
    "While our standard approach of tokenization works fine for many languages, some languages don't have whitespace to allow for easy tokenization (e.g. Japanese, Chinese). There are two ways to avoid problems from this, first is to tokenize using a sophisticated tool, the second way is to use a character-based neural model. As it is likely that you may run into machine translation tasks with these languages, this question is to allow you to acquaint yourself with common tools and techniques that might be useful.\n",
    "### 4.1.1 C/J Tokenization\n",
    "rubric={reasoning:2}\n",
    "\n",
    "There are a number of different tools that you might be able to find for tokenizing Chinese and Japanese. For one of these two languages, perform a general search to identify one tokenization tool. For that tool, identify how often its is cited in the literature by searching on google scholar. We are looking for just a rough estimate (as the engine may have missed some actaul citations). [Hint: Most of these tools was also released as a paper, usually noted on the website of the tool]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.2 Character-based models (Bonus Question)\n",
    "rubric={reasoning:2}\n",
    "\n",
    "We can also avoid using these tools if we instead use a character-based approach (e.g. say our sentence is  ”我不喜欢苹果“ =\"I don't like apples\" you would break this into characters [我,不,喜,欢,苹,果] instead of 'words/phrases' [我,不,喜欢,苹果]). For the language of your choice (Chinese/Japanese), find 2 recent (2018-2020) papers in Machine Translation that use character-based approaches, briefly (3-4 sentences [not highly technical is fine] are enough) describe the model they use and share how they compare against \"word\" based approaches. \n",
    "\n",
    "Which method (Tokenize/character model) would you choose to use and why? (2-3 sentences are enough)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Transformer Concept Questions\n",
    "### 4.2.1 New Model. Better model?\n",
    "rubric={reasoning:2}\n",
    "\n",
    "Identify the clear advantages of the transformer model in comparison to the original seq2seq model based on LSTM?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.2 Transformer Encoder, enough?\n",
    "rubric={reasoning:2}\n",
    "\n",
    "For many tasks (Hint: not machine translation), people have used just the Encoder side of the Transformer architecture. What sorts of tasks might this make sense? What would the traditional alternative be and why might this be more effective?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.3 Encover VS. Decoder (Bonus)\n",
    "rubric={reasoning:2}\n",
    "\n",
    "For given sentence, which side (Encoder/Decoder) of the Transformer architecture would process it faster and why? (Hint: Assume you are running on powerful, parallelized, GPUs or TPUs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 BLEU\n",
    "### Sentence BLEU calculation\n",
    "\n",
    "For the following sentences follow the steps to compute the Sentence BLEU score by hand.  \n",
    "\n",
    "Candidate Sentence 1: \"The the the the the the the the the the the the the the the the\" [Length: 16]\n",
    "\n",
    "Candidate Sentence 2: \"The north wind and sun are awakening, which is even stronger.\" [Length: 11]\n",
    "\n",
    "Candidate Sentence 3: \"Sun had a quarrel\" [Length: 4]\n",
    "\n",
    "Reference Sentence: \"The North Wind and the Sun had a quarrel about which of them was the stronger.\" [Length: 16]\n",
    "\n",
    "### 4.3.1 Unigram Precision\n",
    "rubric={accuracy:2}\n",
    "\n",
    "Calculate the unigram precision (e.g. how each unigram appears in the reference sentence, divided by the length of the candidate sentence) for each of these sentences. (Hint: easy to just count words that are NOT in the reference) LEAVE YOUR ANSWER AS A FRACTION (e.g. 7/13)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Candidate 1:\n",
    "Candidate 2:\n",
    "Candidate 3 (Bonus):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.2 Clipped Unigram Precision\n",
    "rubric={accuracy:2}\n",
    "\n",
    "Calculate the unigram precision (as before) but clip it so that unigrams can only be counted up to the max number of times they occur in the reference sentence. LEAVE YOUR ANSWER AS A FRACTION (e.g. 7/13)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Candidate 1:\n",
    "Candidate 2:\n",
    "Candidate 3 (Bonus):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.3 Clipped n-gram Precision\n",
    "rubric={accuracy:3}\n",
    "\n",
    "As our clipped unigram precision before, but this time calculate the clipped 2,3,and 4-gram precisions of our sentences. (HINT: There are [length - (n-1)] n-grams in a sentence.)  "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Candidate 1:\n",
    "2gram:\n",
    "3gram:\n",
    "4gram:\n",
    "Candidate 2:\n",
    "2gram:\n",
    "3gram:\n",
    "4gram:\n",
    "Candidate 3 (Bonus):\n",
    "2gram:\n",
    "3gram:\n",
    "4gram:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.4 Brevity Penalty (Bonus Question)\n",
    "rubric={accuracy:2}\n",
    "\n",
    "BLEU gives a brevity penalty (BP) based on the length of the candidate sentence ($c$) compared to the reference sentence ($r$) as follows: for $c \\geq r$ then $BP = 1$ for $c < r$ then $BP = exp(1-r/c)$\n",
    "\n",
    "Compute the Brevity Penalty for our candidate sentences.  (Can leave as decimal)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Candidate 1:\n",
    "Candidate 2:\n",
    "Candidate 3:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.5 Final BLEU (Bonus Question)\n",
    "rubric={accuracy:3}\n",
    "\n",
    "The final BLEU score is then calculated as such: \n",
    "$BLEU = BP * exp(\\sum_{n=1}^N w_n log p_n)$   where $w_n$ is the weight for each of the n-grams (in our case use .25 each) and $p_n$ is the clipped precision for each of the n-grams."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Candidate 1:\n",
    "Candidate 2:\n",
    "Candidate 3:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
