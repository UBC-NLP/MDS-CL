{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LkZutwk50Nr-"
   },
   "source": [
    "## Homework 3 - Machine Translation - MDS Computational Linguistics\n",
    "\n",
    "### Assignment Topics\n",
    "- Seq2seq with attention\n",
    "- Evaluation metric\n",
    "\n",
    "\n",
    "### Software Requirements\n",
    "- Python (>=3.6)\n",
    "- PyTorch (>=1.2.0) \n",
    "- Jupyter (latest)\n",
    "\n",
    "### Submission Info.\n",
    "- Due Date: March 14, 2020, 18:00:00 (Vancouver time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1oK-CTEITGoY"
   },
   "source": [
    "## Load required packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "t9cDgntYJeOt",
    "outputId": "e8a23aa2-31d3-4740-dbb9-4c67b2dd5b63"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4Aq7Jt4fzDOz"
   },
   "source": [
    "### Required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R7oLWdxDRzUx"
   },
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import time\n",
    "import datetime\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence\n",
    "import torchtext\n",
    "from torchtext.datasets import TranslationDataset\n",
    "\n",
    "import spacy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7h8EV0DrJDXu"
   },
   "outputs": [],
   "source": [
    "## Set seed of randomization and working device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "tshLF-3YGdhW",
    "outputId": "65b20e10-1503-4946-852f-481b032c01de"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "manual_seed = 77\n",
    "torch.manual_seed(manual_seed)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "n_gpu = torch.cuda.device_count()\n",
    "if n_gpu > 0:\n",
    "    torch.cuda.manual_seed(manual_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hmeQ64MoJLyk"
   },
   "source": [
    "## Attention! Tidy Submission\n",
    "\n",
    "rubric={mechanics:3}\n",
    "\n",
    "To get the marks for tidy submission:\n",
    "- Submit the assignment by filling in this jupyter notebook with your answers embedded.\n",
    "- Be sure to follow the [general lab instructions](https://ubc-mds.github.io/resources_pages/general_lab_instructions).\n",
    "- You should not use this notebook (i.e., Lab3.ipynb) to train your model on Colab. Colab may change the layout of the jupyter notebook, and then we cannot grade on your notebook, the system will miss your grade!\n",
    "- We provide another jupyter notebook (i.e., `Lab3_exp.ipynb`) for you. You can load `Lab3_exp.ipynb` to Colab and run your experiments in this jupyter notebook. \n",
    "- Please download `Lab3_exp.ipynb` from Colab and include it in your final submission. \n",
    "- Some comments in your code will help us grade. Please use heading, comments, and mardown notations to organize your code. \n",
    "- Please feel free to add cells in `Lab3_exp.ipynb`.\n",
    "- You don't need to submit your checkpoints.\n",
    "- You can reuse any scripts of lab tutorials."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kWXGz9s8JP2l"
   },
   "source": [
    "**Dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iZFX5tj9JQ0i"
   },
   "source": [
    "In all the questions of this lab, we continue to use the English-French bilingual corpus of [Multi30k](https://github.com/multi30k/dataset) dataset that we used in lab2 and tutorials. Our task is to `translate text from French language to English language`. All your model should be trained on `train_eng_fre.tsv`, validated on `val_eng_fre.tsv`, and tested on `test_eng_fre.tsv`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_DFZF3ruJUzo"
   },
   "source": [
    "## Exercise 1: Seq2Seq Variant\n",
    "\n",
    "In last week, we used a **uni-directional LSTM Encoder** to compress the information of a source language into two context representation vectors of a fixed length (i.e., the final hidden and cell states). Then, we use the final hidden and cell states to initialize the hidden and cell states of uni-directional LSTM Decoder. However, the capability of these representations can be limited. They can easily forget the earlier information of a long sequence and co-reference relationships. Before introducing the attention mechanism, we can also use some tricks to solve the issue of context **information bottleneck** problem. \n",
    "\n",
    "In this exercise, please write a script to implement the following tricks in **a single seq2seq model**:\n",
    "1. Use a **bi-directional LSTM** as the **Encoder** to get the context representation of the source sentence. \n",
    "\n",
    "2. Instead of using the final hidden and cell states of the bi-directional encoder to initialize the uni-directional decoder, (A) for $h_0$ use **the mean of all hidden states** and (B) for $c_0$ use the **final cell state** of the bi-directional encoder. \n",
    "\n",
    "Combine 1 & 2 in **a single seq2seq model**.\n",
    "\n",
    "**Instruction:**\n",
    "- Please paste your experiment codes to answer the corresponding question below.\n",
    "- You should train your model with the following hyper-parameters:\n",
    "```\n",
    "INPUT_DIM = 6004\n",
    "OUTPUT_DIM = 6004\n",
    "ENC_EMB_DIM = 256\n",
    "DEC_EMB_DIM = 256\n",
    "N_LAYERS = 1\n",
    "BI_DIRECTION = True\n",
    "ENC_HID_DIM = 512\n",
    "DEC_HID_DIM = XXXXX (you should figure out this hyper-parameter). \n",
    "ENC_DROPOUT = 0.5\n",
    "DEC_DROPOUT = 0.3\n",
    "TEACH_FORCING_RATE = 0.5\n",
    "LEARNING_RT = 0.001\n",
    "MAX_EPOCH = 15\n",
    "optimizer = optim.Adam(model.parameters(),lr=LEARNING_RT)\n",
    "```\n",
    "- You should use `init_weights()` function from this week tutorial to initialize model with normal distribution with `mean=0` and `std=0.01`. \n",
    "- Your seed of randomization should be 77 (i.e., manual_seed = 77). \n",
    "- You should use `nn.CrossEntropyLoss()` loss function and ignore `<pad>` tokens.\n",
    "- You should save the model checkpoint at the end of each epoch. You also need to save your vocabularies.\n",
    "- You should use a different checkpoint directory to avoid overwriting previous models. \n",
    "- Then, you load the best checkpoint and evaluate it on TEST set. \n",
    "- You should keep your vocabularies and best checkpoints. We will use them in Exercise 3 (Error analysis). \n",
    "\n",
    "**Hints:**\n",
    "- Although the encoder is bi-directional LSTM, the decoder must be a uni-directional LSTM.\n",
    "- This exercise is more related to last tutorial (i.e., week 2). \n",
    "- The last hidden state of the LSTM is `h_n` of shape (num_layers * num_directions, batch, hidden_size).\n",
    "- The last cell state of the LSTM is `c_n` of shape (num_layers * num_directions, batch, hidden_size).\n",
    "- All the hidden states from the last LSTM layer is `output` of shape (seq_len, batch, num_directions * hidden_size).\n",
    "- The initialization states (i.e., $s_0$,$c_0$) of Decoder must match the dimension of Decoder. Namely, you should give a appropriate number of `DEC_HID_DIM` via analyzing the relation between tensor shapes. \n",
    "- You can use `print(XXX.shape)` to check the shape of your tensor. If the tensor shape doesn't match the desired shape of tensor, you should reshape it using `.view(), .squeeze(), .unsqueeze() or .permute() function.`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bqkALC9SJXas"
   },
   "source": [
    "**To facilitate your model evaluation, we provide a `inference()` function which calculates BLEU score based on a test corpus (test_eng_fre.tsv).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pkTgym10JdPF"
   },
   "outputs": [],
   "source": [
    "def inference(model, file_name, src_vocab, trg_vocab, attention=False, max_trg_len = 64):\n",
    "    '''\n",
    "    Function for translation inference\n",
    "\n",
    "    Input: \n",
    "    model: translation model;\n",
    "    file_name: the directoy of test file that the first column is target reference, and the second column is source language;\n",
    "    trg_vocab: Target torchtext Field\n",
    "    attention: the model returns attention weights or not.\n",
    "    max_trg_len: the maximal length of translation text (optinal), default = 64\n",
    "\n",
    "    Output:\n",
    "    Corpus BLEU score.\n",
    "    '''\n",
    "    from nltk.translate.bleu_score import corpus_bleu\n",
    "    from nltk.translate.bleu_score import sentence_bleu\n",
    "    from torchtext.data import TabularDataset\n",
    "    from torchtext.data import Iterator\n",
    "\n",
    "    # convert index to text string\n",
    "    def convert_itos(convert_vocab, token_ids):\n",
    "        list_string = []\n",
    "        for i in token_ids:\n",
    "            if i == convert_vocab.vocab.stoi['<eos>']:\n",
    "                break\n",
    "            else:\n",
    "                token = convert_vocab.vocab.itos[i]\n",
    "                list_string.append(token)\n",
    "        return list_string\n",
    "\n",
    "    test = TabularDataset(\n",
    "      path=file_name, # the root directory where the data lies\n",
    "      format='tsv',\n",
    "      skip_header=True, # if your tsv file has a header, make sure to pass this to ensure it doesn't get proceesed as data!\n",
    "      fields=[('TRG', trg_vocab), ('SRC', src_vocab)])\n",
    "\n",
    "    test_iter = Iterator(\n",
    "    dataset = test, # we pass in the datasets we want the iterator to draw data from\n",
    "    sort = False,batch_size=128,\n",
    "    sort_key=None,\n",
    "    shuffle=False,\n",
    "    sort_within_batch=False,\n",
    "    device = device,\n",
    "    train=False\n",
    "    )\n",
    "  \n",
    "    model.eval()\n",
    "    all_trg = []\n",
    "    all_translated_trg = []\n",
    "\n",
    "    TRG_PAD_IDX = trg_vocab.vocab.stoi[trg_vocab.pad_token]\n",
    "\n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for i, batch in enumerate(test_iter):\n",
    "\n",
    "            src = batch.SRC\n",
    "            #src = [src len, batch size]\n",
    "\n",
    "            trg = batch.TRG\n",
    "            #trg = [trg len, batch size]\n",
    "\n",
    "            batch_size = trg.shape[1]\n",
    "\n",
    "            # create a placeholder for traget language with shape of [max_trg_len, batch_size] where all the elements are the index of <pad>. Then send to device\n",
    "            trg_placeholder = torch.Tensor(max_trg_len, batch_size)\n",
    "            trg_placeholder.fill_(TRG_PAD_IDX)\n",
    "            trg_placeholder = trg_placeholder.long().to(device)\n",
    "            if attention == True:\n",
    "                output,_ = model(src, trg_placeholder, 0) #turn off teacher forcing\n",
    "            else:\n",
    "                output = model(src, trg_placeholder, 0) #turn off teacher forcing\n",
    "            # get translation results, we ignor first token <sos> in both translation and target sentences. \n",
    "            # output_translate = [(trg len - 1), batch, output dim] output dim is size of target vocabulary.\n",
    "            output_translate = output[1:]\n",
    "            # store gold target sentences to a list \n",
    "            all_trg.append(trg[1:].cpu())\n",
    "\n",
    "            # Choose top 1 word from decoder's output, we get the probability and index of the word\n",
    "            prob, token_id = output_translate.data.topk(1)\n",
    "            translation_token_id = token_id.squeeze(2).cpu()\n",
    "\n",
    "            # store gold target sentences to a list \n",
    "            all_translated_trg.append(translation_token_id)\n",
    "      \n",
    "    all_gold_text = []\n",
    "    all_translated_text = []\n",
    "    for i in range(len(all_trg)): \n",
    "        cur_gold = all_trg[i]\n",
    "        cur_translation = all_translated_trg[i]\n",
    "        for j in range(cur_gold.shape[1]):\n",
    "            gold_convered_strings = convert_itos(trg_vocab,cur_gold[:,j])\n",
    "            trans_convered_strings = convert_itos(trg_vocab,cur_translation[:,j])\n",
    "\n",
    "            all_gold_text.append(gold_convered_strings)\n",
    "            all_translated_text.append(trans_convered_strings)\n",
    "\n",
    "    corpus_all_gold_text = [[item] for item in all_gold_text]\n",
    "    corpus_bleu_score = corpus_bleu(corpus_all_gold_text, all_translated_text)  \n",
    "    return corpus_bleu_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SC6WA4ZcJxao"
   },
   "source": [
    "`inference()` function will take five variables `model, file_name, trg_vocab,attention, and max_trg_len` as inputs and return a corpus cumulative BLEU-4 score. Here is a use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "279rwqFNKC0n"
   },
   "outputs": [],
   "source": [
    "# use case\n",
    "print(inference(model_best, \"./drive/My Drive/Colab Notebooks/eng-fre/test_eng_fre.tsv\", SRC, TRG, True, 64))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-CAFa492KEcx"
   },
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 952
    },
    "colab_type": "code",
    "id": "BdFGgNbdR7XJ",
    "outputId": "1d1a1eea-98ad-4af0-fdfa-db9928ab128b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: en_core_web_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz#egg=en_core_web_sm==2.2.5 in /usr/local/lib/python3.6/dist-packages (2.2.5)\n",
      "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.21.0)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
      "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.3)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.18.2)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.6.0)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.2)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (46.0.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.38.0)\n",
      "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.5.0)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2019.11.28)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.1.0)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n",
      "Collecting fr_core_news_sm==2.2.5\n",
      "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/fr_core_news_sm-2.2.5/fr_core_news_sm-2.2.5.tar.gz (14.7MB)\n",
      "\u001b[K     |████████████████████████████████| 14.7MB 96.1MB/s \n",
      "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from fr_core_news_sm==2.2.5) (2.2.4)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (0.4.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (1.18.2)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (1.0.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (46.0.0)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (0.6.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (4.38.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (2.0.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (2.21.0)\n",
      "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (7.4.0)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (1.0.2)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (3.0.2)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (1.0.2)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (1.1.3)\n",
      "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->fr_core_news_sm==2.2.5) (1.5.0)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->fr_core_news_sm==2.2.5) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->fr_core_news_sm==2.2.5) (2019.11.28)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->fr_core_news_sm==2.2.5) (1.24.3)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->fr_core_news_sm==2.2.5) (2.8)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->fr_core_news_sm==2.2.5) (3.1.0)\n",
      "Building wheels for collected packages: fr-core-news-sm\n",
      "  Building wheel for fr-core-news-sm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for fr-core-news-sm: filename=fr_core_news_sm-2.2.5-cp36-none-any.whl size=14727027 sha256=cec04933c67cd2bd0c96b9cba6e326171642888a5c621af402919cabb7fb2f7c\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-xl0gkjsb/wheels/46/1b/e6/29b020e3f9420a24c3f463343afe5136aaaf955dbc9e46dfc5\n",
      "Successfully built fr-core-news-sm\n",
      "Installing collected packages: fr-core-news-sm\n",
      "Successfully installed fr-core-news-sm-2.2.5\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('fr_core_news_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm\n",
    "!python -m spacy download fr_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iObo7XbCR9Jp"
   },
   "outputs": [],
   "source": [
    "import fr_core_news_sm\n",
    "import en_core_web_sm\n",
    "\n",
    "spacy_fr = fr_core_news_sm.load()\n",
    "spacy_en = en_core_web_sm.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Fm1t67sqR-wb"
   },
   "outputs": [],
   "source": [
    "def tokenize_fr(text):\n",
    "    \"\"\"\n",
    "    Tokenizes French text from a string into a list of strings (tokens)\n",
    "    \"\"\"\n",
    "    return [tok.text for tok in spacy_fr.tokenizer(text)]\n",
    "\n",
    "def tokenize_en(text):\n",
    "    \"\"\"\n",
    "    Tokenizes English text from a string into a list of strings (tokens)\n",
    "    \"\"\"\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_jRAEDqLSBgO"
   },
   "outputs": [],
   "source": [
    "SRC = torchtext.data.Field(tokenize = tokenize_fr, \n",
    "            init_token = '<sos>', \n",
    "            eos_token = '<eos>', \n",
    "            lower = True)\n",
    "TRG = torchtext.data.Field(tokenize = tokenize_en, \n",
    "            init_token = '<sos>', \n",
    "            eos_token = '<eos>', \n",
    "            lower = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WlGJ0oI5SC78"
   },
   "outputs": [],
   "source": [
    "train, val, test = torchtext.data.TabularDataset.splits(\n",
    "    path='./eng-fre/', train='train_eng_fre.tsv',validation='val_eng_fre.tsv', test='test_eng_fre.tsv', \n",
    "    format='tsv', skip_header=True, fields=[('TRG', TRG), ('SRC', SRC)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "GwEwSf2tSEQ-",
    "outputId": "778e9531-822c-442a-ef77-d1eb81c9c4e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 29000\n",
      "Number of validation examples: 1014\n",
      "Number of testing examples: 1000\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of training examples: {len(train.examples)}\")\n",
    "print(f\"Number of validation examples: {len(val.examples)}\")\n",
    "print(f\"Number of testing examples: {len(test.examples)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "9pEhwU5QSFth",
    "outputId": "ce788637-42fd-445b-dfa7-1109a7bb0ad4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'TRG': ['two', 'young', ',', 'white', 'males', 'are', 'outside', 'near', 'many', 'bushes', '.'], 'SRC': ['deux', 'jeunes', 'hommes', 'blancs', 'sont', 'dehors', 'près', 'de', 'buissons', '.']}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(vars(train.examples[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "hUZ-3ccnSHUL",
    "outputId": "aa0c7fdd-5f50-408b-bc4c-229dc701501f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'TRG': ['an', 'older', ',', 'overweight', 'man', 'flips', 'a', 'pancake', 'while', 'making', 'breakfast', '.'], 'SRC': ['un', 'homme', 'âgé', 'en', 'surpoids', 'fait', 'sauter', 'une', 'crêpe', 'en', 'préparant', 'le', 'petit', 'déjeuner', '.']}\n"
     ]
    }
   ],
   "source": [
    "print(vars(val.examples[100]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9wmQA1LkSIiM"
   },
   "outputs": [],
   "source": [
    "TRG.build_vocab(train, max_size = 6000) #<--------------\n",
    "SRC.build_vocab(train, max_size = 6000) #<--------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "52jaZ1XtSmDH",
    "outputId": "6365033e-2ff2-48fa-acdf-1d56a522135a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique tokens in source (fr) vocabulary: 6004\n",
      "Unique tokens in target (en) vocabulary: 6004\n"
     ]
    }
   ],
   "source": [
    "print(f\"Unique tokens in source (fr) vocabulary: {len(SRC.vocab)}\")\n",
    "print(f\"Unique tokens in target (en) vocabulary: {len(TRG.vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0JwhFrNyS0OB"
   },
   "outputs": [],
   "source": [
    "train_iter, val_iter, test_iter = torchtext.data.BucketIterator.splits(\n",
    "    (train, val, test), # we pass in the datasets we want the iterator to draw data from\n",
    "    batch_sizes=(16, 256, 256),device = device,\n",
    "    sort_key=lambda x: len(x.SRC), # the BucketIterator needs to be told what function it should use to group the data.\n",
    "    sort_within_batch=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ss9kRF08e8wA"
   },
   "source": [
    "## Functions of fully training process\n",
    "If we test our code successfully. We will start the fully training loop as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CSdedl1-W4Uv"
   },
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, clip):\n",
    "    manual_seed = 77\n",
    "    torch.manual_seed(manual_seed)\n",
    "    if n_gpu > 0:\n",
    "        torch.cuda.manual_seed(manual_seed)\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for i, batch in enumerate(iterator):\n",
    "        \n",
    "        src = batch.SRC\n",
    "        trg = batch.TRG\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(src, trg)\n",
    "        \n",
    "        #trg = [trg len, batch size]\n",
    "        #output = [trg len, batch size, output dim]\n",
    "        \n",
    "        output_dim = output.shape[-1]\n",
    "        \n",
    "        output = output[1:].view(-1, output_dim)\n",
    "        trg = trg[1:].view(-1)\n",
    "        \n",
    "        # loss function works only 2d logits, 1d targets\n",
    "        # so flatten the trg, output tensors. Ignore the <sos> token\n",
    "        # trg shape shape should be [(sequence_len - 1) * batch_size]\n",
    "        # output shape should be [(sequence_len - 1) * batch_size, output_dim]\n",
    "        \n",
    "        loss = criterion(output, trg)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    bleu = inference(model, \"./eng-fre/train_eng_fre.tsv\", SRC, TRG, False, 64) \n",
    "    return epoch_loss / len(iterator), bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hUbwIKpCe656"
   },
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for i, batch in enumerate(iterator):\n",
    "\n",
    "            src = batch.SRC\n",
    "            trg = batch.TRG\n",
    "\n",
    "            output = model(src, trg, 0) #turn off teacher forcing\n",
    "\n",
    "            #trg = [trg len, batch size]\n",
    "            #output = [trg len, batch size, output dim]\n",
    "\n",
    "            output_dim = output.shape[-1]\n",
    "            \n",
    "            output = output[1:].view(-1, output_dim)\n",
    "            trg = trg[1:].view(-1)\n",
    "\n",
    "            #trg = [(trg len - 1) * batch size]\n",
    "            #output = [(trg len - 1) * batch size, output dim]\n",
    "\n",
    "            loss = criterion(output, trg)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "    bleu = inference(model, \"./eng-fre/val_eng_fre.tsv\", SRC, TRG, False, 64)\n",
    "    return epoch_loss / len(iterator), bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Fkv8iCn1Vsm7"
   },
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    manual_seed = 77\n",
    "    torch.manual_seed(manual_seed)\n",
    "    if n_gpu > 0:\n",
    "        torch.cuda.manual_seed(manual_seed)\n",
    "    for name, param in m.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            nn.init.normal_(param.data, mean=0, std=0.01)\n",
    "        else:\n",
    "            nn.init.constant_(param.data, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PWJOFuf4e_Q1"
   },
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7D6IkXuiTDOo"
   },
   "source": [
    "## Start your works here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u7yevrg8Tjd6"
   },
   "source": [
    "`class Encoder(nn.Module):`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PayD6nnLS-zi"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, enc_hid_dim,n_layers, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        self.emb_dim = emb_dim\n",
    "        self.enc_hid_dim = enc_hid_dim\n",
    "        self.dropout = dropout\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        self.lstm = nn.LSTM(emb_dim, enc_hid_dim, n_layers, dropout=dropout, bidirectional=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, src):\n",
    "        \n",
    "        #src = [src len, batch size]\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "        \n",
    "        #embedded = [src len, batch size, emb dim]\n",
    "        \n",
    "        outputs, (hidden, cell) = self.lstm(embedded)\n",
    "       \n",
    "        # outputs are always from the top hidden layer, if bidirectional outputs are concatenated.\n",
    "        # outputs shape [sequence_length, batch_size, hidden_dim * num_directions]\n",
    "        # hidden is of shape [num_layers * num_directions, batch_size, hidden_size]\n",
    "        # cell is of shape [num_layers * num_directions, batch_size, hidden_size]\n",
    "        \n",
    "        return outputs, hidden, cell #<--------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3w45C-RYTnTT"
   },
   "source": [
    "`class Decoder(nn.Module):`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vpPMP55aTXjb"
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, dec_hid_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        self.emb_dim = emb_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.dec_hid_dim = dec_hid_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        self.lstm = nn.LSTM(emb_dim, dec_hid_dim, n_layers, dropout=dropout)\n",
    "        self.fc_out = nn.Linear(dec_hid_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, input, hidden, cell):\n",
    "             \n",
    "        # input is of shape [batch_size]\n",
    "        # hidden is of shape [n_layer * num_directions, batch_size, hidden_size]\n",
    "        # cell is of shape [n_layer * num_directions, batch_size, hidden_size]\n",
    "        \n",
    "        input = input.unsqueeze(0)\n",
    "        \n",
    "        # input shape is [1, batch_size]. reshape is needed rnn expects a rank 3 tensors as input.\n",
    "        # so reshaping to [1, batch_size] means a batch of batch_size each containing 1 index.\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        \n",
    "        #embedded = [1, batch size, emb dim]    \n",
    "        output, (hidden, cell) = self.lstm(embedded, (hidden, cell))\n",
    "        \n",
    "        # output shape is [sequence_len, batch_size, hidden_dim * num_directions]\n",
    "        # hidden shape is [num_layers * num_directions, batch_size, hidden_dim]\n",
    "        # cell shape is [num_layers * num_directions, batch_size, hidden_dim]\n",
    "\n",
    "        # sequence_len and num_directions will always be 1 in the decoder.\n",
    "        # output shape is [1, batch_size, hidden_dim]\n",
    "        # hidden shape is [num_layers, batch_size, hidden_dim]\n",
    "        # cell shape is [num_layers, batch_size, hidden_dim]\n",
    "        \n",
    "        prediction = self.fc_out(hidden.squeeze(0)) # linear expects as rank 2 tensor as input\n",
    "        # predicted shape is [batch_size, output_dim]\n",
    "        \n",
    "        return prediction, hidden, cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P5sxsPv3TqU8"
   },
   "source": [
    "`class Seq2Seq(nn.Module):`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bIZCHNK1Tf46"
   },
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    ''' This class contains the implementation of complete sequence to sequence network.\n",
    "    It uses to encoder to produce the context vectors.\n",
    "    It uses the decoder to produce the predicted target sentence.\n",
    "    Args:\n",
    "        encoder: A Encoder class instance.\n",
    "        decoder: A Decoder class instance.\n",
    "    '''\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
    "        # src is of shape [sequence_len, batch_size]\n",
    "        # trg is of shape [sequence_len, batch_size]\n",
    "        # if teacher_forcing_ratio is 0.5 we use ground-truth inputs 50% of time and 50% time we use decoder outputs.\n",
    "\n",
    "        batch_size = trg.shape[1]\n",
    "\n",
    "        max_len = trg.shape[0]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "\n",
    "        # to store the outputs of the decoder\n",
    "        outputs = torch.zeros(max_len, batch_size, trg_vocab_size).to(self.device)\n",
    "\n",
    "        # context vector, last hidden and cell state of encoder to initialize the decoder\n",
    "        encoder_outputs, _, cell = self.encoder(src)\n",
    "        # hidden shape is [num_layers * num_directions, batch_size, hidden_dim]\n",
    "        hidden = torch.mean(encoder_outputs, dim=0).unsqueeze(0) #<--------------\n",
    "\n",
    "        decoder_dim = hidden.shape[-2]\n",
    "        # cell is of shape [num_layers * num_directions, batch_size, hidden_size]\n",
    "        cell = torch.cat((cell[0,:,:],cell[1,:,:]),dim=-1).unsqueeze(0) #<--------------\n",
    "        \n",
    "        # first input to the decoder is the <sos> tokens\n",
    "        input = trg[0, :]\n",
    "\n",
    "        for t in range(1, max_len):\n",
    "            output, hidden, cell = self.decoder(input, hidden, cell)\n",
    "            outputs[t] = output\n",
    "            use_teacher_force = random.random() < teacher_forcing_ratio\n",
    "            top1 = output.max(1)[1]\n",
    "            input = (trg[t] if use_teacher_force else top1)\n",
    "\n",
    "        # outputs is of shape [sequence_len, batch_size, output_dim]\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c3efGZqfyYU5"
   },
   "source": [
    "## Instantiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "rw9NBrhkTtz1",
    "outputId": "57f0cecf-a54f-4633-cc99-2347620f7c42"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chiyu94/py3.6/lib/python3.6/site-packages/torch/nn/modules/rnn.py:51: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n",
      "/home/chiyu94/py3.6/lib/python3.6/site-packages/torch/nn/modules/rnn.py:51: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    }
   ],
   "source": [
    "INPUT_DIM = len(SRC.vocab)\n",
    "OUTPUT_DIM = len(TRG.vocab)\n",
    "ENC_EMB_DIM = 256\n",
    "DEC_EMB_DIM = 256\n",
    "ENC_HID_DIM = 512\n",
    "DEC_HID_DIM = 1024 #<--------------\n",
    "ENC_DROPOUT = 0.5\n",
    "DEC_DROPOUT = 0.3\n",
    "N_LAYERS = 1\n",
    "LEARNING_RT = 0.001\n",
    "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, N_LAYERS, ENC_DROPOUT)\n",
    "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, DEC_HID_DIM, N_LAYERS, DEC_DROPOUT)\n",
    "\n",
    "model = Seq2Seq(enc, dec,device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KIe8z4CpyGTC"
   },
   "outputs": [],
   "source": [
    "model.apply(init_weights)\n",
    "optimizer = optim.Adam(model.parameters(), lr = LEARNING_RT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "WR0L0HXDWbg8",
    "outputId": "43dc5655-9121-4e83-95c8-a34720b22424"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pad> token index:  1\n"
     ]
    }
   ],
   "source": [
    "TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]\n",
    "print('<pad> token index: ',TRG_PAD_IDX)\n",
    "## we will ignor the pad token in true target set\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZrwiDMRUy8Js"
   },
   "source": [
    "## Prototype Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "PYECcogzWhzU",
    "outputId": "7c838bd7-0125-43a6-da65-af97b51587df"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5438, device='cuda:0', grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "clip = 1\n",
    "model.train()\n",
    "\n",
    "for i, batch in enumerate(train_iter):\n",
    "\n",
    "    src = batch.SRC\n",
    "    trg = batch.TRG\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    output = model(src, trg)\n",
    "    #trg = [trg len, batch size]\n",
    "    #output = [trg len, batch size, output dim]\n",
    "\n",
    "    output_dim = output.shape[-1]\n",
    "\n",
    "    output = output[1:].view(-1, output_dim)\n",
    "    trg = trg[1:].view(-1)\n",
    "\n",
    "    #trg = [(trg len - 1) * batch size]\n",
    "    #output = [(trg len - 1) * batch size, output dim]\n",
    "\n",
    "    loss = criterion(output, trg)\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "    print(loss/src.shape[1])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BN5c7jG1fyRY"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"./ckpt_ex1/TRG.Field\",\"wb\")as f:\n",
    "     pickle.dump(TRG,f)\n",
    "\n",
    "with open(\"./ckpt_ex1/SRC.Field\",\"wb\")as f:\n",
    "     pickle.dump(SRC,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XM-4OWGvxzMR"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 232
    },
    "colab_type": "code",
    "id": "ZxEynmkAfB1T",
    "outputId": "dc0aaf8e-f5e5-4194-d49d-2300826aac71"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Time: 2m 9s\n",
      "\tTrain Loss: 4.327 | Train PPL:  75.721\n",
      "\t Val. Loss: 4.369 |  Val. PPL:  78.996\n",
      "\t Train BLEU:0.032 |  Val. BLEU:   0.031\n",
      "Epoch: 02 | Time: 2m 6s\n",
      "\tTrain Loss: 3.466 | Train PPL:  32.009\n",
      "\t Val. Loss: 3.911 |  Val. PPL:  49.973\n",
      "\t Train BLEU:0.122 |  Val. BLEU:   0.117\n",
      "Epoch: 03 | Time: 2m 6s\n",
      "\tTrain Loss: 2.977 | Train PPL:  19.625\n",
      "\t Val. Loss: 3.589 |  Val. PPL:  36.211\n",
      "\t Train BLEU:0.170 |  Val. BLEU:   0.152\n",
      "Epoch: 04 | Time: 2m 6s\n",
      "\tTrain Loss: 2.616 | Train PPL:  13.678\n",
      "\t Val. Loss: 3.452 |  Val. PPL:  31.559\n",
      "\t Train BLEU:0.217 |  Val. BLEU:   0.185\n",
      "Epoch: 05 | Time: 2m 6s\n",
      "\tTrain Loss: 2.320 | Train PPL:  10.177\n",
      "\t Val. Loss: 3.393 |  Val. PPL:  29.743\n",
      "\t Train BLEU:0.267 |  Val. BLEU:   0.217\n",
      "Epoch: 06 | Time: 2m 7s\n",
      "\tTrain Loss: 2.063 | Train PPL:   7.870\n",
      "\t Val. Loss: 3.396 |  Val. PPL:  29.843\n",
      "\t Train BLEU:0.321 |  Val. BLEU:   0.231\n",
      "Epoch: 07 | Time: 2m 6s\n",
      "\tTrain Loss: 1.821 | Train PPL:   6.176\n",
      "\t Val. Loss: 3.451 |  Val. PPL:  31.540\n",
      "\t Train BLEU:0.366 |  Val. BLEU:   0.239\n",
      "Epoch: 08 | Time: 2m 6s\n",
      "\tTrain Loss: 1.619 | Train PPL:   5.050\n",
      "\t Val. Loss: 3.512 |  Val. PPL:  33.529\n",
      "\t Train BLEU:0.416 |  Val. BLEU:   0.248\n",
      "Epoch: 09 | Time: 2m 6s\n",
      "\tTrain Loss: 1.432 | Train PPL:   4.189\n",
      "\t Val. Loss: 3.599 |  Val. PPL:  36.562\n",
      "\t Train BLEU:0.466 |  Val. BLEU:   0.253\n",
      "Epoch: 10 | Time: 2m 7s\n",
      "\tTrain Loss: 1.283 | Train PPL:   3.607\n",
      "\t Val. Loss: 3.700 |  Val. PPL:  40.427\n",
      "\t Train BLEU:0.510 |  Val. BLEU:   0.246\n",
      "Epoch: 11 | Time: 2m 7s\n",
      "\tTrain Loss: 1.140 | Train PPL:   3.126\n",
      "\t Val. Loss: 3.763 |  Val. PPL:  43.075\n",
      "\t Train BLEU:0.547 |  Val. BLEU:   0.243\n",
      "Epoch: 12 | Time: 2m 6s\n",
      "\tTrain Loss: 1.036 | Train PPL:   2.818\n",
      "\t Val. Loss: 3.798 |  Val. PPL:  44.631\n",
      "\t Train BLEU:0.590 |  Val. BLEU:   0.250\n",
      "Epoch: 13 | Time: 2m 7s\n",
      "\tTrain Loss: 0.931 | Train PPL:   2.537\n",
      "\t Val. Loss: 3.890 |  Val. PPL:  48.896\n",
      "\t Train BLEU:0.624 |  Val. BLEU:   0.250\n",
      "Epoch: 14 | Time: 2m 6s\n",
      "\tTrain Loss: 0.846 | Train PPL:   2.331\n",
      "\t Val. Loss: 4.007 |  Val. PPL:  54.995\n",
      "\t Train BLEU:0.645 |  Val. BLEU:   0.244\n",
      "Epoch: 15 | Time: 2m 7s\n",
      "\tTrain Loss: 0.776 | Train PPL:   2.173\n",
      "\t Val. Loss: 4.085 |  Val. PPL:  59.454\n",
      "\t Train BLEU:0.680 |  Val. BLEU:   0.247\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 15\n",
    "CLIP = 1\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss,bleu_train = train(model, train_iter, optimizer, criterion, CLIP)\n",
    "    valid_loss,bleu_val = evaluate(model, val_iter, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    # Create checkpoint at end of each epoch\n",
    "    state_dict_model = model.state_dict() \n",
    "    state = {\n",
    "        'epoch': epoch,\n",
    "        'state_dict': state_dict_model,\n",
    "        'optimizer': optimizer.state_dict()\n",
    "        }\n",
    "\n",
    "    torch.save(state, \"./ckpt_ex1/seq2seq_\"+str(epoch+1)+\".pt\")\n",
    "\n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n",
    "    print(f'\\t Train BLEU:{bleu_train:.3f} |  Val. BLEU: {bleu_val:7.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./ckpt_ex1/TRG.Field\",\"rb\") as f:\n",
    "     TRG_ex1_aved = pickle.load(f)\n",
    "\n",
    "with open(\"./ckpt_ex1/SRC.Field\",\"rb\") as f:\n",
    "     SRC_ex1_saved = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "9xRyCfVC440b",
    "outputId": "0fe1a842-f944-460d-ddd9-645fed2a9f4f"
   },
   "outputs": [],
   "source": [
    "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, N_LAYERS, ENC_DROPOUT)\n",
    "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, DEC_HID_DIM, N_LAYERS, DEC_DROPOUT)\n",
    "\n",
    "model_ex1_best = Seq2Seq(enc, dec, device)\n",
    "\n",
    "model_ex1_best.load_state_dict(torch.load('./ckpt_ex1/seq2seq_9.pt')['state_dict'])\n",
    "model_ex1_best = model_ex1_best.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Rm9kVH3V4Bzm",
    "outputId": "971a29f7-962e-4ef6-ac78-e2002ebc6c60"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2655384705729738"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inference(model_ex1_best, \"./eng-fre/test_eng_fre.tsv\", SRC_ex1_saved, TRG_ex1_aved, False, 64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jy8jqg1AKcNe"
   },
   "source": [
    "### 1.0 Please paste your full training log here. Which epoch is the best?\n",
    "rubric={accuracy:2}\n",
    "Your log should look like this:\n",
    "```\n",
    "Epoch: 01 | Time: 1m 25s\n",
    "\tTrain Loss: 4.293 | Train PPL:  73.188\n",
    "\t Val. Loss: 4.263 |  Val. PPL:  71.012 \n",
    "   ............\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bAsq8SKOKhvi"
   },
   "source": [
    "**Your answer:**\n",
    "**My best model is trained with 9 epochs.**   (Your best model may get at different epochs. The best model should be the model which obtained best BLEU score on validation set.) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XwOaIdmHKyaV"
   },
   "source": [
    "```\n",
    "Epoch: 01 | Time: 2m 9s\n",
    "\tTrain Loss: 4.327 | Train PPL:  75.721\n",
    "\t Val. Loss: 4.369 |  Val. PPL:  78.996\n",
    "\t Train BLEU:0.032 |  Val. BLEU:   0.031\n",
    "Epoch: 02 | Time: 2m 6s\n",
    "\tTrain Loss: 3.466 | Train PPL:  32.009\n",
    "\t Val. Loss: 3.911 |  Val. PPL:  49.973\n",
    "\t Train BLEU:0.122 |  Val. BLEU:   0.117\n",
    "Epoch: 03 | Time: 2m 6s\n",
    "\tTrain Loss: 2.977 | Train PPL:  19.625\n",
    "\t Val. Loss: 3.589 |  Val. PPL:  36.211\n",
    "\t Train BLEU:0.170 |  Val. BLEU:   0.152\n",
    "Epoch: 04 | Time: 2m 6s\n",
    "\tTrain Loss: 2.616 | Train PPL:  13.678\n",
    "\t Val. Loss: 3.452 |  Val. PPL:  31.559\n",
    "\t Train BLEU:0.217 |  Val. BLEU:   0.185\n",
    "Epoch: 05 | Time: 2m 6s\n",
    "\tTrain Loss: 2.320 | Train PPL:  10.177\n",
    "\t Val. Loss: 3.393 |  Val. PPL:  29.743\n",
    "\t Train BLEU:0.267 |  Val. BLEU:   0.217\n",
    "Epoch: 06 | Time: 2m 7s\n",
    "\tTrain Loss: 2.063 | Train PPL:   7.870\n",
    "\t Val. Loss: 3.396 |  Val. PPL:  29.843\n",
    "\t Train BLEU:0.321 |  Val. BLEU:   0.231\n",
    "Epoch: 07 | Time: 2m 6s\n",
    "\tTrain Loss: 1.821 | Train PPL:   6.176\n",
    "\t Val. Loss: 3.451 |  Val. PPL:  31.540\n",
    "\t Train BLEU:0.366 |  Val. BLEU:   0.239\n",
    "Epoch: 08 | Time: 2m 6s\n",
    "\tTrain Loss: 1.619 | Train PPL:   5.050\n",
    "\t Val. Loss: 3.512 |  Val. PPL:  33.529\n",
    "\t Train BLEU:0.416 |  Val. BLEU:   0.248\n",
    "Epoch: 09 | Time: 2m 6s\n",
    "\tTrain Loss: 1.432 | Train PPL:   4.189\n",
    "\t Val. Loss: 3.599 |  Val. PPL:  36.562\n",
    "\t Train BLEU:0.466 |  Val. BLEU:   0.253\n",
    "Epoch: 10 | Time: 2m 7s\n",
    "\tTrain Loss: 1.283 | Train PPL:   3.607\n",
    "\t Val. Loss: 3.700 |  Val. PPL:  40.427\n",
    "\t Train BLEU:0.510 |  Val. BLEU:   0.246\n",
    "Epoch: 11 | Time: 2m 7s\n",
    "\tTrain Loss: 1.140 | Train PPL:   3.126\n",
    "\t Val. Loss: 3.763 |  Val. PPL:  43.075\n",
    "\t Train BLEU:0.547 |  Val. BLEU:   0.243\n",
    "Epoch: 12 | Time: 2m 6s\n",
    "\tTrain Loss: 1.036 | Train PPL:   2.818\n",
    "\t Val. Loss: 3.798 |  Val. PPL:  44.631\n",
    "\t Train BLEU:0.590 |  Val. BLEU:   0.250\n",
    "Epoch: 13 | Time: 2m 7s\n",
    "\tTrain Loss: 0.931 | Train PPL:   2.537\n",
    "\t Val. Loss: 3.890 |  Val. PPL:  48.896\n",
    "\t Train BLEU:0.624 |  Val. BLEU:   0.250\n",
    "Epoch: 14 | Time: 2m 6s\n",
    "\tTrain Loss: 0.846 | Train PPL:   2.331\n",
    "\t Val. Loss: 4.007 |  Val. PPL:  54.995\n",
    "\t Train BLEU:0.645 |  Val. BLEU:   0.244\n",
    "Epoch: 15 | Time: 2m 7s\n",
    "\tTrain Loss: 0.776 | Train PPL:   2.173\n",
    "\t Val. Loss: 4.085 |  Val. PPL:  59.454\n",
    "\t Train BLEU:0.680 |  Val. BLEU:   0.247\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5cjlNIeALW4g"
   },
   "source": [
    "### 1.1 Please report the cumulative BLEU-4 score on test set (i.e., `test_eng_fre.tsv`) via corpus_bleu() function.\n",
    "rubric={accuracy:2}\n",
    "\n",
    "Hint: You can use `inference()` function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2Gy0xzi8LYEA"
   },
   "source": [
    "**Your answer goes here**\n",
    "\n",
    "**My best model obtains 0.27 cumulative BLEU-4 score.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IUKeK6YTL1os"
   },
   "source": [
    "### Please paste your code for the corresponding questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OfKi9skwL2iG"
   },
   "source": [
    "### 1.2 Pleae revise the following code to build the appropriate vocabularies:  (the torchtext Field with add four more special tokens: \\<pad\\>,\\<unk\\>,\\<sos\\>,\\<eos\\>.)\n",
    "rubric={accuracy:2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r93Ig8RYMYpB"
   },
   "outputs": [],
   "source": [
    "TRG.build_vocab(train, max_size = 6000) #<--------------\n",
    "SRC.build_vocab(train, max_size = 6000) #<--------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "76sCGlKSL3Lw"
   },
   "source": [
    "### 1.3 You may need to revise `class Encoder`. Please show your code for `class Encoder`: (You average hidden states and concatenate cell states either in Encoder class or Seq2Seq class.) \n",
    "rubric={accuracy:2, efficiency:2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wcn_Ykf6NsMj"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, enc_hid_dim,n_layers, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        self.emb_dim = emb_dim\n",
    "        self.enc_hid_dim = enc_hid_dim\n",
    "        self.dropout = dropout\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        self.lstm = nn.LSTM(emb_dim, enc_hid_dim, n_layers, dropout=dropout, bidirectional=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, src):\n",
    "        \n",
    "        #src = [src len, batch size]\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "        \n",
    "        #embedded = [src len, batch size, emb dim]\n",
    "        \n",
    "        outputs, (hidden, cell) = self.lstm(embedded)\n",
    "       \n",
    "        # outputs are always from the top hidden layer, if bidirectional outputs are concatenated.\n",
    "        # outputs shape [sequence_length, batch_size, hidden_dim * num_directions]\n",
    "        # hidden is of shape [num_layers * num_directions, batch_size, hidden_size]\n",
    "        # cell is of shape [num_layers * num_directions, batch_size, hidden_size]\n",
    "        \n",
    "        return outputs, hidden, cell #<--------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r9ioVF4uL3Ot"
   },
   "source": [
    "### 1.4  You may need to revise `class Decoder`. Please show your code for `class Decoder`:\n",
    "rubric={accuracy:2, efficiency:2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ApQz7yqDObQw"
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, dec_hid_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        self.emb_dim = emb_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.dec_hid_dim = dec_hid_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        self.lstm = nn.LSTM(emb_dim, dec_hid_dim, n_layers, dropout=dropout)\n",
    "        self.fc_out = nn.Linear(dec_hid_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, input, hidden, cell):\n",
    "             \n",
    "        # input is of shape [batch_size]\n",
    "        # hidden is of shape [n_layer * num_directions, batch_size, hidden_size]\n",
    "        # cell is of shape [n_layer * num_directions, batch_size, hidden_size]\n",
    "        \n",
    "        input = input.unsqueeze(0)\n",
    "        \n",
    "        # input shape is [1, batch_size]. reshape is needed rnn expects a rank 3 tensors as input.\n",
    "        # so reshaping to [1, batch_size] means a batch of batch_size each containing 1 index.\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        \n",
    "        #embedded = [1, batch size, emb dim]    \n",
    "        output, (hidden, cell) = self.lstm(embedded, (hidden, cell))\n",
    "        \n",
    "        # output shape is [sequence_len, batch_size, hidden_dim * num_directions]\n",
    "        # hidden shape is [num_layers * num_directions, batch_size, hidden_dim]\n",
    "        # cell shape is [num_layers * num_directions, batch_size, hidden_dim]\n",
    "\n",
    "        # sequence_len and num_directions will always be 1 in the decoder.\n",
    "        # output shape is [1, batch_size, hidden_dim]\n",
    "        # hidden shape is [num_layers, batch_size, hidden_dim]\n",
    "        # cell shape is [num_layers, batch_size, hidden_dim]\n",
    "        \n",
    "        prediction = self.fc_out(hidden.squeeze(0)) # linear expects as rank 2 tensor as input\n",
    "        # predicted shape is [batch_size, output_dim]\n",
    "        \n",
    "        return prediction, hidden, cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J2tavMgYL3R8"
   },
   "source": [
    "### 1.5 You may need to revise `class Seq2Seq`. Please show your code for `class Seq2Seq`:\n",
    "rubric={accuracy:2, efficiency:2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g1nbZVgQOpbK"
   },
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    ''' This class contains the implementation of complete sequence to sequence network.\n",
    "    It uses to encoder to produce the context vectors.\n",
    "    It uses the decoder to produce the predicted target sentence.\n",
    "    Args:\n",
    "        encoder: A Encoder class instance.\n",
    "        decoder: A Decoder class instance.\n",
    "    '''\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
    "        # src is of shape [sequence_len, batch_size]\n",
    "        # trg is of shape [sequence_len, batch_size]\n",
    "        # if teacher_forcing_ratio is 0.5 we use ground-truth inputs 50% of time and 50% time we use decoder outputs.\n",
    "\n",
    "        batch_size = trg.shape[1]\n",
    "\n",
    "        max_len = trg.shape[0]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "\n",
    "        # to store the outputs of the decoder\n",
    "        outputs = torch.zeros(max_len, batch_size, trg_vocab_size).to(self.device)\n",
    "\n",
    "        # context vector, last hidden and cell state of encoder to initialize the decoder\n",
    "        encoder_outputs, _, cell = self.encoder(src)\n",
    "\n",
    "        # hidden shape is [num_layers * num_directions, batch_size, hidden_dim]\n",
    "        hidden = torch.mean(encoder_outputs, dim=0).unsqueeze(0) #<--------------\n",
    "\n",
    "        decoder_dim = hidden.shape[-2]\n",
    "\n",
    "        # cell is of shape [num_layers * num_directions, batch_size, hidden_size], our decoder only has 1 layer and 1 direction. \n",
    "        # But our Encoder has two directions. So we need concatenate two directional representations from Encoder. \n",
    "        cell = torch.cat((cell[0],cell[1]),dim=-1).unsqueeze(0) #<-------------- \n",
    "        \n",
    "        # first input to the decoder is the <sos> tokens\n",
    "        input = trg[0, :]\n",
    "\n",
    "        for t in range(1, max_len):\n",
    "            output, hidden, cell = self.decoder(input, hidden, cell)\n",
    "            outputs[t] = output\n",
    "            use_teacher_force = random.random() < teacher_forcing_ratio\n",
    "            top1 = output.max(1)[1]\n",
    "            input = (trg[t] if use_teacher_force else top1)\n",
    "\n",
    "        # outputs is of shape [sequence_len, batch_size, output_dim]\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "krRhOOJAOp0C"
   },
   "source": [
    "### 1.6 You may need to revise the code of instantiating classes. Please show your code for instantiation:\n",
    "rubric={accuracy:2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zZhtXYyxPkpA"
   },
   "outputs": [],
   "source": [
    "INPUT_DIM = len(SRC.vocab)\n",
    "OUTPUT_DIM = len(TRG.vocab)\n",
    "ENC_EMB_DIM = 256\n",
    "DEC_EMB_DIM = 256\n",
    "ENC_HID_DIM = 512\n",
    "DEC_HID_DIM = 1024 #<--------------\n",
    "ENC_DROPOUT = 0.5\n",
    "DEC_DROPOUT = 0.3\n",
    "N_LAYERS = 1\n",
    "LEARNING_RT = 0.001\n",
    "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, N_LAYERS, ENC_DROPOUT)\n",
    "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, DEC_HID_DIM, N_LAYERS, DEC_DROPOUT)\n",
    "\n",
    "model = Seq2Seq(enc, dec,device).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "peV1u1ecOp-D"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NABzJDRLgyKd"
   },
   "source": [
    "## Exercise 2: Seq2Seq with varient of attention mechanism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MOW88jqoSRby"
   },
   "source": [
    "Attention mechanisms boost the performance of deep learning models in machine translation and classification tasks, helping models select informative context to form the context representation. They also provide meaningful interpretation of the behavior of black-box deep learning models. In the tutorial of this week, we implemented the **concatenative/additive attention** which was initially proposed by [Bahdanau et al. (2015)](https://arxiv.org/pdf/1409.0473.pdf) to help memorize long source sentences in neural machine translation model. Additive attention computes the attention score in the following way, $e_{ij} = v_a tanh (W_a [s_{i-1};h_j;]) \\in \\mathcal{R}$ where $W_a \\in \\mathcal{R}^{h\\times 2h}$ and $v_a \\in \\mathcal{R}^{1\\times h}$.\n",
    "\n",
    "In the followup work, [Luong et al. (2015)](https://arxiv.org/pdf/1508.04025.pdf) examines two type of attention-based models (i.e., **global attetnion and local attention**) with effective classes of attention alignment functions (e.g., concat product, dot product, and general product). Generally, **a global approach** always attends to all source tokens, and **a local approach** only looks at a subset of source tokens at a time. [Luong et al. (2015)](https://arxiv.org/pdf/1508.04025.pdf) report the performances of models under different settings. The results show the model that is trained with **global attention with dot product alignment function** is better than MLP model that is trained with **global attention and concat product alignment function**. We briefly studied **global attention and concat product alignment function** as **Dot-Product/Multiplicative** attention in the tutorial.     \n",
    "\n",
    "In this exercise, you need to write code to implement **global attention with dot product alignment function** (**Dot-Product/Multiplicative**) from [Luong et al. (2015)](https://arxiv.org/pdf/1508.04025.pdf) to solve our translation task. \n",
    "\n",
    "Hint:\n",
    "- This exercise is more related to the tutorial of this week (i.e., week 3)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1up4vdjiSUIk"
   },
   "source": [
    "### 2.1 Warm Up\n",
    "rubric={accuracy:8}\n",
    "\n",
    "As a quick warm-up, take a minute to review **the code from the tutorial of this week** and identify the code to answer corresponding. You can copy and paste the code from the tutorial into the box below. You don't need to write any of your code in this section. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FFaDESeISWEW"
   },
   "source": [
    "Bahdanau et al., use **global attention** with one alignment function, the **concatenative/additive attention**. This will take in the previous hidden state of the decoder, $s_{t-1} \\in \\mathcal{R}^h$, and all of the stacked hidden states from the encoder, $H \\in \\mathcal{R}^{T\\times h}$. First, we calculate the *energy* between the previous decoder hidden state (i.e., $s_{t-1}$) and the encoder hidden states (i.e., $H$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OT4D9OCKSYFy"
   },
   "source": [
    "The first thing we do is `repeat` the previous decoder hidden state $T-1$ times to obtain a matrix, $S_{t-1} \\in \\mathcal{R}^{T \\times h}$.\n",
    "\n",
    "**Please paste the line(s) that perform this `repeat` operation:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sKRlu-CuTRjg"
   },
   "outputs": [],
   "source": [
    "#repeat encoder hidden state src_len-1 times\n",
    "hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qECOgojMSaTB"
   },
   "source": [
    "We then calculate the energy, $E_t$, between them by concatenating them together and passing them through a linear layer (`W_a`) and a $\\tanh$ activation function. \n",
    "\n",
    "$$E_t = \\tanh(W_a([S_{t-1}; H]^T))$$\n",
    "\n",
    "where $E_t$ is a **[decoder_hidden_dim, src_len]** tensor.\n",
    "\n",
    "**Please paste the line(s) that calculate $E_t$:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dlRxRUbxTWY0"
   },
   "outputs": [],
   "source": [
    "# attention scoring function - part 1 - tanh(W_a[s;h])\n",
    "energy = torch.tanh(self.W_a(torch.cat((hidden, encoder_outputs), dim = 2))) \n",
    "\n",
    "#energy = [batch size, src len, dec hid dim]\n",
    "\n",
    "energy = energy.permute(0, 2, 1)\n",
    "\n",
    "#energy = [batch size, dec hid dim, src len]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XKDZVpqHT6IR"
   },
   "source": [
    " Then, we want convert $E_t$ to be a vector of **[src_len]** size for **each example** in the batch as the attention should be over the length of the source sentence. This is achieved by multiplying the `energy` by a **[1, decoder_hidden_dim]** tensor, $v_a$.\n",
    "\n",
    "$$\\hat{a}_t = v_a E_t$$\n",
    "\n",
    "**Please paste the line(s) that calculate $\\hat{a}_t$ in a batch:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6rWmdLSpUKQq"
   },
   "outputs": [],
   "source": [
    "#v = [dec hid dim]\n",
    "\n",
    "v = self.v_a.repeat(batch_size, 1).unsqueeze(1)\n",
    "\n",
    "#v = [batch size, 1, dec hid dim]\n",
    "\n",
    "# attention scoring function - part 2 - v_a(tanh(W_a[s;h]))\n",
    "attention = torch.bmm(v, energy).squeeze(1)\n",
    "\n",
    "#attention= [batch size, src len]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "12t31SkeULvm"
   },
   "source": [
    "Finally, we ensure the attention vector fits the constraints of having all elements between 0 and 1 and the vector summing to 1 by passing it through a $\\text{softmax}$ layer.\n",
    "\n",
    "$$a_t = \\text{softmax}(\\hat{a_t})$$\n",
    "\n",
    "**Please paste the line(s) that calculate $a_t$ and return it to Decoder:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dcMDl8qgUXjR"
   },
   "outputs": [],
   "source": [
    "        # attention scoring function - part 2 - softmax(v_a(tanh(W_a[s;h])))\n",
    "        return F.softmax(attention, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MLidk4BdU0-j"
   },
   "source": [
    "### 2.2 Write code to inplement global attention with dot product/multiplicative attention function\n",
    "rubric={mechanics:1}\n",
    "\n",
    "Intead of using the **additive attention** to get attenntion score $a_t$:\n",
    "$$\\alpha_i = softmax(v_a \\tanh(W_a[S_{i-1}; H]^T))$$\n",
    "\n",
    "**We will use the *global attention with dot product alignment function* (Dot-Product/Multiplicative based attention function, that is, $\\alpha_i = softmax([s_{i}^T h_1,\\cdots,s_{i}^T h_t])$ to calculate attention score $\\alpha_i$ and follow the subsequent steps to generate translation token $\\hat{y_{i+1}}$**: \n",
    "1. initialize the `outputs` tensor is created to hold all predictions, $\\hat{Y} = \\{\\hat{y_1} ... \\hat{y_t}\\}$ where t is the maximal length of target language;\n",
    "2. the source sequence, $X = \\{x_1,..., x_t\\}$, is fed into the encoder to receive last hidden state, $h_t$, and last cell state $c^{Encoder}_t$;\n",
    "3. the initial decoder hidden state is set to the $h_t$, and the initial decoder cell state is set to the $c_t$. (i.e., $s_0$ = $h_t$; $c^{Decoder}_0$ = $c^{Encoder}_t$);\n",
    "4. we use a batch of `<sos>` tokens as the first `input` (i.e., $y_1$);\n",
    "5. we then decode within a loop:\n",
    "\n",
    " for i in range(1,t): #t is the maximal length of target language\n",
    "    1. inserting the input token $y_{i}$, previous hidden state, $s_{i-1}$, and previous cell state, $c^{Decoder}_{i-1}$, into the Decoder, we get new hidden and cell states, $s_{i}$ and $c^{Decoder}_{i}$;\n",
    "    2. use **`attention_function()`** to calculate the attention vector, $\\alpha_i$, based on $h_1,\\dots,h_t$ (all encoder hidden states stacked up is $H$) and $s_{i}$;\n",
    "    3. use this attention vector to create a weighted context vector, $c_i$, denoted by `weighted`, which is a weighted sum of the encoder hidden states, $H$, using $\\alpha_i$ as the weights (i.e., $c_i = \\alpha_i^T H$);\n",
    "    4. concatenate the current hidden state $s_i$ with weighted context vector $c_i$, then pass this concatanation through a linear layer, $f_{mid}$, to get a new hidden state $s'_{i}$ that shape is `[batch, decoder hidden dimension]`;\n",
    "    5. pass $s'_{i}$ through the linear layer, $f_{output}$, to make a prediction of the next word in the target sentence, $\\hat{y}_{i+1}$.\n",
    "    6. decide if use **teacher forcing** or not, setting the next input as appropriate.\n",
    "\n",
    "\n",
    "where **`attention_function()`** is based on **dot product attention**:\n",
    "$\\alpha_i = softmax([s_{i}^T h_1,\\cdots,s_{i}^T h_t])$\n",
    "\n",
    "\n",
    "**The pseudo code for computing attention vector:**\n",
    "\n",
    "```\n",
    "class Attention(nn.Module):\n",
    "        INPUT: \n",
    "        current Decoder hidden state (s_{i}), decoder_output: [batch size, dec hid dim]\n",
    "        all hidden state of last layer of Encoder (H), encoder_outputs: [src len, batch size, enc hid dim]\n",
    "        OUTPUT: \n",
    "        attention_vector (\\alpha_i), attention_vector: [batch size, src len]\n",
    "        ------------------------------------------------------------------------------------------\n",
    "        # For-loop version: \n",
    "        attention_vector = Variable(torch.zeros(batch_size, encoder_src_len))\n",
    "\n",
    "        # For every batch, every time step of encoder's hidden state, calculate attention score.\n",
    "        for b in range(batch_size):\n",
    "            for t in range(max_src_len):\n",
    "                # Luong et al. (2015) equation(8) -- dot form content-based attention:\n",
    "                attention_vector[b,t] = decoder_output[b] **dot product** encoder_outputs[t,b]\n",
    "                \n",
    "        ------------------------------------------------------------------------------------------\n",
    "        # Vectorized version:\n",
    "\n",
    "        1. attention_vector =  \n",
    "           (batch_size, seq_len=1, hidden_size) **batch matrix-matrix product** (batch_size, hidden_size, max_src_len) = (batch_size, seq_len=1, max_src_len)\n",
    "           \n",
    "         return attention_vector\n",
    "```\n",
    "\n",
    "Equation 8 from Luong's paper is:\n",
    "<img src=\"attention_images/luong_eqn8.png\\\" title=\"equation 8 from Luong paper\" height=\"550\" width=\"450\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tishmPfQU-70"
   },
   "source": [
    "We should built a new train and evaluate funtions for training attention model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Zqw8ImpC73Zt"
   },
   "outputs": [],
   "source": [
    "def train_attn(model, iterator, optimizer, criterion, clip):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for i, batch in enumerate(iterator):\n",
    "        \n",
    "        src = batch.SRC\n",
    "        trg = batch.TRG\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output,_ = model(src, trg)\n",
    "        \n",
    "        #trg = [trg len, batch size]\n",
    "        #output = [trg len, batch size, output dim]\n",
    "        \n",
    "        output_dim = output.shape[-1]\n",
    "        \n",
    "        output = output[1:].view(-1, output_dim)\n",
    "        trg = trg[1:].view(-1)\n",
    "        \n",
    "        #trg = [(trg len - 1) * batch size]\n",
    "        #output = [(trg len - 1) * batch size, output dim]\n",
    "        \n",
    "        loss = criterion(output, trg)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "    bleu = inference(model, \"./eng-fre/val_eng_fre.tsv\", SRC, TRG, True, 64)    \n",
    "    return epoch_loss / len(iterator), bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KFVQWDFw8ALw"
   },
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_attn(model, iterator, criterion):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for i, batch in enumerate(iterator):\n",
    "\n",
    "            src = batch.SRC\n",
    "            trg = batch.TRG\n",
    "\n",
    "            output,_ = model(src, trg, 0) #turn off teacher forcing\n",
    "\n",
    "            #trg = [trg len, batch size]\n",
    "            #output = [trg len, batch size, output dim]\n",
    "\n",
    "            output_dim = output.shape[-1]\n",
    "            \n",
    "            output = output[1:].view(-1, output_dim)\n",
    "            trg = trg[1:].view(-1)\n",
    "\n",
    "            #trg = [(trg len - 1) * batch size]\n",
    "            #output = [(trg len - 1) * batch size, output dim]\n",
    "\n",
    "            loss = criterion(output, trg)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "    bleu = inference(model, \"./eng-fre/val_eng_fre.tsv\", SRC, TRG, True, 64)    \n",
    "    return epoch_loss / len(iterator), bleu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fGgxyBiMxgtF"
   },
   "source": [
    "## Start your works here.\n",
    "\n",
    "Full code for training dot-product model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9eMCWBLAxirl"
   },
   "source": [
    "`class Encoder(nn.Module):`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FNKOnMyDhVl0"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, enc_hid_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        self.emb_dim = emb_dim\n",
    "        self.enc_hid_dim = enc_hid_dim\n",
    "        self.dropout = dropout\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        self.lstm = nn.LSTM(emb_dim, enc_hid_dim, n_layers, dropout=dropout,bidirectional=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, src):\n",
    "        \n",
    "        #src = [src len, batch size]\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "        \n",
    "        #embedded = [src len, batch size, emb dim]\n",
    "        \n",
    "        outputs, (hidden, cell) = self.lstm(embedded)\n",
    "       \n",
    "        # outputs are always from the top hidden layer, if bidirectional outputs are concatenated.\n",
    "        # outputs shape [sequence_length, batch_size, hidden_dim * num_directions]\n",
    "        # hidden is of shape [num_layers * num_directions, batch_size, hidden_size]\n",
    "        # cell is of shape [num_layers * num_directions, batch_size, hidden_size]\n",
    "        \n",
    "        return outputs, (hidden, cell)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fsG5xsC8xkzH"
   },
   "source": [
    "`class Attention(nn.Module):`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dViMl_gzirjy"
   },
   "outputs": [],
   "source": [
    "class Attention(nn.Module): #<--------------\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # you don't need to initial any parameters ib dot-product attention. \n",
    "        \n",
    "    def forward(self, hidden, encoder_outputs): #<--------------\n",
    "        #hidden = [batch size, dec hid dim]\n",
    "        #encoder_outputs = [src len, batch size, enc hid dim]\n",
    "        attention = torch.bmm(hidden.unsqueeze(1), encoder_outputs.permute(1,2,0)).squeeze(1)#<--------------\n",
    "        # attention = (batch_size, src_len)\n",
    "        return F.softmax(attention, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oJaXG0CJxnsn"
   },
   "source": [
    "`class Decoder(nn.Module):`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M7Wu-qlbiySd"
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, enc_hid_dim, dec_hid_dim, n_layers, dropout, attention):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.output_dim = output_dim\n",
    "        self.attention = attention\n",
    "        \n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        \n",
    "        self.rnn = nn.LSTM(emb_dim, dec_hid_dim, n_layers, dropout=dropout)\n",
    "        \n",
    "        self.fc_md = nn.Linear(dec_hid_dim*2, dec_hid_dim) #<--------------\n",
    "\n",
    "        self.fc_out = nn.Linear(dec_hid_dim, output_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, input, hidden, cell, encoder_outputs):\n",
    "             \n",
    "        #input = [batch size]\n",
    "        #hidden = [batch size, dec hid dim]\n",
    "        #encoder_outputs = [src len, batch size, enc hid dim]\n",
    "        input = input.unsqueeze(0)\n",
    "        \n",
    "        #input = [1, batch size]\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        #embedded = [1, batch size, emb dim]\n",
    "\n",
    "        output, (hidden, cell) = self.rnn(embedded, (hidden.unsqueeze(0),cell)) #<--------------\n",
    "\n",
    "        #output = [seq len, batch size, dec hid dim * n directions]\n",
    "        #hidden = [n layers * n directions, batch size, dec hid dim]\n",
    "        \n",
    "        #seq len, n layers and n directions will always be 1 in this decoder, therefore:\n",
    "        #output = [1, batch size, dec hid dim]\n",
    "        #hidden = [1, batch size, dec hid dim]\n",
    "        #this also means that output == hidden\n",
    "\n",
    "        hidden = hidden.squeeze(0)\n",
    "        attention_weights = self.attention(hidden, encoder_outputs)#<--------------\n",
    "        \n",
    "        #attention_weights = [batch size, src len]\n",
    "        \n",
    "        attention_weights = attention_weights.unsqueeze(1) #<--------------\n",
    "\n",
    "        #a = [batch size, 1, src len]\n",
    "        \n",
    "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
    "        \n",
    "        #encoder_outputs = [batch size, src len, enc hid dim]\n",
    "        \n",
    "        weighted = torch.bmm(attention_weights, encoder_outputs)\n",
    "        weighted = weighted.squeeze(1)\n",
    "        #weighted = [batch size, 1, enc hid dim]\n",
    "        \n",
    "        concate_output = torch.cat((hidden, weighted), dim = -1) #<--------------\n",
    "\n",
    "        concate_output = self.fc_md(concate_output) #<--------------\n",
    "\n",
    "        prediction = self.fc_out(concate_output)\n",
    "        \n",
    "        #prediction = [batch size, output dim]\n",
    "        \n",
    "        return prediction, hidden, cell, attention_weights "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dc2aCp6FxqEr"
   },
   "source": [
    "`class Seq2Seq(nn.Module):`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UTNnnYOIi0Kh"
   },
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        \n",
    "    def forward(self, src, trg, teacher_forcing_ratio = 0.5):\n",
    "        \n",
    "        #src = [src len, batch size]\n",
    "        #trg = [trg len, batch size]\n",
    "        #teacher_forcing_ratio is probability to use teacher forcing\n",
    "        #e.g. if teacher_forcing_ratio is 0.75 we use teacher forcing 75% of the time\n",
    "        batch_size = src.shape[1]\n",
    "        trg_len = trg.shape[0]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "        \n",
    "        #tensor to store decoder outputs\n",
    "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
    "        # save the encoder-decoder attention weights\n",
    "        # all_attention_weights = [batch_size, trg len-1, src len ]\n",
    "        all_attention_weights = torch.zeros(trg.shape[1], trg.shape[0]-1, src.shape[0])\n",
    "        #encoder_outputs is all hidden states of the input sequence, back and forwards\n",
    "        #hidden is the final forward and backward hidden states, passed through a linear layer\n",
    "        encoder_outputs, (hidden, cell) = self.encoder(src) \n",
    "\n",
    "        hidden = torch.cat((hidden[-2,:,:],hidden[-1,:,:]),dim=-1)\n",
    "        cell = torch.cat((cell[-2,:,:],cell[-1,:,:]),dim=-1).unsqueeze(0)\n",
    "        #first input to the decoder is the <sos> tokens\n",
    "        input = trg[0,:]\n",
    "        \n",
    "        for t in range(1, trg_len):\n",
    "            \n",
    "            #insert input token embedding, previous hidden state and all encoder hidden states\n",
    "            #receive output tensor (predictions) and new hidden state\n",
    "            output, hidden, cell, attention_weights = self.decoder(input, hidden, cell, encoder_outputs)\n",
    "            \n",
    "            # all_attention_weights[t-1] = [src len, batch size]\n",
    "            all_attention_weights[:,t-1,:] = attention_weights.squeeze(1)\n",
    "            \n",
    "            #place predictions in a tensor holding predictions for each token\n",
    "            outputs[t] = output\n",
    "            \n",
    "            #decide if we are going to use teacher forcing or not\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            \n",
    "            #get the highest predicted token from our predictions\n",
    "            top1 = output.argmax(1) \n",
    "            \n",
    "            #if teacher forcing, use actual next token as next input\n",
    "            #if not, use predicted token\n",
    "            input = trg[t] if teacher_force else top1\n",
    "\n",
    "        return outputs,all_attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9rTNUpY_yhyA"
   },
   "source": [
    "## Instantiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "XrjqFOb-j-yg",
    "outputId": "c1718cd6-ca24-4b1b-829a-c9ebee7e8ccb"
   },
   "outputs": [],
   "source": [
    "INPUT_DIM = len(SRC.vocab)\n",
    "OUTPUT_DIM = len(TRG.vocab)\n",
    "ENC_EMB_DIM = 256\n",
    "DEC_EMB_DIM = 256\n",
    "ENC_HID_DIM = 512\n",
    "DEC_HID_DIM = 1024 #<--------------\n",
    "ENC_DROPOUT = 0.5\n",
    "DEC_DROPOUT = 0.5\n",
    "N_LAYERS = 1\n",
    "LEARNING_RT = 0.001\n",
    "\n",
    "attn = Attention()\n",
    "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, N_LAYERS, ENC_DROPOUT)\n",
    "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, N_LAYERS, DEC_DROPOUT, attn)\n",
    "\n",
    "model = Seq2Seq(enc, dec, device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9A3sxRvq-1wf"
   },
   "outputs": [],
   "source": [
    "model.apply(init_weights)\n",
    "optimizer = optim.Adam(model.parameters(), lr = LEARNING_RT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ABKf5qEd-9Lq",
    "outputId": "5d4cd7b0-06d7-4076-a8f3-5ffb92fa6565"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pad> token index:  1\n"
     ]
    }
   ],
   "source": [
    "TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]\n",
    "print('<pad> token index: ',TRG_PAD_IDX)\n",
    "## we will ignor the pad token in true target set\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eTyJ-IEeyuUC"
   },
   "source": [
    "## Prototype Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"./ckpt_ex2/TRG.Field\",\"wb\")as f:\n",
    "     pickle.dump(TRG,f)\n",
    "\n",
    "with open(\"./ckpt_ex2/SRC.Field\",\"wb\")as f:\n",
    "     pickle.dump(SRC,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "PA83WKyhkZ8P",
    "outputId": "68332de6-2fa1-4eea-8ed1-b7ac358752cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5438, device='cuda:0', grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chiyu94/py3.6/lib/python3.6/site-packages/nltk/translate/bleu_score.py:523: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/home/chiyu94/py3.6/lib/python3.6/site-packages/nltk/translate/bleu_score.py:523: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/home/chiyu94/py3.6/lib/python3.6/site-packages/nltk/translate/bleu_score.py:523: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "clip = 1\n",
    "model.train()\n",
    "\n",
    "for i, batch in enumerate(train_iter):\n",
    "\n",
    "    src = batch.SRC\n",
    "    trg = batch.TRG\n",
    "    optimizer.zero_grad()\n",
    "    output,_ = model(src, trg)\n",
    "    #trg = [trg len, batch size]\n",
    "    #output = [trg len, batch size, output dim]\n",
    "\n",
    "    output_dim = output.shape[-1]\n",
    "\n",
    "    output = output[1:].view(-1, output_dim)\n",
    "    trg = trg[1:].view(-1)\n",
    "\n",
    "    #trg = [(trg len - 1) * batch size]\n",
    "    #output = [(trg len - 1) * batch size, output dim]\n",
    "\n",
    "    loss = criterion(output, trg)\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "    print(loss/src.shape[1])\n",
    "    bleu = inference(model, \"./eng-fre/val_eng_fre.tsv\", SRC, TRG, True, 64)    \n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Sk9tm0Acykim"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "IxnbE2mxkdDp",
    "outputId": "53d8e3fa-9aab-4b0e-e4d2-b8abb4119831"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Time: 1m 19s\n",
      "\tTrain Loss: 3.874 | Train PPL:  48.134\n",
      "\t Val. Loss: 3.487 |  Val. PPL:  32.686\n",
      "\t Train BLEU:0.254 |  Val. BLEU:   0.254\n",
      "Epoch: 02 | Time: 1m 18s\n",
      "\tTrain Loss: 2.358 | Train PPL:  10.569\n",
      "\t Val. Loss: 3.070 |  Val. PPL:  21.542\n",
      "\t Train BLEU:0.399 |  Val. BLEU:   0.399\n",
      "Epoch: 03 | Time: 1m 18s\n",
      "\tTrain Loss: 1.876 | Train PPL:   6.524\n",
      "\t Val. Loss: 2.994 |  Val. PPL:  19.970\n",
      "\t Train BLEU:0.445 |  Val. BLEU:   0.445\n",
      "Epoch: 04 | Time: 1m 18s\n",
      "\tTrain Loss: 1.624 | Train PPL:   5.072\n",
      "\t Val. Loss: 2.919 |  Val. PPL:  18.531\n",
      "\t Train BLEU:0.461 |  Val. BLEU:   0.461\n",
      "Epoch: 05 | Time: 1m 18s\n",
      "\tTrain Loss: 1.464 | Train PPL:   4.321\n",
      "\t Val. Loss: 2.919 |  Val. PPL:  18.524\n",
      "\t Train BLEU:0.467 |  Val. BLEU:   0.467\n",
      "Epoch: 06 | Time: 1m 18s\n",
      "\tTrain Loss: 1.337 | Train PPL:   3.807\n",
      "\t Val. Loss: 2.886 |  Val. PPL:  17.917\n",
      "\t Train BLEU:0.470 |  Val. BLEU:   0.470\n",
      "Epoch: 07 | Time: 1m 18s\n",
      "\tTrain Loss: 1.231 | Train PPL:   3.423\n",
      "\t Val. Loss: 3.017 |  Val. PPL:  20.428\n",
      "\t Train BLEU:0.466 |  Val. BLEU:   0.466\n",
      "Epoch: 08 | Time: 1m 18s\n",
      "\tTrain Loss: 1.149 | Train PPL:   3.155\n",
      "\t Val. Loss: 2.970 |  Val. PPL:  19.493\n",
      "\t Train BLEU:0.480 |  Val. BLEU:   0.480\n",
      "Epoch: 09 | Time: 1m 18s\n",
      "\tTrain Loss: 1.068 | Train PPL:   2.911\n",
      "\t Val. Loss: 3.208 |  Val. PPL:  24.733\n",
      "\t Train BLEU:0.472 |  Val. BLEU:   0.472\n",
      "Epoch: 10 | Time: 1m 18s\n",
      "\tTrain Loss: 0.994 | Train PPL:   2.702\n",
      "\t Val. Loss: 3.210 |  Val. PPL:  24.791\n",
      "\t Train BLEU:0.475 |  Val. BLEU:   0.475\n",
      "Epoch: 11 | Time: 1m 18s\n",
      "\tTrain Loss: 0.932 | Train PPL:   2.539\n",
      "\t Val. Loss: 3.236 |  Val. PPL:  25.429\n",
      "\t Train BLEU:0.472 |  Val. BLEU:   0.472\n",
      "Epoch: 12 | Time: 1m 18s\n",
      "\tTrain Loss: 0.873 | Train PPL:   2.394\n",
      "\t Val. Loss: 3.410 |  Val. PPL:  30.262\n",
      "\t Train BLEU:0.461 |  Val. BLEU:   0.461\n",
      "Epoch: 13 | Time: 1m 18s\n",
      "\tTrain Loss: 0.823 | Train PPL:   2.278\n",
      "\t Val. Loss: 3.615 |  Val. PPL:  37.149\n",
      "\t Train BLEU:0.463 |  Val. BLEU:   0.463\n",
      "Epoch: 14 | Time: 1m 18s\n",
      "\tTrain Loss: 0.766 | Train PPL:   2.151\n",
      "\t Val. Loss: 3.452 |  Val. PPL:  31.552\n",
      "\t Train BLEU:0.466 |  Val. BLEU:   0.466\n",
      "Epoch: 15 | Time: 1m 18s\n",
      "\tTrain Loss: 0.731 | Train PPL:   2.076\n",
      "\t Val. Loss: 3.335 |  Val. PPL:  28.077\n",
      "\t Train BLEU:0.460 |  Val. BLEU:   0.460\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 15\n",
    "CLIP = 1\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, bleu_train = train_attn(model, train_iter, optimizer, criterion, CLIP)\n",
    "    valid_loss, bleu_val = evaluate_attn(model, val_iter, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    # Create checkpoint at end of each epoch\n",
    "    state_dict_model = model.state_dict() \n",
    "    state = {\n",
    "        'epoch': epoch,\n",
    "        'state_dict': state_dict_model,\n",
    "        'optimizer': optimizer.state_dict()\n",
    "        }\n",
    "\n",
    "    torch.save(state, \"./ckpt_ex2/seq2seq_\"+str(epoch+1)+\".pt\")\n",
    "\n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n",
    "    print(f'\\t Train BLEU:{bleu_train:.3f} |  Val. BLEU: {bleu_val:7.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./ckpt_ex2/TRG.Field\",\"rb\") as f:\n",
    "     TRG_ex2_aved = pickle.load(f)\n",
    "\n",
    "with open(\"./ckpt_ex2/SRC.Field\",\"rb\") as f:\n",
    "     SRC_ex2_saved = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "pJTSQHWi7PKn",
    "outputId": "575bf6ae-4a74-44be-a04f-482f75631344"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chiyu94/py3.6/lib/python3.6/site-packages/torch/nn/modules/rnn.py:51: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    }
   ],
   "source": [
    "attn = Attention()\n",
    "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, N_LAYERS, ENC_DROPOUT)\n",
    "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, N_LAYERS, DEC_DROPOUT, attn)\n",
    "\n",
    "model_ex2_best = Seq2Seq(enc, dec, device).to(device)\n",
    "\n",
    "model_ex2_best.load_state_dict(torch.load('./ckpt_ex2/seq2seq_8.pt')['state_dict'])\n",
    "model_ex2_best = model_ex2_best.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "e3dXIVk7Ggpt",
    "outputId": "77d74aad-6e78-450e-c8e3-8c0c55533733"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4768248363431581"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inference(model_ex2_best, \"./eng-fre/test_eng_fre.tsv\", SRC_ex2_saved, TRG_ex2_aved, True, 64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "asht92xQDqzE"
   },
   "source": [
    "### 2.2.1 Please paste your fully training log here. Which epoch is the best?\n",
    "rubric={accuracy:2}\n",
    "You log should look like this:\n",
    "```\n",
    "Epoch: 01 | Time: 1m 25s\n",
    "\tTrain Loss: 4.293 | Train PPL:  73.188\n",
    "\t Val. Loss: 4.263 |  Val. PPL:  71.012 \n",
    "   ............\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EMVxlZP1DrmF"
   },
   "source": [
    "**You answer:**\n",
    "**My best model is trained with 8 epochs.** (Your best model may get at different epochs. The best model should be the model which obtained best BLEU score on validation set.) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cM63lBfREGJ6"
   },
   "source": [
    "```\n",
    "Epoch: 01 | Time: 1m 19s\n",
    "\tTrain Loss: 3.874 | Train PPL:  48.134\n",
    "\t Val. Loss: 3.487 |  Val. PPL:  32.686\n",
    "\t Train BLEU:0.254 |  Val. BLEU:   0.254\n",
    "Epoch: 02 | Time: 1m 18s\n",
    "\tTrain Loss: 2.358 | Train PPL:  10.569\n",
    "\t Val. Loss: 3.070 |  Val. PPL:  21.542\n",
    "\t Train BLEU:0.399 |  Val. BLEU:   0.399\n",
    "Epoch: 03 | Time: 1m 18s\n",
    "\tTrain Loss: 1.876 | Train PPL:   6.524\n",
    "\t Val. Loss: 2.994 |  Val. PPL:  19.970\n",
    "\t Train BLEU:0.445 |  Val. BLEU:   0.445\n",
    "Epoch: 04 | Time: 1m 18s\n",
    "\tTrain Loss: 1.624 | Train PPL:   5.072\n",
    "\t Val. Loss: 2.919 |  Val. PPL:  18.531\n",
    "\t Train BLEU:0.461 |  Val. BLEU:   0.461\n",
    "Epoch: 05 | Time: 1m 18s\n",
    "\tTrain Loss: 1.464 | Train PPL:   4.321\n",
    "\t Val. Loss: 2.919 |  Val. PPL:  18.524\n",
    "\t Train BLEU:0.467 |  Val. BLEU:   0.467\n",
    "Epoch: 06 | Time: 1m 18s\n",
    "\tTrain Loss: 1.337 | Train PPL:   3.807\n",
    "\t Val. Loss: 2.886 |  Val. PPL:  17.917\n",
    "\t Train BLEU:0.470 |  Val. BLEU:   0.470\n",
    "Epoch: 07 | Time: 1m 18s\n",
    "\tTrain Loss: 1.231 | Train PPL:   3.423\n",
    "\t Val. Loss: 3.017 |  Val. PPL:  20.428\n",
    "\t Train BLEU:0.466 |  Val. BLEU:   0.466\n",
    "Epoch: 08 | Time: 1m 18s\n",
    "\tTrain Loss: 1.149 | Train PPL:   3.155\n",
    "\t Val. Loss: 2.970 |  Val. PPL:  19.493\n",
    "\t Train BLEU:0.480 |  Val. BLEU:   0.480\n",
    "Epoch: 09 | Time: 1m 18s\n",
    "\tTrain Loss: 1.068 | Train PPL:   2.911\n",
    "\t Val. Loss: 3.208 |  Val. PPL:  24.733\n",
    "\t Train BLEU:0.472 |  Val. BLEU:   0.472\n",
    "Epoch: 10 | Time: 1m 18s\n",
    "\tTrain Loss: 0.994 | Train PPL:   2.702\n",
    "\t Val. Loss: 3.210 |  Val. PPL:  24.791\n",
    "\t Train BLEU:0.475 |  Val. BLEU:   0.475\n",
    "Epoch: 11 | Time: 1m 18s\n",
    "\tTrain Loss: 0.932 | Train PPL:   2.539\n",
    "\t Val. Loss: 3.236 |  Val. PPL:  25.429\n",
    "\t Train BLEU:0.472 |  Val. BLEU:   0.472\n",
    "Epoch: 12 | Time: 1m 18s\n",
    "\tTrain Loss: 0.873 | Train PPL:   2.394\n",
    "\t Val. Loss: 3.410 |  Val. PPL:  30.262\n",
    "\t Train BLEU:0.461 |  Val. BLEU:   0.461\n",
    "Epoch: 13 | Time: 1m 18s\n",
    "\tTrain Loss: 0.823 | Train PPL:   2.278\n",
    "\t Val. Loss: 3.615 |  Val. PPL:  37.149\n",
    "\t Train BLEU:0.463 |  Val. BLEU:   0.463\n",
    "Epoch: 14 | Time: 1m 18s\n",
    "\tTrain Loss: 0.766 | Train PPL:   2.151\n",
    "\t Val. Loss: 3.452 |  Val. PPL:  31.552\n",
    "\t Train BLEU:0.466 |  Val. BLEU:   0.466\n",
    "Epoch: 15 | Time: 1m 18s\n",
    "\tTrain Loss: 0.731 | Train PPL:   2.076\n",
    "\t Val. Loss: 3.335 |  Val. PPL:  28.077\n",
    "\t Train BLEU:0.460 |  Val. BLEU:   0.460\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WweY1i5ZEMLa"
   },
   "source": [
    "### 2.2.2 Please report the cumulative BLEU-4 score on test set (i.e., `test_eng_fre.tsv`) via the corpus_bleu() function.\n",
    "rubric={accuracy:2}\n",
    "\n",
    "Hint: You can use `inference()` function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tgjT_zd_EPVg"
   },
   "source": [
    "**Your answer goes here**\n",
    "\n",
    "**My best model obtains 0.48 cumulative BLEU-4 score.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "juP9oWrNEY2i"
   },
   "source": [
    "### Please paste your code for the corresponding questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C9QnCRvoEa1C"
   },
   "source": [
    "### 2.2.3 You may need to revise `class Encoder`. Pleae show your code for `class Encoder`:\n",
    "rubric={accuracy:2, efficiency:2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NwtPVrRmEbGz"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, enc_hid_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        self.emb_dim = emb_dim\n",
    "        self.enc_hid_dim = enc_hid_dim\n",
    "        self.dropout = dropout\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        self.lstm = nn.LSTM(emb_dim, enc_hid_dim, n_layers, dropout=dropout,bidirectional=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, src):\n",
    "        \n",
    "        #src = [src len, batch size]\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "        \n",
    "        #embedded = [src len, batch size, emb dim]\n",
    "        \n",
    "        outputs, (hidden, cell) = self.lstm(embedded)\n",
    "       \n",
    "        # outputs are always from the top hidden layer, if bidirectional outputs are concatenated.\n",
    "        # outputs shape [sequence_length, batch_size, hidden_dim * num_directions]\n",
    "        # hidden is of shape [num_layers * num_directions, batch_size, hidden_size]\n",
    "        # cell is of shape [num_layers * num_directions, batch_size, hidden_size]\n",
    "        \n",
    "        return outputs, (hidden, cell)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2S2NxWzzEhpe"
   },
   "source": [
    "### 2.2.4  You may need to revise `class Decoder`. Pleae show your code for `class Decoder`:\n",
    "rubric={accuracy:2, efficiency:2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ps6h3TxZEjU9"
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, enc_hid_dim, dec_hid_dim, n_layers, dropout, attention):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.output_dim = output_dim\n",
    "        self.attention = attention\n",
    "        \n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        \n",
    "        self.rnn = nn.LSTM(emb_dim, dec_hid_dim, n_layers, dropout=dropout)\n",
    "        \n",
    "        self.fc_md = nn.Linear(dec_hid_dim*2, dec_hid_dim) #<--------------\n",
    "\n",
    "        self.fc_out = nn.Linear(dec_hid_dim, output_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, input, hidden, cell, encoder_outputs):\n",
    "             \n",
    "        #input = [batch size]\n",
    "        #hidden = [batch size, dec hid dim]\n",
    "        #encoder_outputs = [src len, batch size, enc hid dim]\n",
    "        input = input.unsqueeze(0)\n",
    "        \n",
    "        #input = [1, batch size]\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        #embedded = [1, batch size, emb dim]\n",
    "\n",
    "        output, (hidden, cell) = self.rnn(embedded, (hidden.unsqueeze(0),cell)) #<--------------\n",
    "\n",
    "        #output = [seq len, batch size, dec hid dim * n directions]\n",
    "        #hidden = [n layers * n directions, batch size, dec hid dim]\n",
    "        \n",
    "        #seq len, n layers and n directions will always be 1 in this decoder, therefore:\n",
    "        #output = [1, batch size, dec hid dim]\n",
    "        #hidden = [1, batch size, dec hid dim]\n",
    "        #this also means that output == hidden\n",
    "\n",
    "        hidden = hidden.squeeze(0)\n",
    "        attention_weights = self.attention(hidden, encoder_outputs)#<--------------\n",
    "        \n",
    "        #attention_weights = [batch size, src len]\n",
    "        \n",
    "        attention_weights = attention_weights.unsqueeze(1) #<--------------\n",
    "\n",
    "        #a = [batch size, 1, src len]\n",
    "        \n",
    "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
    "        \n",
    "        #encoder_outputs = [batch size, src len, enc hid dim]\n",
    "        \n",
    "        weighted = torch.bmm(attention_weights, encoder_outputs)\n",
    "        weighted = weighted.squeeze(1)\n",
    "        #weighted = [batch size, 1, enc hid dim]\n",
    "        \n",
    "        concate_output = torch.cat((hidden, weighted), dim = -1) #<--------------\n",
    "\n",
    "        concate_output = self.fc_md(concate_output) #<--------------\n",
    "\n",
    "        prediction = self.fc_out(concate_output)\n",
    "        \n",
    "        #prediction = [batch size, output dim]\n",
    "        \n",
    "        return prediction, hidden, cell, attention_weights "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FpmsNlKZI18U"
   },
   "source": [
    "### 2.2.4a  Give your code for the `class Attention`:\n",
    "rubric={accuracy:4, efficiency:2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c2xoEasBI9fL"
   },
   "outputs": [],
   "source": [
    "class Attention(nn.Module): #<--------------\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # you don't need to initial any parameters ib dot-product attention. \n",
    "        \n",
    "    def forward(self, hidden, encoder_outputs): #<--------------\n",
    "        #hidden = [batch size, dec hid dim]\n",
    "        #encoder_outputs = [src len, batch size, enc hid dim]\n",
    "        attention = torch.bmm(hidden.unsqueeze(1), encoder_outputs.permute(1,2,0)).squeeze(1)#<--------------\n",
    "        # attention = (batch_size, src_len)\n",
    "        return F.softmax(attention, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6Pb3ZfDwI8Mi"
   },
   "source": [
    "### 2.2.5 You may need to revise `class Seq2Seq`. Pleae show your code for `class Seq2Seq`:\n",
    "rubric={accuracy:2, efficiency:2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SkYV7dKFJDXe"
   },
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        \n",
    "    def forward(self, src, trg, teacher_forcing_ratio = 0.5):\n",
    "        \n",
    "        #src = [src len, batch size]\n",
    "        #trg = [trg len, batch size]\n",
    "        #teacher_forcing_ratio is probability to use teacher forcing\n",
    "        #e.g. if teacher_forcing_ratio is 0.75 we use teacher forcing 75% of the time\n",
    "        batch_size = src.shape[1]\n",
    "        trg_len = trg.shape[0]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "        \n",
    "        #tensor to store decoder outputs\n",
    "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
    "        # save the encoder-decoder attention weights\n",
    "        # all_attention_weights = [batch_size, trg len-1, src len ]\n",
    "        all_attention_weights = torch.zeros(trg.shape[1], trg.shape[0]-1, src.shape[0])\n",
    "        #encoder_outputs is all hidden states of the input sequence, back and forwards\n",
    "        #hidden is the final forward and backward hidden states, passed through a linear layer\n",
    "        encoder_outputs, (hidden, cell) = self.encoder(src) \n",
    "\n",
    "        hidden = torch.cat((hidden[-2,:,:],hidden[-1,:,:]),dim=-1)\n",
    "        cell = torch.cat((cell[-2,:,:],cell[-1,:,:]),dim=-1).unsqueeze(0)\n",
    "        #first input to the decoder is the <sos> tokens\n",
    "        input = trg[0,:]\n",
    "        \n",
    "        for t in range(1, trg_len):\n",
    "            \n",
    "            #insert input token embedding, previous hidden state and all encoder hidden states\n",
    "            #receive output tensor (predictions) and new hidden state\n",
    "            output, hidden, cell, attention_weights = self.decoder(input, hidden, cell, encoder_outputs)\n",
    "            \n",
    "            # all_attention_weights[t-1] = [src len, batch size]\n",
    "            all_attention_weights[:,t-1,:] = attention_weights.squeeze(1)\n",
    "            \n",
    "            #place predictions in a tensor holding predictions for each token\n",
    "            outputs[t] = output\n",
    "            \n",
    "            #decide if we are going to use teacher forcing or not\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            \n",
    "            #get the highest predicted token from our predictions\n",
    "            top1 = output.argmax(1) \n",
    "            \n",
    "            #if teacher forcing, use actual next token as next input\n",
    "            #if not, use predicted token\n",
    "            input = trg[t] if teacher_force else top1\n",
    "\n",
    "        return outputs,all_attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F0Mr0CLfJGG-"
   },
   "source": [
    "### 2.2.6 You may need to revise the code of instantiating classes. Pleae show your instantiation code:\n",
    "rubric={accuracy:2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8I_WInP7JHqM"
   },
   "outputs": [],
   "source": [
    "INPUT_DIM = len(SRC.vocab)\n",
    "OUTPUT_DIM = len(TRG.vocab)\n",
    "ENC_EMB_DIM = 256\n",
    "DEC_EMB_DIM = 256\n",
    "ENC_HID_DIM = 512\n",
    "DEC_HID_DIM = 1024 #<--------------\n",
    "ENC_DROPOUT = 0.5\n",
    "DEC_DROPOUT = 0.5\n",
    "N_LAYERS = 1\n",
    "LEARNING_RT = 0.001\n",
    "\n",
    "attn = Attention()\n",
    "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, N_LAYERS, ENC_DROPOUT)\n",
    "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, N_LAYERS, DEC_DROPOUT, attn)\n",
    "\n",
    "model = Seq2Seq(enc, dec, device).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZsOJjBDqJPBR"
   },
   "source": [
    "### 2.2.7 Please write code to visualize the attention alignment of following sentences using your best model in 2.2 and include your visualization pictures in your Lab3 directory. Your visualization pictures should be named `ex2_sentence_n.png` where $n$ is the index. Please include your code in `Lab3_exp.ipynb`.\n",
    "rubric={accuracy:3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dahNevOHJRp5"
   },
   "outputs": [],
   "source": [
    "sentence_1 = \"Une femme lit un magazine par dessus l'épaule d'une autre femme.\"\n",
    "sentence_2 = \"Un gars torse nu regardant au loin tandis que trois femmes passent devant une foule assise devant un café.\"\n",
    "sentence_3 = \"Two groups of swimmers wade out.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IHP-b3RLkXah"
   },
   "source": [
    "**Put your translation here**\n",
    "\n",
    "My translation:\n",
    "- sentence_1: a woman is reading a magazine over another woman 's shoulder .\n",
    "- sentence_2: a shirtless guy staring into a distance while three women walk past a crowd sitting outside of a cafe .\n",
    "- sentence_3: two groups of `<unk> <unk>` outside ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ptt2tKPZJV_W"
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "def translation_vis(sentence,SRC,TRG,model):\n",
    "  model.eval()\n",
    "  src_token = SRC.preprocess(sentence)\n",
    "  print(\"src_token:\", src_token)\n",
    "  src_tensor = SRC.process([src_token],device=device)\n",
    "  print(\"shape of source language: \", src_tensor.shape)\n",
    "\n",
    "  trg_token = ['<pad>']*64\n",
    "  trg_tensor = TRG.process([trg_token],device=device)\n",
    "  print(\"shape of target language: \", trg_tensor.shape)\n",
    "\n",
    "  output, all_attention_weights = model(src_tensor, trg_tensor,teacher_forcing_ratio = 0.0)\n",
    "  output_dim = output.shape[-1]\n",
    "  # get translation results, we ignore first token <sos> in both translation and target sentences. \n",
    "  # output_translate = [(trg len - 1), batch, output dim] output dim is size of target vocabulary.    \n",
    "  output_translate = output[1:]\n",
    "\n",
    "  source_languege = src_tensor[1:,0].cpu().numpy() ## ***We should also remove the <sos> token in source sentence.***\n",
    "  translation_logit = output_translate[:,0,:].squeeze(1)\n",
    "\n",
    "  # Choose top 1 word from decoder's output, we get the probability and index of the word\n",
    "  prob, token_id = translation_logit.data.topk(1)\n",
    "  token_id = token_id.squeeze(1).cpu().numpy()\n",
    "  \n",
    "  # get source langauge in text\n",
    "  src_langauge = []\n",
    "\n",
    "  for i in source_languege:\n",
    "      if i == SRC.vocab.stoi['<eos>']:\n",
    "          # token = SRC.vocab.itos[i]\n",
    "          # src_langauge.append(token)\n",
    "          break\n",
    "      else:\n",
    "          token = SRC.vocab.itos[i]\n",
    "          src_langauge.append(token)\n",
    "  print(\"Source language:\", src_langauge)\n",
    "\n",
    "  # get machine translation in text\n",
    "  trans_langauge = []\n",
    "  for i in token_id:\n",
    "      if i == TRG.vocab.stoi['<eos>']:\n",
    "          # token = SRC.vocab.itos[i]\n",
    "          # trans_langauge.append(token)\n",
    "          break\n",
    "      else:\n",
    "          token = TRG.vocab.itos[i]\n",
    "          trans_langauge.append(token)\n",
    "  print(\"Our model translation: \", \" \".join(trans_langauge))\n",
    "\n",
    "  all_attention_weights = all_attention_weights[0,:,1:].cpu().detach().numpy()\n",
    "  print(all_attention_weights.shape)\n",
    "  all_attention_weights = np.array(all_attention_weights)[0:len(trans_langauge),0:len(src_langauge)] #<-----\n",
    "  print(all_attention_weights.shape)\n",
    "  ###When we ingor the first <sos> tokens in both traget and source languges (in previous steps), we should also skip all the attention weights that compare with <sos> hidden state in Encoder. \n",
    "  fig, ax = plt.subplots()\n",
    "  im = ax.imshow(all_attention_weights,cmap=plt.cm.Blues)\n",
    "  # plt.colorbar(im)\n",
    "  # We want to show all ticks...\n",
    "  ax.set_xticks(np.arange(len(src_langauge)))\n",
    "  ax.set_yticks(np.arange(len(trans_langauge)))\n",
    "  # ... and label them with the respective list entries\n",
    "  ax.set_xticklabels(src_langauge)\n",
    "  ax.set_yticklabels(trans_langauge)\n",
    "\n",
    "  # Rotate the tick labels and set their alignment.\n",
    "  plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "          rotation_mode=\"anchor\")\n",
    "\n",
    "  ax.set_title(\"seq2seq-Dot-product-Attn\")\n",
    "  fig.tight_layout()\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 436
    },
    "colab_type": "code",
    "id": "SCHVbO6XK6N3",
    "outputId": "d2bf1cc3-954e-44c2-faa9-6bb698ec4ca6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src_token: ['une', 'femme', 'lit', 'un', 'magazine', 'par', 'dessus', \"l'\", 'épaule', \"d'\", 'une', 'autre', 'femme', '.']\n",
      "shape of source language:  torch.Size([16, 1])\n",
      "shape of target language:  torch.Size([66, 1])\n",
      "Source language: ['une', 'femme', 'lit', 'un', 'magazine', 'par', 'dessus', \"l'\", 'épaule', \"d'\", 'une', 'autre', 'femme', '.']\n",
      "Our model translation:  a woman reads a magazine over another woman 's shoulder .\n",
      "(65, 15)\n",
      "(11, 14)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "translation_vis(sentence_1,SRC,TRG,model_ex2_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 436
    },
    "colab_type": "code",
    "id": "Bfy82mczLzc7",
    "outputId": "0bf7754b-f12a-4498-d3a7-f275e0547352"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src_token: ['un', 'gars', 'torse', 'nu', 'regardant', 'au', 'loin', 'tandis', 'que', 'trois', 'femmes', 'passent', 'devant', 'une', 'foule', 'assise', 'devant', 'un', 'café', '.']\n",
      "shape of source language:  torch.Size([22, 1])\n",
      "shape of target language:  torch.Size([66, 1])\n",
      "Source language: ['un', 'gars', 'torse', 'nu', 'regardant', 'au', 'loin', 'tandis', 'que', 'trois', 'femmes', 'passent', 'devant', 'une', 'foule', 'assise', 'devant', 'un', 'café', '.']\n",
      "Our model translation:  a shirtless guy staring into the distance while three women walk past a crowd sitting outside of a cafe .\n",
      "(65, 21)\n",
      "(20, 20)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "translation_vis(sentence_2,SRC,TRG,model_ex2_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 416
    },
    "colab_type": "code",
    "id": "WMLXIiZNaa38",
    "outputId": "4183b386-7cba-4d3d-f496-32b15142b2c2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src_token: ['deux', 'groupes', 'de', 'baigneurs', 'barbotent', 'dehors', '.']\n",
      "shape of source language:  torch.Size([9, 1])\n",
      "shape of target language:  torch.Size([66, 1])\n",
      "Source language: ['deux', 'groupes', 'de', '<unk>', '<unk>', 'dehors', '.']\n",
      "Our model translation:  two groups of <unk> <unk> outside .\n",
      "(65, 8)\n",
      "(7, 7)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "translation_vis(sentence_3,SRC,TRG,model_ex2_best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Rv_N97KuK5n8"
   },
   "source": [
    "### 2.2.8 Please write code to visualize the attention alignment of three sentences in 2.2.7 using your best model of `seq2seq+additive attention (this week tutorial)` and include your visualization pictures in your Lab3 directory. Your visualization pictures should be named `tutorial3_sentence_n.png` where $n$ is the index. Please include your code in `Lab3_exp.ipynb`.\n",
    "rubric={accuracy:3}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d28ppAp_nMLQ"
   },
   "source": [
    "**Put your translation here**\n",
    "\n",
    "My translation:\n",
    "- sentence_1: a woman is reading a magazine over another woman 's shoulder .\n",
    "- sentence_2: a shirtless guy staring into a distance while three women walk past a crowd sitting outside of a cafe .\n",
    "- sentence_3: two groups of `<unk> <unk>` outside ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PgBVWBjpb_Ib"
   },
   "source": [
    "### Load best seq2seq+additive attention model and corresponding Fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FvmWs1zMb_uA"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, enc_hid_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        self.emb_dim = emb_dim\n",
    "        self.enc_hid_dim = enc_hid_dim\n",
    "        self.dropout = dropout\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        self.lstm = nn.LSTM(emb_dim, enc_hid_dim, n_layers, dropout=dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, src):\n",
    "        \n",
    "        #src = [src len, batch size]\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "        \n",
    "        #embedded = [src len, batch size, emb dim]\n",
    "        \n",
    "        outputs, (hidden, cell) = self.lstm(embedded)\n",
    "       \n",
    "        # outputs are always from the top hidden layer, if bidirectional outputs are concatenated.\n",
    "        # outputs shape [sequence_length, batch_size, hidden_dim * num_directions]\n",
    "        # hidden is of shape [num_layers * num_directions, batch_size, hidden_size]\n",
    "        # cell is of shape [num_layers * num_directions, batch_size, hidden_size]\n",
    "        \n",
    "        return outputs, (hidden.squeeze(0), cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KLLSLEkbirAz"
   },
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, enc_hid_dim, dec_hid_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.W_a = nn.Linear(enc_hid_dim + dec_hid_dim, dec_hid_dim)\n",
    "        self.v_a = nn.Parameter(torch.rand(dec_hid_dim)) # same as doing nn.Linear(dec_hid_dim, 1, bias=False)\n",
    "        \n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        #hidden = [batch size, dec hid dim]\n",
    "        #encoder_outputs = [src len, batch size, enc hid dim]\n",
    "        \n",
    "        batch_size = encoder_outputs.shape[1]\n",
    "        src_len = encoder_outputs.shape[0]\n",
    "        \n",
    "        #repeat encoder hidden state src_len-1 times\n",
    "        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
    "        \n",
    "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
    "        \n",
    "        #hidden = [batch size, src len, dec hid dim]\n",
    "        #encoder_outputs = [batch size, src len, enc hid dim]\n",
    "        \n",
    "        # attention scoring function - part 1 - tanh(W_a[s;h])\n",
    "        energy = torch.tanh(self.W_a(torch.cat((hidden, encoder_outputs), dim = 2))) \n",
    "        \n",
    "        #energy = [batch size, src len, dec hid dim]\n",
    "        \n",
    "        energy = energy.permute(0, 2, 1)\n",
    "        \n",
    "        #energy = [batch size, dec hid dim, src len]\n",
    "        \n",
    "        #v = [dec hid dim]\n",
    "        \n",
    "        v = self.v_a.repeat(batch_size, 1).unsqueeze(1)\n",
    "        \n",
    "        #v = [batch size, 1, dec hid dim]\n",
    "        \n",
    "        # attention scoring function - part 2 - v_a(tanh(W_a[s;h]))\n",
    "        attention = torch.bmm(v, energy).squeeze(1)\n",
    "        \n",
    "        #attention= [batch size, src len]\n",
    "\n",
    "        # attention scoring function - part 2 - softmax(v_a(tanh(W_a[s;h])))\n",
    "        return F.softmax(attention, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q6_jVdFyiuXm"
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, enc_hid_dim, dec_hid_dim, n_layers, dropout, attention):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.output_dim = output_dim\n",
    "        self.attention = attention\n",
    "        \n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        \n",
    "        self.rnn = nn.LSTM(enc_hid_dim + emb_dim, dec_hid_dim, n_layers, dropout=dropout)\n",
    "        \n",
    "        self.fc_out = nn.Linear(enc_hid_dim + dec_hid_dim + emb_dim, output_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, input, hidden, cell, encoder_outputs):\n",
    "             \n",
    "        #input = [batch size]\n",
    "        #hidden = [batch size, dec hid dim]\n",
    "        #encoder_outputs = [src len, batch size, enc hid dim]\n",
    "        input = input.unsqueeze(0)\n",
    "        \n",
    "        #input = [1, batch size]\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        \n",
    "        #embedded = [1, batch size, emb dim]\n",
    "        # get the attention probabilities\n",
    "        attention_weights = self.attention(hidden, encoder_outputs)\n",
    "                \n",
    "        #attention_weights = [batch size, src len]\n",
    "        \n",
    "        attention_weights = attention_weights.unsqueeze(1)\n",
    "        \n",
    "        #a = [batch size, 1, src len]\n",
    "        \n",
    "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
    "        \n",
    "        #encoder_outputs = [batch size, src len, enc hid dim]\n",
    "        # perform weighted sum of encoder hidden states to get attention output\n",
    "        weighted = torch.bmm(attention_weights, encoder_outputs)\n",
    "        \n",
    "        #weighted = [batch size, 1, enc hid dim]\n",
    "        \n",
    "        weighted = weighted.permute(1, 0, 2)\n",
    "        \n",
    "        #weighted = [1, batch size, enc hid dim]\n",
    "        # concatenate the attention outputs (or context vectors) with the current decoder input\n",
    "        rnn_input = torch.cat((embedded, weighted), dim = 2)\n",
    "        \n",
    "        #rnn_input = [1, batch size, (enc hid dim) + emb dim]\n",
    "        output, (hidden, cell) = self.rnn(rnn_input, (hidden.unsqueeze(0), cell))\n",
    "        \n",
    "        #output = [seq len, batch size, dec hid dim * n directions]\n",
    "        #hidden = [n layers * n directions, batch size, dec hid dim]\n",
    "        \n",
    "        #seq len, n layers and n directions will always be 1 in this decoder, therefore:\n",
    "        #output = [1, batch size, dec hid dim]\n",
    "        #hidden = [1, batch size, dec hid dim]\n",
    "        #this also means that output == hidden\n",
    "        assert (output == hidden).all()\n",
    "        \n",
    "        embedded = embedded.squeeze(0)\n",
    "        output = output.squeeze(0)\n",
    "        weighted = weighted.squeeze(0)\n",
    "        \n",
    "        # classification over the entire word vocabulary\n",
    "        prediction = self.fc_out(torch.cat((output, weighted, embedded), dim = 1))\n",
    "        \n",
    "        #prediction = [batch size, output dim]\n",
    "        \n",
    "        return prediction, hidden.squeeze(0), cell, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "T2Yhqgciiw-k"
   },
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        \n",
    "    def forward(self, src, trg, teacher_forcing_ratio = 0.5):\n",
    "        \n",
    "        #src = [src len, batch size]\n",
    "        #trg = [trg len, batch size]\n",
    "        #teacher_forcing_ratio is probability to use teacher forcing\n",
    "        #e.g. if teacher_forcing_ratio is 0.75 we use teacher forcing 75% of the time\n",
    "        batch_size = src.shape[1]\n",
    "        trg_len = trg.shape[0]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "        \n",
    "        #tensor to store decoder outputs\n",
    "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
    "        # save the encoder-decoder attention weights\n",
    "        # all_attention_weights = [batch_size, trg len-1, src len ]\n",
    "        all_attention_weights = torch.zeros(trg.shape[1], trg.shape[0]-1, src.shape[0])\n",
    "        #encoder_outputs is all hidden states of the input sequence, back and forwards\n",
    "        #hidden is the final forward and backward hidden states, passed through a linear layer\n",
    "        encoder_outputs, (hidden, cell) = self.encoder(src)\n",
    "                \n",
    "        #first input to the decoder is the <sos> tokens\n",
    "        input = trg[0,:]\n",
    "        \n",
    "        for t in range(1, trg_len):\n",
    "            \n",
    "            #insert input token embedding, previous hidden state and all encoder hidden states\n",
    "            #receive output tensor (predictions) and new hidden state\n",
    "            output, hidden, cell, attention_weights = self.decoder(input, hidden, cell, encoder_outputs)\n",
    "            \n",
    "            # all_attention_weights[t-1] = [src len, batch size]\n",
    "            all_attention_weights[:,t-1,:] = attention_weights.squeeze(1)\n",
    "            \n",
    "            #place predictions in a tensor holding predictions for each token\n",
    "            outputs[t] = output\n",
    "            \n",
    "            #decide if we are going to use teacher forcing or not\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            \n",
    "            #get the highest predicted token from our predictions\n",
    "            top1 = output.argmax(1) \n",
    "            \n",
    "            #if teacher forcing, use actual next token as next input\n",
    "            #if not, use predicted token\n",
    "            input = trg[t] if teacher_force else top1\n",
    "\n",
    "        return outputs,all_attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4pZlwWuniryb"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"./drive/My Drive/Colab Notebooks/ckpt_attention_tutorial/TRG.Field\",\"rb\") as f:\n",
    "     TRG_saved_add = pickle.load(f)\n",
    "\n",
    "with open(\"./drive/My Drive/Colab Notebooks/ckpt_attention_tutorial/SRC.Field\",\"rb\") as f:\n",
    "     SRC_saved_add = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "KaFv6zmWizJn",
    "outputId": "4e3451bf-e627-4b07-d208-c6860d8a50b5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py:50: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    }
   ],
   "source": [
    "INPUT_DIM = len(SRC_saved_add.vocab)\n",
    "OUTPUT_DIM = len(TRG_saved_add.vocab)\n",
    "ENC_EMB_DIM = 256\n",
    "DEC_EMB_DIM = 256\n",
    "ENC_HID_DIM = 512\n",
    "DEC_HID_DIM = 512\n",
    "ENC_DROPOUT = 0.5\n",
    "DEC_DROPOUT = 0.5\n",
    "N_LAYERS = 1\n",
    "LEARNING_RT = 0.001\n",
    "\n",
    "attn = Attention(ENC_HID_DIM, DEC_HID_DIM)\n",
    "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, N_LAYERS, ENC_DROPOUT)\n",
    "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, N_LAYERS, DEC_DROPOUT, attn)\n",
    "\n",
    "model_add = Seq2Seq(enc, dec, device)\n",
    "model_add.load_state_dict(torch.load('./drive/My Drive/Colab Notebooks/ckpt_attention_tutorial/seq2seq_attn_4.pt')['state_dict'])\n",
    "model_add = model_add.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 416
    },
    "colab_type": "code",
    "id": "txaKCyhnkNwO",
    "outputId": "490bf964-3c1e-4727-d3c6-cd14043d9864"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src_token: ['une', 'femme', 'lit', 'un', 'magazine', 'par', 'dessus', \"l'\", 'épaule', \"d'\", 'une', 'autre', 'femme', '.']\n",
      "shape of source language:  torch.Size([16, 1])\n",
      "shape of target language:  torch.Size([66, 1])\n",
      "Source language: ['une', 'femme', 'lit', 'un', 'magazine', 'par', 'dessus', \"l'\", 'épaule', \"d'\", 'une', 'autre', 'femme', '.']\n",
      "Our model translation:  a woman reads a magazine by another woman 's shoulder .\n",
      "(65, 15)\n",
      "(11, 14)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUcAAAEYCAYAAADPkTRJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nMDtL5C0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "translation_vis(sentence_1,SRC_saved_add,TRG_saved_add,model_add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 436
    },
    "colab_type": "code",
    "id": "KkXEhhyOm-k1",
    "outputId": "572e4993-df75-4a38-f184-22b7cbcefcf7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src_token: ['un', 'gars', 'torse', 'nu', 'regardant', 'au', 'loin', 'tandis', 'que', 'trois', 'femmes', 'passent', 'devant', 'une', 'foule', 'assise', 'devant', 'un', 'café', '.']\n",
      "shape of source language:  torch.Size([22, 1])\n",
      "shape of target language:  torch.Size([66, 1])\n",
      "Source language: ['un', 'gars', 'torse', 'nu', 'regardant', 'au', 'loin', 'tandis', 'que', 'trois', 'femmes', 'passent', 'devant', 'une', 'foule', 'assise', 'devant', 'un', 'café', '.']\n",
      "Our model translation:  a shirtless man looking at the distance while three women walk past a crowd sitting outside of a cafe .\n",
      "(65, 21)\n",
      "(20, 20)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQsAAAEYCAYAAABLF9NnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "translation_vis(sentence_2,SRC_saved_add,TRG_saved_add,model_add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 416
    },
    "colab_type": "code",
    "id": "5mLqdIK3nAXd",
    "outputId": "2b636051-ff6c-4ad6-9ecc-eb6096c3168c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src_token: ['deux', 'groupes', 'de', 'baigneurs', 'barbotent', 'dehors', '.']\n",
      "shape of source language:  torch.Size([9, 1])\n",
      "shape of target language:  torch.Size([66, 1])\n",
      "Source language: ['deux', 'groupes', 'de', '<unk>', '<unk>', 'dehors', '.']\n",
      "Our model translation:  two <unk> <unk> <unk> outside .\n",
      "(65, 8)\n",
      "(6, 7)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAASwAAAEYCAYAAADmlsvOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "translation_vis(sentence_3,SRC_saved_add,TRG_saved_add,model_add)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9gODH110b9ls"
   },
   "source": [
    "### 2.2.9 Please compare the translation of 2.2.7 and 2.2.8 and provide a error analysis. (OPTIONAL)\n",
    "rubric={reasoning:2}\n",
    "\n",
    "Hints:\n",
    "- try to identify issues with the translation.\n",
    "- If you don’t know the two languages, you can compare against Google translation. \n",
    "- Which model is better? Why?\n",
    "- You answer should be less than 100 words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**You answer goes here:**\n",
    "\n",
    "Gold Translation \n",
    "- sentence_1 = \"A woman is reading a magazine over another woman's shoulder.\"\n",
    "- sentence_2 = \"Shirtless guy staring off in the distance while three woman are walking past a crowd sitting outside a cafe.\"\n",
    "- sentence_3 = \"Two groups of swimmers wade out.\"\n",
    "\n",
    "Additive attention model translation:\n",
    "\n",
    "- sentence_1: a woman reads a magazine by another woman 's shoulder .\n",
    "- sentence_2: a shirtless man looking at the distance while three women walk past a crowd sitting outside of a cafe\n",
    "- sentence_3: two <unk> <unk> <unk> outside .\n",
    "\n",
    "Dot-product attention model translation:\n",
    "- sentence_1: a woman is reading a magazine over another woman 's shoulder .\n",
    "- sentence_2: a shirtless guy staring into a distance while three women walk past a crowd sitting outside of a cafe .\n",
    "- sentence_3: two groups of `<unk> <unk>` outside .\n",
    "\n",
    "Both model are unable to deal with `<unk>` tokens because of losing in source language. \n",
    "\n",
    "Based on the visulizations, the dot-product attention give better alignments. Additive attention seems to do worse with differentiating between similar but slightly different words (man vs. guy, looking vs. staring) going for the more generic word instead of the slightly more specific word. Both models need to be post-processed to deal with tokenization issues ('s and casing).\n",
    "\n",
    "Compare with gold translation, both missed some phrases such as `staring off` and `are walking`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "h8BSZztucCQc"
   },
   "source": [
    "## Exercise 3 Error analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KOqabelvcEyF"
   },
   "source": [
    "Now, we have four seq2seq models: (1) basic seq2seq from week2 tutorial (`seq2seq_tutorial.ipynb`); (2) seq2seq with additive attention from week3 tutorial (`attention_tutorial.ipynb`); (3) seq2seq variant of Exercise 1; (4) seq2seq with dot product attention in Exercise 2. Please use the best models of these four models to answer the following questions. \n",
    "\n",
    "If you had problems creating any of these 4 models, just report your results on the models you were able to make work/develop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RjiTOQJncGlo"
   },
   "source": [
    "### 3.1 Please report the cumulative BLEU-4 score on test set (i.e., `test_eng_fre.tsv`) via the corpus_bleu() function. (Your best model may get at different epochs.)\n",
    "rubric={accuracy:1}\n",
    "\n",
    "Hint: You can use `inference()` function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SzVprpTdcort"
   },
   "source": [
    "**You answer goes here:**\n",
    "- For model (1), **my best model obtains 0.25 cumulative BLEU-4 score with 7 epoch(s).** \n",
    "- For model (2), **my best model obtains 0.43 cumulative BLEU-4 score with 4 epoch(s).** \n",
    "- For model (3), **my best model obtains 0.27 cumulative BLEU-4 score with 9 epoch(s).** \n",
    "- For model (4), **my best model obtains 0.48 cumulative BLEU-4 score with 9 epoch(s).** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "gnGhKmqgrWqk",
    "outputId": "0cc8adea-1a97-4d27-ac3e-e6a0a54923bc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.43020173041366533\n"
     ]
    }
   ],
   "source": [
    "## get BLEU of seq2seq + additive attention\n",
    "print(inference(model_add, \"./drive/My Drive/Colab Notebooks/eng-fre/test_eng_fre.tsv\", SRC_saved_add, TRG_saved_add, True, 64))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8S3ha213cw9Y"
   },
   "source": [
    "### 3.2 Please evaluate any one of four models on on test set (i.e., `test_eng_fre.tsv`) and report its BLEU-1, cumulative BLEU-2, BLEU-3, and BLEU-4 scores via the corpus_bleu() function. (You can any model you have, we use additive attention model as am example here.)\n",
    "rubric={accuracy:2}\n",
    "\n",
    "Hints: \n",
    "- You can use the `inference()` function. But you will need to revise few lines of this function.\n",
    "- You may need to review the BLEU implementation in week 2 tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rLuPvZq-cyE_"
   },
   "source": [
    "**You answer goes here:**\n",
    "- **I evaluate on model (2).** \n",
    "- **This model obtains 0.72 BLEU-1 score** \n",
    "- **This model obtains 0.60 cumulative BLEU-2 score** \n",
    "- **This model obtains 0.51 cumulative BLEU-3 score** \n",
    "- **This model obtains 0.43 cumulative BLEU-4 score** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wLVetkNCz7A1"
   },
   "source": [
    "Revise infererence() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zZ290RBazqnS"
   },
   "outputs": [],
   "source": [
    "def inference_bleun(model, file_name, src_vocab, trg_vocab, attention=False, bleu_n=4, weights=(0.25,0.25,0.25,0.25) ,max_trg_len = 64):\n",
    "    '''\n",
    "    Function for translation inference\n",
    "\n",
    "    Input: \n",
    "    model: translation model;\n",
    "    file_name: the directoy of test file that the first column is target reference, and the second column is source language;\n",
    "    trg_vocab: Target torchtext Field\n",
    "    attention: the model returns attention weights or not.\n",
    "    bleu_n: use n-gram to calculate BLEU score, n from 1 to 4.\n",
    "    weights: weights for calculating the cumulative scores for BLEU-1, BLEU-2, BLEU-3 and BLEU-4.\n",
    "    max_trg_len: the maximal length of translation text (optinal), default = 64\n",
    "\n",
    "    Output:\n",
    "    Corpus BLEU score.\n",
    "    '''\n",
    "    from nltk.translate.bleu_score import corpus_bleu\n",
    "    from nltk.translate.bleu_score import sentence_bleu\n",
    "    from torchtext.data import TabularDataset\n",
    "    from torchtext.data import Iterator\n",
    "\n",
    "    # convert index to text string\n",
    "    def convert_itos(convert_vocab, token_ids):\n",
    "        list_string = []\n",
    "        for i in token_ids:\n",
    "            if i == convert_vocab.vocab.stoi['<eos>']:\n",
    "                break\n",
    "            else:\n",
    "                token = convert_vocab.vocab.itos[i]\n",
    "                list_string.append(token)\n",
    "        return list_string\n",
    "\n",
    "    test = TabularDataset(\n",
    "      path=file_name, # the root directory where the data lies\n",
    "      format='tsv',\n",
    "      skip_header=True, # if your tsv file has a header, make sure to pass this to ensure it doesn't get proceesed as data!\n",
    "      fields=[('TRG', trg_vocab), ('SRC', src_vocab)])\n",
    "\n",
    "    test_iter = Iterator(\n",
    "    dataset = test, # we pass in the datasets we want the iterator to draw data from\n",
    "    sort = False,batch_size=128,\n",
    "    sort_key=None,\n",
    "    shuffle=False,\n",
    "    sort_within_batch=False,\n",
    "    device = device,\n",
    "    train=False\n",
    "    )\n",
    "  \n",
    "    model.eval()\n",
    "    all_trg = []\n",
    "    all_translated_trg = []\n",
    "\n",
    "    TRG_PAD_IDX = trg_vocab.vocab.stoi[trg_vocab.pad_token]\n",
    "\n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for i, batch in enumerate(test_iter):\n",
    "\n",
    "            src = batch.SRC\n",
    "            #src = [src len, batch size]\n",
    "\n",
    "            trg = batch.TRG\n",
    "            #trg = [trg len, batch size]\n",
    "\n",
    "            batch_size = trg.shape[1]\n",
    "\n",
    "            # create a placeholder for traget language with shape of [max_trg_len, batch_size] where all the elements are the index of <pad>. Then send to device\n",
    "            trg_placeholder = torch.Tensor(max_trg_len, batch_size)\n",
    "            trg_placeholder.fill_(TRG_PAD_IDX)\n",
    "            trg_placeholder = trg_placeholder.long().to(device)\n",
    "            if attention == True:\n",
    "                output,_ = model(src, trg_placeholder, 0) #turn off teacher forcing\n",
    "            else:\n",
    "                output = model(src, trg_placeholder, 0) #turn off teacher forcing\n",
    "            # get translation results, we ignor first token <sos> in both translation and target sentences. \n",
    "            # output_translate = [(trg len - 1), batch, output dim] output dim is size of target vocabulary.\n",
    "            output_translate = output[1:]\n",
    "            # store gold target sentences to a list \n",
    "            all_trg.append(trg[1:].cpu())\n",
    "\n",
    "            # Choose top 1 word from decoder's output, we get the probability and index of the word\n",
    "            prob, token_id = output_translate.data.topk(1)\n",
    "            translation_token_id = token_id.squeeze(2).cpu()\n",
    "\n",
    "            # store gold target sentences to a list \n",
    "            all_translated_trg.append(translation_token_id)\n",
    "      \n",
    "    all_gold_text = []\n",
    "    all_translated_text = []\n",
    "    for i in range(len(all_trg)): \n",
    "        cur_gold = all_trg[i]\n",
    "        cur_translation = all_translated_trg[i]\n",
    "        for j in range(cur_gold.shape[1]):\n",
    "            gold_convered_strings = convert_itos(trg_vocab,cur_gold[:,j])\n",
    "            trans_convered_strings = convert_itos(trg_vocab,cur_translation[:,j])\n",
    "\n",
    "            all_gold_text.append(gold_convered_strings)\n",
    "            all_translated_text.append(trans_convered_strings)\n",
    "\n",
    "    corpus_all_gold_text = [[item] for item in all_gold_text]\n",
    "    \n",
    "    corpus_bleu_score = corpus_bleu(corpus_all_gold_text, all_translated_text, weights=weights)\n",
    "    print('Cumulative BLEU-%i: %f' % (bleu_n, corpus_bleu_score))\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "e9ISLnHo2GaX",
    "outputId": "ad925c4e-f7c7-40b2-a6c2-1c5b667e9a01"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cumulative BLEU-1: 0.719197\n",
      "Cumulative BLEU-2: 0.600653\n",
      "Cumulative BLEU-3: 0.509357\n",
      "Cumulative BLEU-4: 0.430202\n"
     ]
    }
   ],
   "source": [
    "inference_bleun(model_add, \"./drive/My Drive/Colab Notebooks/eng-fre/test_eng_fre.tsv\", SRC_saved_add, TRG_saved_add, True,1,(1, 0, 0, 0), 64)\n",
    "inference_bleun(model_add, \"./drive/My Drive/Colab Notebooks/eng-fre/test_eng_fre.tsv\", SRC_saved_add, TRG_saved_add, True,2,(0.5, 0.5, 0, 0), 64)\n",
    "inference_bleun(model_add, \"./drive/My Drive/Colab Notebooks/eng-fre/test_eng_fre.tsv\", SRC_saved_add, TRG_saved_add, True,3,(0.33, 0.33, 0.33, 0), 64)\n",
    "inference_bleun(model_add, \"./drive/My Drive/Colab Notebooks/eng-fre/test_eng_fre.tsv\", SRC_saved_add, TRG_saved_add, True,4,(0.25, 0.25, 0.25, 0.25), 64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FYnAMHtUcyUm"
   },
   "source": [
    "### 3.3 Please explain the results of 3.2 briefly. \n",
    "rubric={reasoning:2}\n",
    "\n",
    "Hints: What is the relationship between BLEU-n scores? Why? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q-CBhdA4cyXq"
   },
   "source": [
    "**You answer goes here:**\n",
    "\n",
    "BLEU-1> BLEU-2 > BLEU-3 > BLEU-4\n",
    "\n",
    "BLEU measures overlapping windows/sequences between source and target, and is the primary (in many works sole) measure for MT systems. \n",
    "\n",
    "It is more hard to match more windows/sequences exactly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uNyofiGgc41Z"
   },
   "source": [
    "To analyze the effects of sentence length, we create two subsets of test file: A. `long_test_eng_fre.tsv` which only includes sentence pairs that English reference is longer than 19 tokens; B. `short_test_eng_fre.tsv` which only includes sentence pairs that English reference is shorter than 9 tokens. `long_test_eng_fre.tsv` includes 67 sentence pairs.  `short_test_eng_fre.tsv` includes 85 sentence pairs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MMsdmE6OdCJP"
   },
   "source": [
    "### 3.4 Please report the cumulative BLEU-4 score on long sentences of test set (i.e., `long_test_eng_fre.tsv`) via the corpus_bleu() function. \n",
    "rubric={accuracy:2}\n",
    "\n",
    "Hint: You can use `inference()` function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uat6YMNAdEGT"
   },
   "source": [
    "**You answer goes here:**\n",
    "\n",
    "Evaluate on `long_test_eng_fre.tsv`:\n",
    "- For model (1), **my best model obtains 0.15 cumulative BLEU-4.** \n",
    "- For model (2), **my best model obtains 0.34 cumulative BLEU-4.**\n",
    "- For model (3), **my best model obtains 0.16 cumulative BLEU-4.**\n",
    "- For model (4), **my best model obtains 0.41 cumulative BLEU-4.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example to answer this question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "LsDVCYBH3PlG",
    "outputId": "cd237481-3b03-4f3b-c0e9-9130a0a88cab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cumulative BLEU-4: 0.411064\n"
     ]
    }
   ],
   "source": [
    "inference_bleun(model_ex2_best, \"./eng-fre/long_test_eng_fre.tsv\", SRC_ex2_saved, TRG_ex2_aved, True,4,(0.25, 0.25, 0.25, 0.25), 64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JYDOgvP5dGFc"
   },
   "source": [
    "### 3.5 Please report the cumulative BLEU-4 score on short sentences of test set (i.e., `short_test_eng_fre.tsv`) via the corpus_bleu() function. \n",
    "rubric={accuracy:2}\n",
    "\n",
    "Hint: You can use `inference()` function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mMbZHTb4eBae"
   },
   "source": [
    "**You answer goes here:**\n",
    "\n",
    "Evaluate on `short_test_eng_fre.tsv`:\n",
    "- For model (1), **my best model obtains 0.31 cumulative BLEU-4.** \n",
    "- For model (2), **my best model obtains 0.43 cumulative BLEU-4.**\n",
    "- For model (3), **my best model obtains 0.31 cumulative BLEU-4.**\n",
    "- For model (4), **my best model obtains 0.49 cumulative BLEU-4.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "tyegskF23Vtj",
    "outputId": "3d5ad728-0752-4a54-d1d7-c9bb57ad0776"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cumulative BLEU-4: 0.486033\n"
     ]
    }
   ],
   "source": [
    "inference_bleun(model_ex2_best, \"./eng-fre/short_test_eng_fre.tsv\", SRC_ex2_saved, TRG_ex2_aved, True,4,(0.25, 0.25, 0.25, 0.25), 64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SdFnmmkMeDay"
   },
   "source": [
    "### 3.6 Please compare the results of questions of 3.1-3.3. Which model does perform best? What differences between their performance? How does the length of sentence affect performance?\n",
    "rubric={reasoning:4}\n",
    "\n",
    "Hint:\n",
    "- You can review the Section 5 of [Luong et al. (2015)](https://arxiv.org/pdf/1508.04025.pdf) to answer this question.\n",
    "- You answer should be less than 100 words. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GesZfl9neGk2"
   },
   "source": [
    "**Your answer goes here:**\n",
    "\n",
    "The (4) model performs best. \n",
    "The models with Attention perform better than the models without attention. \n",
    "All the models performance do better on `short_test_eng_fre.tsv` than on the `long_test_eng_fre.tsv` set.\n",
    "Without attention mechnism, the model do poorly on `long_test_eng_fre.tsv` set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Lrpit_pVeIhU"
   },
   "source": [
    "## Exercise 4 Conceptual Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rXh3n0Z1eLUr"
   },
   "source": [
    "### 4.1 BLEU\n",
    "rubric={reasoning:2}\n",
    "\n",
    "BLEU is one of the most widely used metrics for NLP, but why can't we just use accuracy to measure the success of translations? Does accuracy make sense in machine translation? \n",
    "\n",
    "Hint: Feel free to do some research to answer this question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K9A80jxVeLYT"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_gIDH2sWeLcA"
   },
   "source": [
    "**Your answer goes here:**\n",
    "\n",
    "Because of the variety in language, there may be a number of different possible translation that you can make for a given sentence, BLEU allows you to compare a model output to a set of different reference sentences, making it more flexible than straight accuracy, which might not be able to account for the flexibility of language. For instance if we were to only use accuracy we would have to determine the following: Is word order important? In which case do we only look at whether a word is in the reference sentence to count if it is correct? What about sentences where word order is very flexible but phrasal units are bound in a systematic way? Then we would need something where we account for overlapping n-grams to try to capture those phrasal units. If we adjust for all of those things we will eventually wind up with something very similar to BLEU, and fairly distant from a straight accuracy measurement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m9XfaV-1eLfe"
   },
   "source": [
    "### 4.2 Teacher Forcing\n",
    "rubric={reasoning:2}\n",
    "\n",
    "For training seq2seq models we often use Teacher Forcing as part of the training procedure. Identify two advantages of using this approach in training.  \n",
    "\n",
    "Hint: Feel free to do some research to answer this question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Rxdf2i6_eRnw"
   },
   "source": [
    "**Your answer goes here:**\n",
    "\n",
    "Faster overall to train.\n",
    "\n",
    "Without teacher forcing you might run into convergance issues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qOVBlkMNeTLQ"
   },
   "source": [
    "### 4.3 Attention bottleneck\n",
    "rubric={reasoning:1}\n",
    "\n",
    "Learning attention functions automatically require large volumes of data especially for recurrent neural networks. Can you recommend a solution to overcome this problem? Barrett and Bingel investigate an approach to solve this limitation. Take a few minutes to SKIM the abstract and introduction and write a few sentences summarizing takeaways that you can use in practice.\n",
    "\n",
    "Barrett, M., Bingel, J., Hollenstein, N., Rei, M., & Søgaard, A. (2018, October). Sequence classification with human attention. In Proceedings of the 22nd Conference on Computational Natural Language Learning (pp. 302-312)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jbh2gjYheU5L"
   },
   "source": [
    "**Your answer goes here:**\n",
    "\n",
    "Using 'human-attention' can be beneficial to NLP tasks, you could potentially use open eyetracking databases (if they are on a relevant corpus) to derive these weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tx2pr60zeXG_"
   },
   "source": [
    "### 4.4 Global Attention Mechanisms\n",
    "rubric={reasoning:1}\n",
    "\n",
    "Global attention mechanisms consider all the encoder hidden states when deriving the context vector while local attention mechanisms consider only a subset of the encoder hidden states when deriving the context vector. [Luong et al. 2015](https://arxiv.org/pdf/1508.04025.pdf) proposed **three global attention mechanisms**. In the tutorial, we looked at (terminologies are borrowed from Luongs's paper):\n",
    "* Dot: $e_{ij} = s_{i-1}^Th_j \\in \\mathcal{R}$\n",
    "* Concat: $e_{ij} = v_a tanh( W_a [s_{i-1};h_j;]) \\in \\mathcal{R}$ where $W_a \\in \\mathcal{R}^{h\\times 2h}$ and $v_a \\in \\mathcal{R}^{1\\times h}$\n",
    "\n",
    "Can you find out the third global attention mechanism by going over the paper (especially Section 3.1) and write it down in the same format as above?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JXznLZ2zeYdb"
   },
   "source": [
    "**Your answer goes here:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hJKDAOB4cpCO"
   },
   "source": [
    "* General: $e_{ij} = s_{i-1}^TW_ah_j$ Which is basically just Dot product attention with a learned Weight matrix to modify the importance of relationships between different parts of a sentence. Dot attention is General attention with  $W_a = I$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Lab3_sol.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
