{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COLX 585 Trends in Computational Linguistic\n",
    "##  Lab tutorial 2: BPE and BERT\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Byte Pair Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural machine translation (NMT) models typically operate with a fixed vocabulary, but translation is an **open-vocabulary problem.** (i.e., we can observe words during testing that are not present in the training vocabulary). **Byte pair encoding (BPE)** enables NMT model translation on open-vocabulary by encoding rare and unknown words as sequences of subword units. This is based on an intuition that various word classes are translatable via smaller units than words. \n",
    "\n",
    "More information in the paper:\n",
    "Sennrich, Rico, Barry Haddow, and Alexandra Birch. \"Neural Machine Translation of Rare Words with Subword Units.\" Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Vol. 1. 2016. http://www.aclweb.org/anthology/P16-1162\n",
    "\n",
    "BPE creates a list of merges that are used for splitting out-of-vocabulary words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Compute frequencies of all words in the training corpus\n",
    "* Start with vocabulary that consists of **singleton symbols** (character) from training corpus\n",
    "* To get vocabulary of $n$ merges, iterate $n$ times:\n",
    "    1. Get the most frequent pair of symbols in the training data\n",
    "    2. Add the pair into list of merges\n",
    "    3. Add the merged symbol into vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./bpe_al.png)\n",
    "Picture Courtesy: https://www.aclweb.org/anthology/P16-1162.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We count the frequency of each word shown in the corpus. \n",
    "\n",
    "For each word, we append a special stop token </w\\> at the end of the word.\n",
    "\n",
    "We then split the word into characters. Initially, the tokens of word are all of its characters plus the additional </w\\> token. For example, the tokens for word “low” are [“l”, “o”, “w”, </w\\>] in order. \n",
    "\n",
    "So after counting all the words in the dataset, we will get a vocabulary for the tokenized word with its corresponding counts, such as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'l o w </w>': 5,\n",
       " 'l o w e r </w>': 2,\n",
       " 'n e w e s t </w>': 6,\n",
       " 'w i d e s t </w>': 3}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{'l o w </w>': 5, 'l o w e r </w>': 2, 'n e w e s t </w>': 6, 'w i d e s t </w>': 3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, collections\n",
    "\n",
    "def get_stats(train_data):\n",
    "    \"\"\"Compute frequencies of adjacent pairs of symbols.\n",
    "    input: train_data,  e.g., {'l o w </w>': 5, 'l o w e r </w>': 2, 'n e w e s t </w>': 6, 'w i d e s t </w>': 3}\n",
    "    output: pairs:  e.g., {('l', 'o'): 7, ('o', 'w'): 7, ('w', '</w>'): 5 ...}\n",
    "    \"\"\"\n",
    "    pairs = collections.defaultdict(int) # initialize a counter \n",
    "    for word, freq in train_data.items():  # get the \n",
    "        symbols = word.split()            # split word by whitespace\n",
    "        for i in range(len(symbols)-1):\n",
    "            pairs[symbols[i],symbols[i+1]] += freq    # calculate frequencies of adjacent pairs\n",
    "    return pairs            # return counter\n",
    "\n",
    "def merge_vocab(best_pair, train_data_in):\n",
    "    \"\"\"\n",
    "    merge the most freqent pair and update training dataset \n",
    "    input: \n",
    "    best_pair: e.g., ('e', 's') the most frequent pair\n",
    "    train_data_in:  e.g., {'l o w </w>': 5,..., 'w i d e s t </w>': 3} \n",
    "    output: \n",
    "    train_data_out: {'l o w </w>': 5, ..., 'n e w es t </w>': 6, 'w i d es t </w>': 3} merged dataset\n",
    "    \"\"\" \n",
    "    train_data_out = {}    # create a new container to hold merged dataset\n",
    "    \n",
    "    bigram = ' '.join(best_pair)   # conbine the most frequent pair,  e.g., 'e s'\n",
    "    bigram = re.escape(bigram)     # add backslash before special characters (here is whitespace), e.g., 'e\\ s'\n",
    "    p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)') # regular expression to search the target bigram (the most freqent pair) \n",
    "    for word in train_data_in:\n",
    "        w_out = p.sub(''.join(best_pair), word)   # ''.join(best_pair), e.g., 'es', word e.g., 'n e w e s t' ---> 'n e w es t' \n",
    "        # find the string that match the regular expression and merge the target pair in train_data, \n",
    "        # e.g., the best pair is (e, s), hence we merge 'es' in 'n e w e s t </w>', the new word is 'n e w es t </w>'\n",
    "        train_data_out[w_out] = train_data_in[word]  # add new data point to container\n",
    "    return train_data_out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In each iteration, we count the frequency of each consecutive byte pair, find out the most frequent one, and merge the two byte pair tokens to one token.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Iteration 1"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new merge: ('e', 's')\n",
      "train data: {'l o w </w>': 5, 'l o w e r </w>': 2, 'n e w es t </w>': 6, 'w i d es t </w>': 3}\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Iteration 2"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new merge: ('es', 't')\n",
      "train data: {'l o w </w>': 5, 'l o w e r </w>': 2, 'n e w est </w>': 6, 'w i d est </w>': 3}\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Iteration 3"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new merge: ('est', '</w>')\n",
      "train data: {'l o w </w>': 5, 'l o w e r </w>': 2, 'n e w est</w>': 6, 'w i d est</w>': 3}\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Iteration 4"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new merge: ('l', 'o')\n",
      "train data: {'lo w </w>': 5, 'lo w e r </w>': 2, 'n e w est</w>': 6, 'w i d est</w>': 3}\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Iteration 5"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new merge: ('lo', 'w')\n",
      "train data: {'low </w>': 5, 'low e r </w>': 2, 'n e w est</w>': 6, 'w i d est</w>': 3}\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Iteration 6"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new merge: ('n', 'e')\n",
      "train data: {'low </w>': 5, 'low e r </w>': 2, 'ne w est</w>': 6, 'w i d est</w>': 3}\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Iteration 7"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new merge: ('ne', 'w')\n",
      "train data: {'low </w>': 5, 'low e r </w>': 2, 'new est</w>': 6, 'w i d est</w>': 3}\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Iteration 8"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new merge: ('new', 'est</w>')\n",
      "train data: {'low </w>': 5, 'low e r </w>': 2, 'newest</w>': 6, 'w i d est</w>': 3}\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Iteration 9"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new merge: ('low', '</w>')\n",
      "train data: {'low</w>': 5, 'low e r </w>': 2, 'newest</w>': 6, 'w i d est</w>': 3}\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Iteration 10"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new merge: ('w', 'i')\n",
      "train data: {'low</w>': 5, 'low e r </w>': 2, 'newest</w>': 6, 'wi d est</w>': 3}\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import display, Markdown, Latex\n",
    "\n",
    "train_data = {'l o w </w>': 5, 'l o w e r </w>': 2, 'n e w e s t </w>': 6, 'w i d e s t </w>': 3}\n",
    "\n",
    "bpe_codes = {}\n",
    "bpe_codes_reverse = {}\n",
    "\n",
    "num_merges = 10\n",
    "\n",
    "for i in range(num_merges):\n",
    "    display(Markdown(\"### Iteration {}\".format(i + 1)))\n",
    "    pairs = get_stats(train_data)      # use defined function\n",
    "    best_pair = max(pairs, key=pairs.get)   # get the most frequent pair\n",
    "    train_data = merge_vocab(best_pair, train_data)   # use defined function\n",
    "    \n",
    "    bpe_codes[best_pair] = i    # save merging history\n",
    "    bpe_codes_reverse[best_pair[0] + best_pair[1]] = best_pair  # reversing dictionary\n",
    "    \n",
    "    print(\"new merge: {}\".format(best_pair))\n",
    "    print(\"train data: {}\".format(train_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the result shows, in the first iteration of merge, because byte pair “e” and “s” occurred 6 + 3 = 9 times which is the most frequent. We merge these to a new token “es”."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "##Iteration 1\n",
    "new merge: ('e', 's')\n",
    "train data: {'l o w </w>': 5, 'l o w e r </w>': 2, 'n e w es t </w>': 6, 'w i d es t </w>': 3}\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the second iteration of merge, token “es” and “t” occurred 6 + 3 = 9 times, which is the most frequent. We merge these to a new token “est”."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "##Iteration 2\n",
    "new merge: ('es', 't')\n",
    "train data: {'l o w </w>': 5, 'l o w e r </w>': 2, 'n e w est </w>': 6, 'w i d est </w>': 3}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the third iteration of merge, token 'est' and \"</w\\>\" pair is the most frequent.\n",
    "\n",
    "Stop token \"</w\\>\" is important. Without \"</w\\>\", if there is a token \"st\", this token could be in the word \"star\", or the word \"newest\", however, the meaning of them are quite different. With \"</w\\>\", if there is a token \"st</w\\>\", the model immediately know that it is the token for the word \"newest</w\\>\" but not \"star</w\\>\".\n",
    "\n",
    "We could count the iteration or the maximum number of tokens to control the number of tokens we want to have.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we get a byte-pair encoding vocabulary `bpe_codes`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{('e', 's'): 0, ('es', 't'): 1, ('est', '</w>'): 2, ('l', 'o'): 3, ('lo', 'w'): 4, ('n', 'e'): 5, ('ne', 'w'): 6, ('new', 'est</w>'): 7, ('low', '</w>'): 8, ('w', 'i'): 9}\n"
     ]
    }
   ],
   "source": [
    "print(bpe_codes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we will use this byte-pair encoding vocabulary to tokenize given word. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply BPE\n",
    "\n",
    "This uses a greedy longest-match-first algorithm to perform tokenization using the given vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pairs(word):\n",
    "    \"\"\"Return set of symbol pairs in a word.\n",
    "    Word is represented as a tuple of symbols (symbols being variable-length strings).\n",
    "    \"\"\"\n",
    "    pairs = set()\n",
    "    prev_char = word[0]\n",
    "    for char in word[1:]:\n",
    "        pairs.add((prev_char, char))\n",
    "        prev_char = char\n",
    "    return pairs\n",
    "\n",
    "\n",
    "def encode(orig):\n",
    "    \"\"\"Encode word based on list of BPE merge operations, which are applied consecutively\"\"\"\n",
    "\n",
    "    word = tuple(orig) + ('</w>',)   # tokenize to characters and add <\\w> token\n",
    "    display(Markdown(\"__word split into characters:__ <tt>{}</tt>\".format(word)))\n",
    "\n",
    "    pairs = get_pairs(word)    # use defined function to get symbol pairs, e.g., ('w', 'e'), ('o', 'w')...\n",
    "\n",
    "    if not pairs:\n",
    "        return orig\n",
    "\n",
    "    iteration = 0\n",
    "    while True:\n",
    "        iteration += 1\n",
    "        display(Markdown(\"__Iteration {}:__\".format(iteration)))\n",
    "        \n",
    "        print(\"bigrams in the word: {}\".format(pairs))\n",
    "        # find the candidate pair for merging\n",
    "        bigram = min(pairs, key = lambda pair: bpe_codes.get(pair, float('inf')))  \n",
    "        print(\"candidate for merging: {}\".format(bigram))\n",
    "        #This uses a greedy longest-match-first algorithm. \n",
    "        if bigram not in bpe_codes:\n",
    "            display(Markdown(\"__Candidate not in BPE merges, algorithm stops.__\"))\n",
    "            break\n",
    "        first, second = bigram\n",
    "    # merge the candidate pair and update the word tuple\n",
    "        new_word = []\n",
    "        i = 0\n",
    "        while i < len(word):\n",
    "            try:\n",
    "                j = word.index(first, i)\n",
    "                new_word.extend(word[i:j])\n",
    "                i = j\n",
    "            except:\n",
    "                new_word.extend(word[i:])\n",
    "                break\n",
    "            # if match the candidate pair, merge them\n",
    "            if word[i] == first and i < len(word)-1 and word[i+1] == second:\n",
    "                new_word.append(first+second)\n",
    "                i += 2\n",
    "            else:\n",
    "                new_word.append(word[i])\n",
    "                i += 1\n",
    "        new_word = tuple(new_word)\n",
    "        word = new_word\n",
    "        print(\"word after merging: {}\".format(word))\n",
    "        if len(word) == 1:\n",
    "            break\n",
    "        else:\n",
    "            pairs = get_pairs(word)\n",
    "\n",
    "    # don't print end-of-word symbols\n",
    "    if word[-1] == '</w>':\n",
    "        word = word[:-1]\n",
    "    elif word[-1].endswith('</w>'):\n",
    "        word = word[:-1] + (word[-1].replace('</w>',''),)\n",
    "   \n",
    "    return word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The word **lowest** was not in the training data. Both **low** and ending **est** are the learned merges, so the word splits as we would expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "__word split into characters:__ <tt>('l', 'o', 'w', 'e', 's', 't', '</w>')</tt>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "__Iteration 1:__"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bigrams in the word: {('l', 'o'), ('o', 'w'), ('t', '</w>'), ('w', 'e'), ('e', 's'), ('s', 't')}\n",
      "candidate for merging: ('e', 's')\n",
      "word after merging: ('l', 'o', 'w', 'es', 't', '</w>')\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "__Iteration 2:__"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bigrams in the word: {('l', 'o'), ('o', 'w'), ('t', '</w>'), ('w', 'es'), ('es', 't')}\n",
      "candidate for merging: ('es', 't')\n",
      "word after merging: ('l', 'o', 'w', 'est', '</w>')\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "__Iteration 3:__"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bigrams in the word: {('w', 'est'), ('l', 'o'), ('o', 'w'), ('est', '</w>')}\n",
      "candidate for merging: ('est', '</w>')\n",
      "word after merging: ('l', 'o', 'w', 'est</w>')\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "__Iteration 4:__"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bigrams in the word: {('l', 'o'), ('o', 'w'), ('w', 'est</w>')}\n",
      "candidate for merging: ('l', 'o')\n",
      "word after merging: ('lo', 'w', 'est</w>')\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "__Iteration 5:__"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bigrams in the word: {('lo', 'w'), ('w', 'est</w>')}\n",
      "candidate for merging: ('lo', 'w')\n",
      "word after merging: ('low', 'est</w>')\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "__Iteration 6:__"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bigrams in the word: {('low', 'est</w>')}\n",
      "candidate for merging: ('low', 'est</w>')\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "__Candidate not in BPE merges, algorithm stops.__"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('low', 'est')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encode(\"lowest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentencepiece python module\n",
    "We also introduce a useful python package in this section. \n",
    "\n",
    "Ref: https://github.com/google/sentencepiece"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentencepiece in /Library/Python/3.7/site-packages (0.1.85)\n",
      "\u001b[33mYou are using pip version 19.0.3, however version 20.0.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install sentencepiece"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Sentencepiece` supports BPE (byte-pair-encoding) for subword segmentation with --model_type=bpe flag. \n",
    "\n",
    "`Sentencepiece` will the generated bpe vocabulary automatically. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### generate bpe vocabulary \n",
    "'''\n",
    "vocab_size= n define the number of vocabulary.\n",
    "'''\n",
    "spm.SentencePieceTrainer.train('--input=bpe_text.txt --model_prefix=m_bpe --vocab_size=200 --model_type=bpe')\n",
    "## generate two file: m_bpe.model, m_bpe.vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check the bpe vocabulary, `m_bpe.vocab` file. \n",
    "```\n",
    "<unk>\t0\n",
    "<s>\t0\n",
    "</s>\t0\n",
    "▁t\t-0\n",
    "he\t-1\n",
    "▁a\t-2\n",
    "in\t-3\n",
    "▁s\t-4\n",
    "▁w\t-5\n",
    "▁the\t-6\n",
    "▁o\t-7\n",
    "re\t-8\n",
    "▁b\t-9\n",
    "▁m\t-10\n",
    "ou\t-11\n",
    "ed\t-12\n",
    "▁I\t-13\n",
    ".\n",
    ".\n",
    ".\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply BPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** BPE ***\n",
      "['▁this', '▁is', '▁a', '▁t', 'es', 't', '▁he', 'll', 'o', '▁w', 'or', 'ld']\n"
     ]
    }
   ],
   "source": [
    "# check model \n",
    "sp_bpe = spm.SentencePieceProcessor()\n",
    "sp_bpe.load('m_bpe.model')\n",
    "\n",
    "print('*** BPE ***')\n",
    "print(sp_bpe.encode_as_pieces('this is a test hello world'))\n",
    "# __ represent the start of a word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After tokenization, you can find that there are whole **words** and **subwords**.\n",
    "\n",
    "In this vocabulary, subwords occuring **at the front of a word** are preceded by **‘__’** to denote this case. Namaly, `Sentencepiece` uses the `__` to clarify the boundary of word instead of `</w>` in previous example. \n",
    "\n",
    "In the vocabulary of BERT, Devlin et al. use `##` to denote the boundary of word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference Reading:\n",
    "* [Neural Machine Translation of Rare Words with Subword Units](https://www.aclweb.org/anthology/P16-1162.pdf)\n",
    "* [Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates](https://arxiv.org/pdf/1804.10959.pdf)\n",
    "* [SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing](https://arxiv.org/pdf/1808.06226.pdf)\n",
    "* [A New Algorithm for Data Compression](https://www.derczynski.com/papers/archive/BPE_Gage.pdf)\n",
    "* https://leimao.github.io/blog/Byte-Pair-Encoding/\n",
    "* http://ufal.mff.cuni.cz/~helcl/courses/npfl116/ipython/byte_pair_encoding.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
