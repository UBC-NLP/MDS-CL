{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "name": "bert_TPU.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "6b34cec4f6bf45a6b8ce95035cf39143": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_3604f77e277f40cd8b0ab7c539800423",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_e1665e2ce99d41949a59b7c86d3d3952",
              "IPY_MODEL_3fe8a56aa07a48bdbde0509e9e8f6c99"
            ]
          }
        },
        "3604f77e277f40cd8b0ab7c539800423": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e1665e2ce99d41949a59b7c86d3d3952": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_aa251281d75d40db91860513a7c8cfc2",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 231508,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 231508,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_845a130033de44f8a7b0c0b5666b8111"
          }
        },
        "3fe8a56aa07a48bdbde0509e9e8f6c99": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_8e701881ab6349b9b8ec9bdcac3d9ba7",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 232k/232k [00:00&lt;00:00, 1.53MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_8026af500e434cb8885ea036920d333a"
          }
        },
        "aa251281d75d40db91860513a7c8cfc2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "845a130033de44f8a7b0c0b5666b8111": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8e701881ab6349b9b8ec9bdcac3d9ba7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "8026af500e434cb8885ea036920d333a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "24990fc9ef4f47d9aefd4d3701c87136": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_63e8e1ce514e41f8a1feeec79c9101b2",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_3579e6b8c5624dfaa44b292bbcb117e9",
              "IPY_MODEL_87ce49a1331e4cb1a2e63569cd05c8bb"
            ]
          }
        },
        "63e8e1ce514e41f8a1feeec79c9101b2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3579e6b8c5624dfaa44b292bbcb117e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_bdd3abbfeb19470892770a8177d437b1",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 362,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 362,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_acea9e19e2e34befba5ec492f69d8c88"
          }
        },
        "87ce49a1331e4cb1a2e63569cd05c8bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_88a4066318b24f0dae67e6be4501e2aa",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 362/362 [00:00&lt;00:00, 13.6kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_2fe3c7dc41f24b8db5e320869076819a"
          }
        },
        "bdd3abbfeb19470892770a8177d437b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "acea9e19e2e34befba5ec492f69d8c88": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "88a4066318b24f0dae67e6be4501e2aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "2fe3c7dc41f24b8db5e320869076819a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0970a579f4604ae095cf4b90f7422952": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_352719df298b4efc9c56003efada3f9e",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_0222619e3dfa41b6ad387ab584ee295d",
              "IPY_MODEL_5901ce09a71e4f5e926f5b5e3e381e07"
            ]
          }
        },
        "352719df298b4efc9c56003efada3f9e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0222619e3dfa41b6ad387ab584ee295d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_d2738fe0d8184e86aecbf66d86e2af5c",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 1344997306,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1344997306,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_4a24e2af745444c0b911d31e85483ef6"
          }
        },
        "5901ce09a71e4f5e926f5b5e3e381e07": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_57990d8ef9024ecc94bf0162046ff533",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1.34G/1.34G [00:25&lt;00:00, 53.2MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_440031a041ba4f50ab3e03a2e3e76e26"
          }
        },
        "d2738fe0d8184e86aecbf66d86e2af5c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "4a24e2af745444c0b911d31e85483ef6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "57990d8ef9024ecc94bf0162046ff533": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "440031a041ba4f50ab3e03a2e3e76e26": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QtmMKDhHyXCt"
      },
      "source": [
        "# COLX 585 Trends in Computational Linguistic\n",
        "##  Lab tutorial 4: Training on TPU device\n",
        "\n",
        "* Utilize the Cloud TPUs to fine-tune BERT.\n",
        "* PyTorch XLA package help us use Cloud TPU cores as devices, like any other PyTorch device.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "71iJHb9QNSb7",
        "outputId": "f6e1d2e9-3b64-4f2e-ec1e-d5f2521f85f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3R4_u9OM5ytd",
        "colab_type": "text"
      },
      "source": [
        "## Overview\n",
        "Generally, we only need to change few things to utilize TPUs:\n",
        "  - Install PyTorch XLA package\n",
        "  - Set device as XLA device: `device = xm.xla_device()`\n",
        "  - Change `optimizer.step()` function to `xm.optimizer_step(optimizer, barrier=True)` in order to prevent graphs from growing too large.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "v9tEG9FaeOPN"
      },
      "source": [
        "## Import require Python libraries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1aSak3fS4To4",
        "colab_type": "text"
      },
      "source": [
        "Colab doesn't install `transformers` library automatically. Hence, we should install `transformers` first."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ftsdss2KDXbB",
        "colab_type": "code",
        "outputId": "1b8bc857-75d6-4397-f51e-65ac01658ed2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        }
      },
      "source": [
        "! pip install transformers"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (2.8.0)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.45.0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.12.39)\n",
            "Requirement already satisfied: tokenizers==0.5.2 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.5.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.85)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.41)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.5)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.3.3)\n",
            "Requirement already satisfied: botocore<1.16.0,>=1.15.39 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.15.39)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.4.5.1)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.39->boto3->transformers) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.39->boto3->transformers) (2.8.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6VYmQ07aGis_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "assert os.environ['COLAB_TPU_ADDR'], 'Make sure to select TPU from Edit > Notebook settings > Hardware accelerator'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xEGuHZInGuG2",
        "colab_type": "text"
      },
      "source": [
        "### Installing PyTorch/XLA\n",
        "PyTorch can use Cloud **TPU cores** as devices with the **PyTorch/XLA package**. For more on PyTorch/XLA see its [Github](https://github.com/pytorch/xla) or its [documentation](http://pytorch.org/xla/).\n",
        "\n",
        "Run the following cell to install PyTorch, Torchvision, and PyTorch/XLA. I"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dN5Bgq71Gx-4",
        "colab_type": "code",
        "outputId": "b3d65b53-5264-4383-ded7-c2fc0ebecbcf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 935
        }
      },
      "source": [
        "!curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n",
        "!python pytorch-xla-env-setup.py "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  3727  100  3727    0     0  15464      0 --:--:-- --:--:-- --:--:-- 15400\n",
            "Updating TPU and VM. This may take around 2 minutes.\n",
            "Updating TPU runtime to pytorch-dev20200325 ...\n",
            "Uninstalling torch-1.4.0:\n",
            "Done updating TPU runtime: <Response [200]>\n",
            "  Successfully uninstalled torch-1.4.0\n",
            "Uninstalling torchvision-0.5.0:\n",
            "  Successfully uninstalled torchvision-0.5.0\n",
            "Copying gs://tpu-pytorch/wheels/torch-nightly+20200325-cp36-cp36m-linux_x86_64.whl...\n",
            "- [1 files][ 83.4 MiB/ 83.4 MiB]                                                \n",
            "Operation completed over 1 objects/83.4 MiB.                                     \n",
            "Copying gs://tpu-pytorch/wheels/torch_xla-nightly+20200325-cp36-cp36m-linux_x86_64.whl...\n",
            "- [1 files][114.5 MiB/114.5 MiB]                                                \n",
            "Operation completed over 1 objects/114.5 MiB.                                    \n",
            "Copying gs://tpu-pytorch/wheels/torchvision-nightly+20200325-cp36-cp36m-linux_x86_64.whl...\n",
            "/ [1 files][  2.5 MiB/  2.5 MiB]                                                \n",
            "Operation completed over 1 objects/2.5 MiB.                                      \n",
            "Processing ./torch-nightly+20200325-cp36-cp36m-linux_x86_64.whl\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch==nightly+20200325) (1.18.2)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch==nightly+20200325) (0.16.0)\n",
            "\u001b[31mERROR: fastai 1.0.60 requires torchvision, which is not installed.\u001b[0m\n",
            "Installing collected packages: torch\n",
            "Successfully installed torch-1.5.0a0+d6149a7\n",
            "Processing ./torch_xla-nightly+20200325-cp36-cp36m-linux_x86_64.whl\n",
            "Installing collected packages: torch-xla\n",
            "Successfully installed torch-xla-1.6+e788e5b\n",
            "Processing ./torchvision-nightly+20200325-cp36-cp36m-linux_x86_64.whl\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchvision==nightly+20200325) (1.5.0a0+d6149a7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision==nightly+20200325) (1.12.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision==nightly+20200325) (1.18.2)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision==nightly+20200325) (7.0.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->torchvision==nightly+20200325) (0.16.0)\n",
            "Installing collected packages: torchvision\n",
            "Successfully installed torchvision-0.6.0a0+3c254fb\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  libomp5\n",
            "0 upgraded, 1 newly installed, 0 to remove and 25 not upgraded.\n",
            "Need to get 234 kB of archives.\n",
            "After this operation, 774 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libomp5 amd64 5.0.1-1 [234 kB]\n",
            "Fetched 234 kB in 1s (331 kB/s)\n",
            "Selecting previously unselected package libomp5:amd64.\n",
            "(Reading database ... 144568 files and directories currently installed.)\n",
            "Preparing to unpack .../libomp5_5.0.1-1_amd64.deb ...\n",
            "Unpacking libomp5:amd64 (5.0.1-1) ...\n",
            "Setting up libomp5:amd64 (5.0.1-1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.6/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FXGum0pBNmh9",
        "outputId": "f71861b9-7d6b-4aed-f3ae-1845fa47f14c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import tensorflow\n",
        "import torch\n",
        "\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.autograd as autograd\n",
        "from tqdm import tqdm, trange\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import io\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score, classification_report, confusion_matrix\n",
        "import sys\n",
        "import shutil\n",
        "import argparse\n",
        "import tempfile\n",
        "import urllib.request\n",
        "import zipfile\n",
        "\n",
        "# imports the torch_xla package\n",
        "import torch_xla\n",
        "import torch_xla.core.xla_model as xm"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9T0DfuK7ExLy",
        "colab": {}
      },
      "source": [
        "from transformers import *"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2U71-q3YR5I0",
        "colab_type": "text"
      },
      "source": [
        "Now, we set the device as `xla` device (i.e., TPU). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bqla0aCxRNmP",
        "colab_type": "code",
        "outputId": "b76323fc-51a6-460c-c757-f5420d78b98a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "device = xm.xla_device()\n",
        "print(device)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "xla:1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_0oyjhomExL7"
      },
      "source": [
        "## Data prepare\n",
        "\n",
        "Similar, we use the corpus from the [CL-Aff shared task](https://sites.google.com/view/affcon2019/cl-aff-shared-task?authuser=0). We focus on `sociality classification`. Sociality refers to `whether or not other people than the author are involved in the emotion situation`. \n",
        "We only use labelled dataset which include 10,560 labelled samples. \n",
        "\n",
        "We have already preprocessed (tokenization, removing URLs, mentions, hashtags and so on) the tweets and placed it under ``./happy_db`` folder in three files as ``train.tsv``, ``dev.tsv`` and ``test.tsv``. We split the labeled data into 80\\% training set (8,448 moments) and 20\\% development set (2112 moments)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Li616JkGiku2"
      },
      "source": [
        "First, we define a function to pre-process input data. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "b8L3V8MFExL8",
        "colab": {}
      },
      "source": [
        "# define a function for data preparation\n",
        "def data_prepare(file_path, lab2ind, tokenizer, max_len = 32, mode = 'train'):\n",
        "    '''\n",
        "    file_path: the path to input file. \n",
        "                In train mode, the input must be a tsv file that includes two columns where the first is text, and second column is label.\n",
        "                The first row must be header of columns.\n",
        "\n",
        "                In predict mode, the input must be a tsv file that includes only one column where the first is text.\n",
        "                The first row must be header of column.\n",
        "\n",
        "    lab2ind: dictionary of label classes\n",
        "    tokenizer: BERT tokenizer\n",
        "    max_len: maximal length of input sequence\n",
        "    mode: train or predict\n",
        "    '''\n",
        "    # if we are in train mode, we will load two columns (i.e., text and label).\n",
        "    if mode == 'train':\n",
        "        # Use pandas to load dataset\n",
        "        df = pd.read_csv(file_path, delimiter='\\t',header=0, names=['content','label'])\n",
        "        print(\"Data size \", df.shape)\n",
        "        labels = df.label.values\n",
        "        \n",
        "        # Create sentence and label lists\n",
        "        labels = [lab2ind[i] for i in labels] \n",
        "        print(\"Label is \", labels[0])\n",
        "        \n",
        "        # Convert data into torch tensors\n",
        "        labels = torch.tensor(labels)\n",
        "\n",
        "    # if we are in predict mode, we will load one column (i.e., text).\n",
        "    elif mode == 'predict':\n",
        "        df = pd.read_csv(file_path, delimiter='\\t',header=0, names=['content'])\n",
        "        print(\"Data size \", df.shape)\n",
        "        # create placeholder\n",
        "        labels = []\n",
        "    else:\n",
        "        print(\"the type of mode should be either 'train' or 'predict'. \")\n",
        "        return\n",
        "        \n",
        "    # Create sentence and label lists\n",
        "    content = df.content.values\n",
        "\n",
        "    # We need to add a special token at the beginning for BERT to work properly.\n",
        "    content = [\"[CLS] \" + text for text in content]\n",
        "\n",
        "    # Import the BERT tokenizer, used to convert our text into tokens that correspond to BERT's vocabulary.\n",
        "    tokenized_texts = [tokenizer.tokenize(text) for text in content]\n",
        "\n",
        "    # [cls,s,cd,r,[SEP]]. max_len= 3\n",
        "    # if the sequence is longer the maximal length, we truncate it to the pre-defined maximal length\n",
        "    tokenized_texts = [ text[:max_len+1] for text in tokenized_texts]\n",
        "\n",
        "    # We also need to add a special token at the end.\n",
        "    tokenized_texts = [ text+['[SEP]'] for text in tokenized_texts]\n",
        "    print (\"Tokenize the first sentence:\\n\",tokenized_texts[0])\n",
        "    \n",
        "    # Use the BERT tokenizer to convert the tokens to their index numbers in the BERT vocabulary\n",
        "    input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
        "    print (\"Index numbers of the first sentence:\\n\",input_ids[0])\n",
        "\n",
        "    # Pad our input seqeunce to the fixed length (i.e., max_len) with index of [PAD] token\n",
        "    pad_ind = tokenizer.convert_tokens_to_ids(['[PAD]'])[0]\n",
        "    input_ids = pad_sequences(input_ids, maxlen=max_len+2, dtype=\"long\", truncating=\"post\", padding=\"post\", value=pad_ind)\n",
        "    print (\"Index numbers of the first sentence after padding:\\n\",input_ids[0])\n",
        "\n",
        "    # Create attention masks\n",
        "    attention_masks = []\n",
        "\n",
        "    # Create a mask of 1s for each token followed by 0s for pad tokens \n",
        "    for seq in input_ids:\n",
        "        seq_mask = [float(i>0) for i in seq]\n",
        "        attention_masks.append(seq_mask)\n",
        "\n",
        "    # Convert all of our data into torch tensors, the required datatype for our model\n",
        "    inputs = torch.tensor(input_ids)\n",
        "    masks = torch.tensor(attention_masks)\n",
        "\n",
        "    return inputs, labels, masks"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "LkjWo9hz0Sc6"
      },
      "source": [
        "We use `BertTokenizer.from_pretrained()` to load vocabulary of pretrained model. The first argument should be either a string with the `shortcut name` of a pretrained model or a path to a directory containing model vocabulary file, `vocab.txt`. `Transformers` provides many pre-trained checkpoints with pre-defined `shortcut name`. If the argument is a correct model identifier listed on [here](https://huggingface.co/models), the model will download the vocabulary and load it to tokenizer automatically. If it doesn't match any model identifier, the model will use this argument as a path to load the vocabulary. \n",
        "\n",
        "We give \"bert-large-uncased\" as the first argument, which refers to the **24-layer, 1024-hidden, 16-heads, 340M parameters** [variant of BERT model](https://huggingface.co/transformers/pretrained_models.html). \"bert-large-uncased\" is a model identifier in `Transformers` so the vocabulary will be downloaded and applied to our tokenizer. The vocabulary of \"bert-large-uncased\" was generated using bype-pair encoding and includes 30,522 WordPieces.     "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Tr_NcxYJExL_",
        "outputId": "6ad179e1-03a3-45d5-ee31-6dae17d88175",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "6b34cec4f6bf45a6b8ce95035cf39143",
            "3604f77e277f40cd8b0ab7c539800423",
            "e1665e2ce99d41949a59b7c86d3d3952",
            "3fe8a56aa07a48bdbde0509e9e8f6c99",
            "aa251281d75d40db91860513a7c8cfc2",
            "845a130033de44f8a7b0c0b5666b8111",
            "8e701881ab6349b9b8ec9bdcac3d9ba7",
            "8026af500e434cb8885ea036920d333a"
          ]
        }
      },
      "source": [
        "model_path = \"bert-large-uncased\"\n",
        "# define label to number dictionary\n",
        "lab2ind = {'no': 0, 'yes': 1}\n",
        "\n",
        "# tokenizer from pre-trained BERT model\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased',do_lower_case=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6b34cec4f6bf45a6b8ce95035cf39143",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Downloading', max=231508, style=ProgressStyle(description_wid…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WN21IUc12UIJ",
        "outputId": "11e5b354-07a9-4484-f10a-9e8e54e66e6c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        }
      },
      "source": [
        "# Use defined funtion to extract data\n",
        "train_inputs, train_labels, train_masks = data_prepare(\"./drive/My Drive/Colab Notebooks/happy_db/train.tsv\", lab2ind,tokenizer)\n",
        "validation_inputs, validation_labels, validation_masks = data_prepare(\"./drive/My Drive/Colab Notebooks/happy_db/dev.tsv\", lab2ind,tokenizer)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Data size  (8448, 2)\n",
            "Label is  1\n",
            "Tokenize the first sentence:\n",
            " ['[CLS]', 'it', 'was', 'my', 'birthday', ',', 'and', 'my', 'wife', 'and', 'daughter', 'surprised', 'me', 'with', 'some', 'surprise', 'guests', 'and', 'a', 'small', 'party', '.', '[SEP]']\n",
            "Index numbers of the first sentence:\n",
            " [101, 2009, 2001, 2026, 5798, 1010, 1998, 2026, 2564, 1998, 2684, 4527, 2033, 2007, 2070, 4474, 6368, 1998, 1037, 2235, 2283, 1012, 102]\n",
            "Index numbers of the first sentence after padding:\n",
            " [ 101 2009 2001 2026 5798 1010 1998 2026 2564 1998 2684 4527 2033 2007\n",
            " 2070 4474 6368 1998 1037 2235 2283 1012  102    0    0    0    0    0\n",
            "    0    0    0    0    0    0]\n",
            "Data size  (1056, 2)\n",
            "Label is  1\n",
            "Tokenize the first sentence:\n",
            " ['[CLS]', 'my', 'baby', 'took', 'a', '1', '.', '5', 'hour', 'nap', 'instead', 'of', 'a', '20', '##min', '##ute', 'nap', 'and', 'i', 'was', 'able', 'to', 'get', 'some', 'things', 'done', '!', '[SEP]']\n",
            "Index numbers of the first sentence:\n",
            " [101, 2026, 3336, 2165, 1037, 1015, 1012, 1019, 3178, 18996, 2612, 1997, 1037, 2322, 10020, 10421, 18996, 1998, 1045, 2001, 2583, 2000, 2131, 2070, 2477, 2589, 999, 102]\n",
            "Index numbers of the first sentence after padding:\n",
            " [  101  2026  3336  2165  1037  1015  1012  1019  3178 18996  2612  1997\n",
            "  1037  2322 10020 10421 18996  1998  1045  2001  2583  2000  2131  2070\n",
            "  2477  2589   999   102     0     0     0     0     0     0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0lcPwbokicgT",
        "outputId": "deba88b7-af61-4ac3-802f-54a066f4a853",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train_inputs.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([8448, 34])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rNM2geLmi0fG"
      },
      "source": [
        "Create an iterator of our data with `torch DataLoader`. This helps us to save on memory during training.\n",
        "\n",
        "For fine-tuning BERT on a specific task, the authors recommend a batch size of 16 or 32. We use 32 batch size here. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CU6Xq9C2ExMC",
        "colab": {}
      },
      "source": [
        "batch_size = 32\n",
        "# We'll take training samples in random order in each epoch. \n",
        "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "train_dataloader = DataLoader(train_data, \n",
        "                              sampler = RandomSampler(train_data), # Select batches randomly\n",
        "                              batch_size=batch_size)\n",
        "\n",
        "# We'll just read validation set sequentially.\n",
        "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
        "validation_dataloader = DataLoader(validation_data, \n",
        "                                   sampler = SequentialSampler(validation_data), # Pull out batches sequentially.\n",
        "                                   batch_size=batch_size)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wfY7kczF3B0h"
      },
      "source": [
        "## Creating `Bert_cls` class\n",
        "\n",
        "Now, we put everthing together. We bulid a `Bert_cls` class to train a BERT classifier end-to-end."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JT7oG6XmtOPY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Bert_cls(nn.Module):\n",
        "    def __init__(self, lab2ind, model_path, hidden_size):\n",
        "        super(Bert_cls, self).__init__()\n",
        "        self.model_path = model_path\n",
        "        self.hidden_size = hidden_size\n",
        "        self.bert_model = BertModel.from_pretrained(model_path, output_hidden_states=True, output_attentions=True)\n",
        "        self.label_num = len(lab2ind)\n",
        "        self.fc = nn.Linear(self.hidden_size, self.label_num)\n",
        "    def forward(self, bert_ids, bert_mask):\n",
        "        last_hidden_state, pooler_output, hidden_states, attentions = self.bert_model(input_ids=bert_ids)\n",
        "        fc_output = self.fc(pooler_output)\n",
        "        return fc_output, attentions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "fFXk-8JWoQsX"
      },
      "source": [
        "Instantiate model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oN1cDKUVSkX-",
        "colab_type": "code",
        "outputId": "5803ebff-90fc-40b3-b114-351fb0e2dfe3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 115,
          "referenced_widgets": [
            "24990fc9ef4f47d9aefd4d3701c87136",
            "63e8e1ce514e41f8a1feeec79c9101b2",
            "3579e6b8c5624dfaa44b292bbcb117e9",
            "87ce49a1331e4cb1a2e63569cd05c8bb",
            "bdd3abbfeb19470892770a8177d437b1",
            "acea9e19e2e34befba5ec492f69d8c88",
            "88a4066318b24f0dae67e6be4501e2aa",
            "2fe3c7dc41f24b8db5e320869076819a",
            "0970a579f4604ae095cf4b90f7422952",
            "352719df298b4efc9c56003efada3f9e",
            "0222619e3dfa41b6ad387ab584ee295d",
            "5901ce09a71e4f5e926f5b5e3e381e07",
            "d2738fe0d8184e86aecbf66d86e2af5c",
            "4a24e2af745444c0b911d31e85483ef6",
            "57990d8ef9024ecc94bf0162046ff533",
            "440031a041ba4f50ab3e03a2e3e76e26"
          ]
        }
      },
      "source": [
        "bert_model = Bert_cls(lab2ind, 'bert-large-uncased', 1024).to(device)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "24990fc9ef4f47d9aefd4d3701c87136",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Downloading', max=362, style=ProgressStyle(description_width=…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0970a579f4604ae095cf4b90f7422952",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Downloading', max=1344997306, style=ProgressStyle(description…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "OV7ES5nqokUJ"
      },
      "source": [
        "Count the number of parameters. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "v5zZMwKMokwe",
        "outputId": "2799f537-242d-4bda-91e3-ae960cfb8b03",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(bert_model):,} trainable parameters')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The model has 335,143,938 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Jxrtir7GpDFr"
      },
      "source": [
        "This model will use 10.72 Gb memory!!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wE7Dw_3VExMJ"
      },
      "source": [
        "## Optimizer and Learning Rate Scheduler"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "qyx2_qG2ExMK"
      },
      "source": [
        "For the purposes of fine-tuning, the authors recommend the following hyperparameter ranges (from Appendix A.3 of the [paper](https://arxiv.org/pdf/1810.04805.pdf)):\n",
        "\n",
        "* Batch size: 16, 32\n",
        "* Learning rate (Adam): 5e-5, 3e-5, 2e-5\n",
        "* Number of epochs: 2, 3, 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "w6jvvyZBYqWX"
      },
      "source": [
        "We use:\n",
        "\n",
        "* Batch size: 32\n",
        "* Learning rate (Adam): 2e-5\n",
        "* Number of epochs: 3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dQFXSd4nExMK",
        "colab": {}
      },
      "source": [
        "# Parameters:\n",
        "lr = 2e-5\n",
        "max_grad_norm = 1.0\n",
        "epochs = 3\n",
        "warmup_proportion = 0.1\n",
        "num_training_steps  = len(train_dataloader) * epochs\n",
        "num_warmup_steps = num_training_steps * warmup_proportion\n",
        "\n",
        "### In Transformers, optimizer and schedules are instantiated like this:\n",
        "# Note: AdamW is a class from the huggingface library\n",
        "# the 'W' stands for 'Weight Decay\"\n",
        "optimizer = AdamW(bert_model.parameters(), lr=lr, correct_bias=False)\n",
        "# schedules\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_training_steps)  # PyTorch scheduler\n",
        "\n",
        "# We use nn.CrossEntropyLoss() as our loss function. \n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "65tgu1WeExMV"
      },
      "source": [
        "# Model training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HvI47019plpC"
      },
      "source": [
        "We define a `train()` function. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nmWjTgRRefUq",
        "colab": {}
      },
      "source": [
        "def train(model, iterator, optimizer, scheduler, criterion):\n",
        "    \n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    \n",
        "    for i, batch in enumerate(iterator):\n",
        "        \n",
        "        # Add batch to TPU\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        # Unpack the inputs from our dataloader\n",
        "        input_ids, input_mask, labels = batch\n",
        "\n",
        "        outputs,_ = model(input_ids, input_mask)\n",
        "\n",
        "        loss = criterion(outputs, labels)\n",
        "        # delete used variables to free memory\n",
        "        del batch, input_ids, input_mask, labels\n",
        "\n",
        "        loss.backward()\n",
        "        # Clip the norm of the gradients to 1.0.\n",
        "        # This is to help prevent the \"exploding gradients\" problem.\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)  # Gradient clipping is not in AdamW anymore\n",
        "\n",
        "        xm.optimizer_step(optimizer, barrier=True)  # Note: Cloud TPU-specific code!!!\n",
        "\n",
        "        scheduler.step()\n",
        "        epoch_loss += loss.cpu().item()\n",
        "        optimizer.zero_grad()\n",
        "  \n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62QORpvsUa3T",
        "colab_type": "text"
      },
      "source": [
        "PyTorch uses Cloud TPUs through the [XLA deep learning compiler](https://www.tensorflow.org/xla). This compiler records the operations we perform into a graph which is evaluated all-at-once and as needed. `xm.optimizer_step(optimizer, barrier=True)` inserts a \"barrier\" in the graph that forces evaluation every time the gradients are updated. This prevents XLA's graphs from growing too large. For more details about how PyTorch uses XLA and Cloud TPUs see [the documentation](http://pytorch.org/xla/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "YXoagDUlpuhq"
      },
      "source": [
        "We define a `evaluate()` function. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1WP6nFZkfgpS",
        "colab": {}
      },
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    all_pred=[]\n",
        "    all_label = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for i, batch in enumerate(iterator):\n",
        "\n",
        "            # Add batch to TPU\n",
        "            batch = tuple(t.to(device) for t in batch)\n",
        "            # Unpack the inputs from our dataloader\n",
        "            input_ids, input_mask, labels = batch\n",
        "\n",
        "            outputs,_ = model(input_ids, input_mask)\n",
        "            \n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # delete used variables to free TPU memory\n",
        "            del batch, input_ids, input_mask\n",
        "            \n",
        "            epoch_loss += loss.cpu().item()\n",
        "\n",
        "            # identify the predicted class for each example in the batch\n",
        "            probabilities, predicted = torch.max(outputs.cpu().data, 1)\n",
        "            # put all the true labels and predictions to two lists\n",
        "            all_pred.extend(predicted)\n",
        "            all_label.extend(labels.cpu())\n",
        "    \n",
        "    accuracy = accuracy_score(all_label, all_pred)\n",
        "    f1score = f1_score(all_label, all_pred, average='macro') \n",
        "    return epoch_loss / len(iterator), accuracy, f1score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "G86QFF03hBkn"
      },
      "source": [
        "Training model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "eixPjzgSExMW",
        "outputId": "c5fb5264-1830-4dfa-9c9a-1c53b609dcf0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "# Train the model\n",
        "loss_list = []\n",
        "acc_list = []\n",
        "\n",
        "for epoch in trange(epochs, desc=\"Epoch\"):\n",
        "    train_loss = train(bert_model, train_dataloader, optimizer, scheduler, criterion)  \n",
        "    val_loss, val_acc, val_f1 = evaluate(bert_model, validation_dataloader, criterion)\n",
        "\n",
        "    # Create checkpoint at end of each epoch\n",
        "    state = {\n",
        "        'epoch': epoch,\n",
        "        'state_dict': bert_model.cpu().state_dict(),\n",
        "        'optimizer': optimizer.state_dict(),\n",
        "        'scheduler': scheduler.state_dict()\n",
        "        }\n",
        "\n",
        "    torch.save(state, \"./drive/My Drive/Colab Notebooks/ckpt_BERT/BERT_\"+str(epoch+1)+\".pt\")\n",
        "\n",
        "    print('\\n Epoch [{}/{}], Train Loss: {:.4f}, Validation Loss: {:.4f}, Validation Accuracy: {:.4f}, Validation F1: {:.4f}'.format(epoch+1, epochs, train_loss, val_loss, val_acc, val_f1))\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:   0%|          | 0/3 [00:00<?, ?it/s]/pytorch/torch/csrc/utils/python_arg_parser.cpp:750: UserWarning: This overload of add_ is deprecated:\n",
            "\tadd_(Number alpha, Tensor other)\n",
            "Consider using one of the following signatures instead:\n",
            "\tadd_(Tensor other, Number alpha)\n",
            "Epoch:  33%|███▎      | 1/3 [03:24<06:49, 204.92s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Epoch [1/3], Train Loss: 0.3165, Validation Loss: 0.2382, Validation Accuracy: 0.9252, Validation F1: 0.9250\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  67%|██████▋   | 2/3 [05:26<02:59, 179.93s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Epoch [2/3], Train Loss: 0.1761, Validation Loss: 0.1928, Validation Accuracy: 0.9328, Validation F1: 0.9323\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 100%|██████████| 3/3 [07:27<00:00, 149.19s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Epoch [3/3], Train Loss: 0.1261, Validation Loss: 0.1992, Validation Accuracy: 0.9375, Validation F1: 0.9370\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "I6ZLOYT-U_L-"
      },
      "source": [
        "### References:\n",
        "* http://pytorch.org/xla/release/1.5/index.html\n",
        "* https://github.com/pytorch/xla\n",
        "* http://mccormickml.com/2019/07/22/BERT-fine-tuning/\n",
        "* https://huggingface.co/transformers/index.html\n",
        "* https://colab.research.google.com/drive/1ywsvwO6thOVOrfagjjfuxEf6xVRxbUNO#scrollTo=_QXZhFb4LnV5\n",
        "* https://github.com/google-research/bert\n",
        "* https://towardsdatascience.com/deconstructing-bert-part-2-visualizing-the-inner-workings-of-attention-60a16d86b5c1"
      ]
    }
  ]
}